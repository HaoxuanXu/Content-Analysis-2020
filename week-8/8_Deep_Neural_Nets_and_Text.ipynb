{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "8-Deep-Neural-Nets-and-Text.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ajUetvTuVU0U",
        "Oi28IAogVU0n",
        "_GE6b7dsVU04",
        "rPGyyBVxVU1D",
        "FEBRn0odVU1M",
        "af950QCSVU1s",
        "kA7XPr0gVU2C",
        "kS1KRxaIVU3N"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c995c9019d04a5e900b6f29347a94d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d1c6b88e05154c76900d95017a266476",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_deebd8712aae488384badbe49e44fba9",
              "IPY_MODEL_021bbb687b3442dd82c4d2386478b695"
            ]
          }
        },
        "d1c6b88e05154c76900d95017a266476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "deebd8712aae488384badbe49e44fba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef0d0e77a8e441e0bfb816bdd83b8da4",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_61af859720844edfa7258c9d3125f1d0"
          }
        },
        "021bbb687b3442dd82c4d2386478b695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_70e2d0ab2ca7483385769722e76a8a32",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 232k/232k [00:00&lt;00:00, 416kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18be198a32724eb6970bde37a97d3bfd"
          }
        },
        "ef0d0e77a8e441e0bfb816bdd83b8da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "61af859720844edfa7258c9d3125f1d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70e2d0ab2ca7483385769722e76a8a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18be198a32724eb6970bde37a97d3bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "308b0fc5e57d4046ad8bc39458dc830b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1607f0ac607d42a591c7b2a701b6bd9a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a975b7c65321451385ef1d5b6f9660a9",
              "IPY_MODEL_c9e3ef0f6a854485812566b7fbf0ff65"
            ]
          }
        },
        "1607f0ac607d42a591c7b2a701b6bd9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a975b7c65321451385ef1d5b6f9660a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2df886bfdfc5425bb38b23efc60d7973",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba3b06c35194467a8e4845b6e70e4a4e"
          }
        },
        "c9e3ef0f6a854485812566b7fbf0ff65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0af23832f98e483695fe2bb225ff3e83",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 361/361 [00:00&lt;00:00, 16.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4cab99baf1304971abefa5c280ea23af"
          }
        },
        "2df886bfdfc5425bb38b23efc60d7973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba3b06c35194467a8e4845b6e70e4a4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0af23832f98e483695fe2bb225ff3e83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4cab99baf1304971abefa5c280ea23af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba42893a30964bba9abb0663751e1ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2d57fabc87e24b4fbabc3f1551b38f23",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_105008078c744a94944112eca6fb89e8",
              "IPY_MODEL_8038d93b644e4447b19a768256a069a7"
            ]
          }
        },
        "2d57fabc87e24b4fbabc3f1551b38f23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "105008078c744a94944112eca6fb89e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_75aca782860448ccb659085b63bf2bed",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0595c422e2b7423389ebcc4bab0beb03"
          }
        },
        "8038d93b644e4447b19a768256a069a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40980ba8bc794fd68ac8f639cec3da9e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 440M/440M [00:39&lt;00:00, 11.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_482ccf5af2c34510b2c7484f9c8aea9e"
          }
        },
        "75aca782860448ccb659085b63bf2bed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0595c422e2b7423389ebcc4bab0beb03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40980ba8bc794fd68ac8f639cec3da9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "482ccf5af2c34510b2c7484f9c8aea9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9def3917127043e4ad7801a58505edb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_222ff29243194de0abcbb29adf0e4fa8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_20602626f93f485d992bde68adf73a4f",
              "IPY_MODEL_2f25b6b7a2fe4d139d19a0c9ee820809"
            ]
          }
        },
        "222ff29243194de0abcbb29adf0e4fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "20602626f93f485d992bde68adf73a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8ae8fff832ff41cf878336cd209b52a6",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 546,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 546,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_75ee0d70f7264635bda48feeb4720548"
          }
        },
        "2f25b6b7a2fe4d139d19a0c9ee820809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f0d9e8f8d1d844d48ee9d3f397c2d5cd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 546/546 [00:00&lt;00:00, 33.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2b452f5071047eca41d788db8f58c98"
          }
        },
        "8ae8fff832ff41cf878336cd209b52a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "75ee0d70f7264635bda48feeb4720548": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0d9e8f8d1d844d48ee9d3f397c2d5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2b452f5071047eca41d788db8f58c98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "878bacd9fa1349cd971ff4090198b4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_754f1baba7c24561be6ef7512425e45f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7abec9c901c34dc9a33ff5be98c43a6b",
              "IPY_MODEL_844a6c424d924475b7d1f532028d07cc"
            ]
          }
        },
        "754f1baba7c24561be6ef7512425e45f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7abec9c901c34dc9a33ff5be98c43a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_89110e75334d4766a09c12019f38b036",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 754,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 754,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e8d97221a9c44d02aa835c401929dd89"
          }
        },
        "844a6c424d924475b7d1f532028d07cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2986f635e05c45399bd03b7cdf29fe95",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 754/754 [00:00&lt;00:00, 31.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ac5cff3edb849fa95c353a008ca67f3"
          }
        },
        "89110e75334d4766a09c12019f38b036": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e8d97221a9c44d02aa835c401929dd89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2986f635e05c45399bd03b7cdf29fe95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ac5cff3edb849fa95c353a008ca67f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e73f130af51840dc851c4b2f415087d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a41ca2afa42a4611aa6f6019c5c781fb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_89fbd3b381544cf58f0e1fb459d82b1e",
              "IPY_MODEL_7e4b9c20c7cb488ebe4444c114664d02"
            ]
          }
        },
        "a41ca2afa42a4611aa6f6019c5c781fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89fbd3b381544cf58f0e1fb459d82b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_503d74b3dc334228acecbe62972a4104",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 230,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 230,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3dec6473721a4b4d9656169518dcd19d"
          }
        },
        "7e4b9c20c7cb488ebe4444c114664d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9f28272d7f2b4e509a1ec48784ffd441",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 230/230 [00:00&lt;00:00, 11.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_161d2b5c9c2c4cfea9e2cc00d235d2e1"
          }
        },
        "503d74b3dc334228acecbe62972a4104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3dec6473721a4b4d9656169518dcd19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f28272d7f2b4e509a1ec48784ffd441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "161d2b5c9c2c4cfea9e2cc00d235d2e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a1bb4a33f5f466eae890e1b441393c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f58615c4d84a4780a797889feed1288b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7c1f53d7a828433ca968a46e51340fd7",
              "IPY_MODEL_674fa38e3e574b50b41fde5639326b10"
            ]
          }
        },
        "f58615c4d84a4780a797889feed1288b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c1f53d7a828433ca968a46e51340fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5f702c8462a4e229644a9cf20ecab46",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 267844284,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267844284,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a209549fc824e7e96f7cd767754d400"
          }
        },
        "674fa38e3e574b50b41fde5639326b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c90d9e6aeec84ffbb4c04824255abdf3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 268M/268M [00:22&lt;00:00, 11.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2701bfb6728a417da08e3c33e37c22a7"
          }
        },
        "a5f702c8462a4e229644a9cf20ecab46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a209549fc824e7e96f7cd767754d400": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c90d9e6aeec84ffbb4c04824255abdf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2701bfb6728a417da08e3c33e37c22a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "781480d1155f43d1b7b67e010ec3f1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_328095142a6b450ab4715bd2ad7a477c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77d78ba207fa4655a2084c4d85d2e38f",
              "IPY_MODEL_91cbde568aad4e6895230c6fd9dbbccb"
            ]
          }
        },
        "328095142a6b450ab4715bd2ad7a477c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77d78ba207fa4655a2084c4d85d2e38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cd431ea7d7744c0bcc9e106b38541fb",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 230,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 230,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_251eb2ffa9044967bd8ea42621c63722"
          }
        },
        "91cbde568aad4e6895230c6fd9dbbccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ff67ad2cbd814659ad84fd688be2583e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 230/230 [00:00&lt;00:00, 10.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22486f6d84d84257ac568d71e597c126"
          }
        },
        "0cd431ea7d7744c0bcc9e106b38541fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "251eb2ffa9044967bd8ea42621c63722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff67ad2cbd814659ad84fd688be2583e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22486f6d84d84257ac568d71e597c126": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4b6aedc2d534e599ab3812e376cdb4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8bda4a79dce34f2684f97ae87782dc7f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a11170460d264e81a12ff97781ff78d9",
              "IPY_MODEL_6ca1bee0ede847508b99ac3524bf0022"
            ]
          }
        },
        "8bda4a79dce34f2684f97ae87782dc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a11170460d264e81a12ff97781ff78d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_36e001801d61431f8607da1bafebc99a",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 555,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 555,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_78e4ad1550b148ecbad7d292f3c7a578"
          }
        },
        "6ca1bee0ede847508b99ac3524bf0022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f8f671674c944185b2a5273411906e29",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 555/555 [00:00&lt;00:00, 26.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_37312bd099284fa887ff89175db55b97"
          }
        },
        "36e001801d61431f8607da1bafebc99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "78e4ad1550b148ecbad7d292f3c7a578": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8f671674c944185b2a5273411906e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "37312bd099284fa887ff89175db55b97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a32831ca071418d9ba8a9d884160a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_314bcd23f53b4041b76058bbd44f4393",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c29256b9ab894819a714dba4e0a0a3b6",
              "IPY_MODEL_46e4083a3b69468aabdf8831ef8a6c73"
            ]
          }
        },
        "314bcd23f53b4041b76058bbd44f4393": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c29256b9ab894819a714dba4e0a0a3b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d51d2a79baf04cac9cad1b6d2f06df62",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 265481570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 265481570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_262e1bc51cd54271bdc8053927911b1c"
          }
        },
        "46e4083a3b69468aabdf8831ef8a6c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_add0aa1680d7455c99d77f36a4559843",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 265M/265M [00:23&lt;00:00, 11.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c3af8e6022b5494a9ccf0ae3a5aec650"
          }
        },
        "d51d2a79baf04cac9cad1b6d2f06df62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "262e1bc51cd54271bdc8053927911b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "add0aa1680d7455c99d77f36a4559843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c3af8e6022b5494a9ccf0ae3a5aec650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6bd4e8071ea04f54a1236db97e5d3b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0b163c10db3b4ee4aa07325b21cbccf6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dcb487c18d014542aa28bc0bb9114782",
              "IPY_MODEL_93b8c107ea4f4af49b802efef342e433"
            ]
          }
        },
        "0b163c10db3b4ee4aa07325b21cbccf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcb487c18d014542aa28bc0bb9114782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_df5de5efae5c4308946c6573b57991a5",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 230,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 230,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_00428507d1c84f95bce9f6e551dde84e"
          }
        },
        "93b8c107ea4f4af49b802efef342e433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_554f2d962e4649fcb7f9ec78c60eb806",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 230/230 [00:00&lt;00:00, 13.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77641d2ac64b4e9497df53e7b8e29853"
          }
        },
        "df5de5efae5c4308946c6573b57991a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "00428507d1c84f95bce9f6e551dde84e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "554f2d962e4649fcb7f9ec78c60eb806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77641d2ac64b4e9497df53e7b8e29853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cbd3348f2c94964a0031b7e9edf0445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4a9e05693be04e5e88f7e379bd59a62f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_559691b1f0cc47f1bda7a0a164810704",
              "IPY_MODEL_63a9af6b96aa4a72973876868eae0089"
            ]
          }
        },
        "4a9e05693be04e5e88f7e379bd59a62f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "559691b1f0cc47f1bda7a0a164810704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d04b386c8e064f81b2e40a6ee98c405d",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67972abd5b424557bac2f8bb18adf50f"
          }
        },
        "63a9af6b96aa4a72973876868eae0089": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_684a5d67ae2f4b96937dd54e7c96a9b8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 268M/268M [00:23&lt;00:00, 11.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee8b93da6b4a464394a11b4a21ad9e7e"
          }
        },
        "d04b386c8e064f81b2e40a6ee98c405d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67972abd5b424557bac2f8bb18adf50f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "684a5d67ae2f4b96937dd54e7c96a9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee8b93da6b4a464394a11b4a21ad9e7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "419583b13181418a8f0fb14ca0c159e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eb9af3a95dba4cddb9837d6b38a6d1b6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_375b0fc149bb4d9f9237b6e2ca5308b3",
              "IPY_MODEL_ba2d3b938c9449a39b43e8ef92f6955f"
            ]
          }
        },
        "eb9af3a95dba4cddb9837d6b38a6d1b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "375b0fc149bb4d9f9237b6e2ca5308b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c899aa63a81b4900af5f9c6cbc123631",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 230,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 230,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f200809d6c1946168c71b221c4148b1d"
          }
        },
        "ba2d3b938c9449a39b43e8ef92f6955f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fa271e71d9d040febe7edbb0bbda8469",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 230/230 [00:00&lt;00:00, 12.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48d101e1f47b4d74bd8aa5c561f21df2"
          }
        },
        "c899aa63a81b4900af5f9c6cbc123631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f200809d6c1946168c71b221c4148b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa271e71d9d040febe7edbb0bbda8469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48d101e1f47b4d74bd8aa5c561f21df2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3920749555a941638f53fb875d305289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6069aa702c854535b12675a1ab1e49e3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a299cb9d41bb431ca4af66742d6e8844",
              "IPY_MODEL_b7a6739e829d460eb748e2e2b9579cb9"
            ]
          }
        },
        "6069aa702c854535b12675a1ab1e49e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a299cb9d41bb431ca4af66742d6e8844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_44359362b6d144fa931a0d508dc4b01f",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 224,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 224,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d94f5fbc7c68470bb7b914caebc70efb"
          }
        },
        "b7a6739e829d460eb748e2e2b9579cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a220eaebe77d4f54b9e0688f3403cc2b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 224/224 [00:00&lt;00:00, 11.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_956d03f069e748c48049fcc63cc1b7af"
          }
        },
        "44359362b6d144fa931a0d508dc4b01f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d94f5fbc7c68470bb7b914caebc70efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a220eaebe77d4f54b9e0688f3403cc2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "956d03f069e748c48049fcc63cc1b7af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db39367aee87405bbc3c36847fb9f9e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fbc6da3bedca43ebb6b8bf670725c657",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d66ed220a64847158d4b9df782c2421f",
              "IPY_MODEL_780f6727c3ab41dbaca15a778f7b019b"
            ]
          }
        },
        "fbc6da3bedca43ebb6b8bf670725c657": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d66ed220a64847158d4b9df782c2421f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cd3feaf6a1d84c6d89b033c067d99071",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77be4070ef454a01b3195f89c2bbbc70"
          }
        },
        "780f6727c3ab41dbaca15a778f7b019b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a679e1ec3ef144b9b8f3e55e16f8b849",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 1.04M/1.04M [00:01&lt;00:00, 951kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_55353aead72e4078957007a31e767d4e"
          }
        },
        "cd3feaf6a1d84c6d89b033c067d99071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77be4070ef454a01b3195f89c2bbbc70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a679e1ec3ef144b9b8f3e55e16f8b849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "55353aead72e4078957007a31e767d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "11364f433f134157907dab72475e2903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f9b3108b31f34f328cd9965c6f857da0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ed8875965a044113a2d4e2a8e64f8c5f",
              "IPY_MODEL_0585182ddec54a3da10edcf66fa057d5"
            ]
          }
        },
        "f9b3108b31f34f328cd9965c6f857da0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed8875965a044113a2d4e2a8e64f8c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8ecf708c50394928948c5e762dd2c862",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0bb279123bb343bb8e26d78ff8075edb"
          }
        },
        "0585182ddec54a3da10edcf66fa057d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_01829b7d76974b5295dbe36806d8f2d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 456k/456k [00:00&lt;00:00, 622kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7999d73833f42508aca4ef687d0b9db"
          }
        },
        "8ecf708c50394928948c5e762dd2c862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0bb279123bb343bb8e26d78ff8075edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "01829b7d76974b5295dbe36806d8f2d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7999d73833f42508aca4ef687d0b9db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cdbdc66f13374ca0b0aee5fea9027b6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f98de45b0fbf45ab8faa14078acaa860",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_05c557d4a2704b9eb9b31fe384982775",
              "IPY_MODEL_55e9c2d3169c46b3a08165397bf9c875"
            ]
          }
        },
        "f98de45b0fbf45ab8faa14078acaa860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05c557d4a2704b9eb9b31fe384982775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e965849cd974b07805e9a2d06591260",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_988d10b8b6da46a4bf885867dd3bd2bb"
          }
        },
        "55e9c2d3169c46b3a08165397bf9c875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5230857292b54f34a77ac729a4d94dda",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 548M/548M [00:48&lt;00:00, 11.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f9eb419fb6343609952da5270e3afd7"
          }
        },
        "9e965849cd974b07805e9a2d06591260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "988d10b8b6da46a4bf885867dd3bd2bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5230857292b54f34a77ac729a4d94dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f9eb419fb6343609952da5270e3afd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaoxuanXu/Content-Analysis-2020/blob/master/week-8/8_Deep_Neural_Nets_and_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWM6Cy46VUx9",
        "colab_type": "text"
      },
      "source": [
        "## Week - 8 - Deep Neural Nets and Text\n",
        "\n",
        "In this week we introduce the use of Deep Neural Networks to work with text. We have already seen some uses of neural networks for text in our classification HW, where we used a simple neural network--the one-layer perceptron--to classify text. It performed quite well, but comes up short in more sophisticated classification tasks, such as in predicting intent. We have also seen slightly deeper, 2-level neural nets in the form of word embeddings such as Word2Vec. While they work well, they have some drawbacks, such as representing words with multiple meanings in a singular space. \n",
        "\n",
        "BERT, which is a language model built using bidirectional encoders, allows us to take advantage of a powerful pre-trained model which we can then use to perform our own tasks based on data we analyze. \n",
        "\n",
        "In this notebook we use ```huggingface/transformers```, a python package that allows for easy interface to use pre-trained BERT models. It is built using Tensorflow and PyTorch, two computational graph packages which are built specifically for creating powerful neural networks. We will also be introducing Keras, which allows us to easily build Neural Networks in an abstracted way. Keras is a popular way to understand how we can stack layers to create such Neural Networks, but to reach state-of-the-art results we will stick with using BERT and similar models that can be tuned to extremely high performance on specific language understanding and generation tasks.\n",
        "\n",
        "To demonstrate this, we begin by using the [Corpus of Linguistic Acceptability](https://nyu-mll.github.io/CoLA/). We will also use BERT by learning how to extract embeddings from such a model and use it to semantically probe sentences. There are a number of new packages and methods we will be using so be sure to update lucem_illud_2020.\n",
        "\n",
        "## NOTE\n",
        "\n",
        "This notebook **requires** GPUs for training models in section 1 and section 3. To train models, please use this [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) to create the models. Note that I have only given you view access: please create your own colab file to train your models, using the code and instructions I have given in the Colab file. So while you have to do the homework on this notebook, the models which you will train should be done on Google Colab, which has GPU access. If you happen to have GPU access on your personal machines or some other way to train the models, you are welcome to do that too.\n",
        "\n",
        "Note that if you run the computationally intensive models on your local computer they will take a long time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5jv09GGVkLP",
        "colab_type": "code",
        "outputId": "f99ea7b6-8c97-4243-c22d-f45b57175c9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git\n",
            "  Cloning git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git to /tmp/pip-req-build-xabu7gv2\n",
            "  Running command git clone -q git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git /tmp/pip-req-build-xabu7gv2\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.17.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.25.3)\n",
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (6.2.2)\n",
            "Collecting pdfminer2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/97/bd2a2de878438c27ffd710b5d6562c7a0230b0f3ca86059ec635ed231eb1/pdfminer2-20151206-py2.py3-none-any.whl (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 44.0MB/s \n",
            "\u001b[?25hCollecting GitPython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/2f/6a366d56c9b1355b0880be9ea66b166cb3536392638d8d91413ec66305ad/GitPython-3.1.0-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 50.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wordcloud in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: seaborn in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: gensim in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (3.1.3)\n",
            "Collecting pyanno3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/1a/ee2b136ea0283adb2a9302c29594127f84b6e34cb0b02b91c63bed0a534b/pyanno3-2.0.2.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (4.6.3)\n",
            "Requirement already satisfied, skipping upgrade: graphviz in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.11.15)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.4)\n",
            "Collecting pydub\n",
            "  Downloading https://files.pythonhosted.org/packages/79/db/eaf620b73a1eec3c8c6f8f5b0b236a50f9da88ad57802154b7ba7664d0b8/pydub-0.23.1-py2.py3-none-any.whl\n",
            "Collecting speechrecognition\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/e1/7f5678cd94ec1234269d23756dbdaa4c8cfaed973412f88ae8adf7893a50/SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8MB)\n",
            "\u001b[K     |████████████████████████████████| 32.8MB 88kB/s \n",
            "\u001b[?25hCollecting pysoundfile\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/b3/0b871e5fd31b9a8e54b4ee359384e705a1ca1e2870706d2f081dc7cc1693/PySoundFile-0.9.0.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: scikit-image in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (0.16.2)\n",
            "Requirement already satisfied, skipping upgrade: IPython in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: spacy in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.1.9)\n",
            "Collecting transformers==2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: keras in /usr/local/lib/python3.6/dist-packages (from lucem-illud-2020==8.0.1) (2.2.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->lucem-illud-2020==8.0.1) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->lucem-illud-2020==8.0.1) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->lucem-illud-2020==8.0.1) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: lxml>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from python-docx->lucem-illud-2020==8.0.1) (4.2.6)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from pdfminer2->lucem-illud-2020==8.0.1) (1.12.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/f5/8f84b3bf9d94bdf2454a302f2fa375832b53660ea532586b8a55ff16ae9a/gitdb-4.0.2-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->lucem-illud-2020==8.0.1) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim->lucem-illud-2020==8.0.1) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lucem-illud-2020==8.0.1) (1.1.0)\n",
            "Collecting traits\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/af/c6dc88130106d69e4f9a192043c85ed4cb522f83b9041b8691f0b0678405/traits-6.0.0.tar.gz (441kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 48.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->lucem-illud-2020==8.0.1) (1.14.15)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->lucem-illud-2020==8.0.1) (4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=0.6 in /usr/local/lib/python3.6/dist-packages (from pysoundfile->lucem-illud-2020==8.0.1) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->lucem-illud-2020==8.0.1) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->lucem-illud-2020==8.0.1) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (45.2.0)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (2.1.3)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (4.3.3)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from IPython->lucem-illud-2020==8.0.1) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.2.4)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (7.0.8)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->lucem-illud-2020==8.0.1) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (4.28.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 40.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 25.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1->lucem-illud-2020==8.0.1) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.15.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (1.27.1)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->lucem-illud-2020==8.0.1) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras->lucem-illud-2020==8.0.1) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->lucem-illud-2020==8.0.1) (3.13)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/d2/27777ab463cd44842c78305fa8097dfba0d94768abbb7e1c4d88f1fa1a0b/smmap-3.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim->lucem-illud-2020==8.0.1) (2.49.0)\n",
            "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->lucem-illud-2020==8.0.1) (0.15.2)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=0.6->pysoundfile->lucem-illud-2020==8.0.1) (2.19)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->IPython->lucem-illud-2020==8.0.1) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->IPython->lucem-illud-2020==8.0.1) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->lucem-illud-2020==8.0.1) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1->lucem-illud-2020==8.0.1) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->lucem-illud-2020==8.0.1) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow->lucem-illud-2020==8.0.1) (1.0.0)\n",
            "Building wheels for collected packages: lucem-illud-2020, python-docx, pyanno3, traits, sacremoses\n",
            "  Building wheel for lucem-illud-2020 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lucem-illud-2020: filename=lucem_illud_2020-8.0.1-cp36-none-any.whl size=35151 sha256=edc781e6b95eecba573a31cae582f467e8f1fa0c03541adec806cc26b05cd20e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bqx6k6j6/wheels/a8/16/91/3c63788e494d360378317fe5ec9f4972f661844af8ae8c26f0\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp36-none-any.whl size=184491 sha256=f3f6727e29cddff88ad0747284558cae2f269ab38b64190d2c5c833347e5ebcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "  Building wheel for pyanno3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyanno3: filename=pyanno3-2.0.2-cp36-none-any.whl size=116993 sha256=cc1bd41a9678b0e1262828ce99726e24b41900ffc2bd6ec67d65a7890e6fc6b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/5d/f6/7c1618b7471ec03ac97f6913baba31ae007003c4fa2bc99855\n",
            "  Building wheel for traits (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for traits: filename=traits-6.0.0-cp36-cp36m-linux_x86_64.whl size=385405 sha256=6b219d2353fce530ea62053a9c6cd7c5ea8772a2d7b5d152767a738f3ce6ce5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/a7/f0/d1dfae8d3a4e5638a40818830c741c1c0e9f8a590b9ea22935\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=1ea2828a93d8f253fa0859a8cb2ed330dc78aa21ed99a2a4b3c244bd6b4fb702\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built lucem-illud-2020 python-docx pyanno3 traits sacremoses\n",
            "Installing collected packages: python-docx, pdfminer2, smmap, gitdb, GitPython, traits, pyanno3, pydub, speechrecognition, pysoundfile, sacremoses, tokenizers, sentencepiece, transformers, lucem-illud-2020\n",
            "Successfully installed GitPython-3.1.0 gitdb-4.0.2 lucem-illud-2020-8.0.1 pdfminer2-20151206 pyanno3-2.0.2 pydub-0.23.1 pysoundfile-0.9.0.post1 python-docx-0.8.10 sacremoses-0.0.38 sentencepiece-0.1.85 smmap-3.0.1 speechrecognition-3.8.1 tokenizers-0.0.11 traits-6.0.0 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h333npcbWFlQ",
        "colab_type": "code",
        "outputId": "c3ba9b00-55cf-4401-ff9f-74d6c198bf87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "!pip install transformers==2.4.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==2.4.1 in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (0.0.38)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (0.0.11)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (1.11.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.4.1) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.4.1) (2019.11.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.4.1) (7.0)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.4.1) (0.9.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.4.1) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers==2.4.1) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4MLOYjwWMsE",
        "colab_type": "code",
        "outputId": "ecaaa163-5c6b-4c91-f8e4-05e4c9545abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2XLcQmYVUyA",
        "colab_type": "code",
        "outputId": "cfac72c3-f34e-495e-ccb4-29c2678f48da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import torch # pip install torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertConfig # pip install transformers==2.4.1\n",
        "from transformers import AdamW, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import lucem_illud_2020 # pip install -U git+git://github.com/Computational-Content-Analysis-2020/lucem_illud_2020.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88IM5VHzVUyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6F_GxhnVUyL",
        "colab_type": "code",
        "outputId": "2c531642-7885-49f6-9ae6-67289490dc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzvusN-ZVUyQ",
        "colab_type": "text"
      },
      "source": [
        "## CoLA Dataset and pre-processing\n",
        "\n",
        "We start with loading our dataset and pre-processing it. The pre-processing follows similar steps as we have done in the past, but we will be using pre-written modules offered by the transformers package. These are some of the things we have to take care of when using this particular BERT model.\n",
        "\n",
        "    -special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
        "    -tokens that conforms with the fixed vocabulary used in BERT\n",
        "    -token IDs from BERT’s tokenizer\n",
        "    -mask IDs to indicate which elements in the sequence are tokens and which are padding elements\n",
        "    -segment IDs used to distinguish different sentences\n",
        "    -positional embeddings used to show token position within the sequence\n",
        "\n",
        "\n",
        "We will be using parts of the code from [this notebook](https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=BJR6t_gCQe_x) which walks us through the process of using a pre-trained BERT model. The interface to use these models comes from the package [huggingface/transformers](https://github.com/huggingface/transformers). \n",
        "\n",
        "We start by setting up our GPU if we can - this may not work on your machine, so it has been commented out.\n",
        "\n",
        "An aside: check out this tutorial too - https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFAgsqtwVUyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpu = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-78DfdpVUyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if gpu:\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BNzfVdZVUyY",
        "colab_type": "code",
        "outputId": "5ec9abde-5706-4ebd-8cc2-00425af40d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPSUsUfpdZQ-",
        "colab_type": "code",
        "outputId": "ff0a1918-1645-46bd-8a0c-4984dbb0c947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mskLR6uKVUyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/My Drive/week 8/data/cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQu1c8z8VUyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "\n",
        "# We need to add special tokens at the beginning and end of each sentence for BERT to work properly\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dniSdSRLVUyj",
        "colab_type": "code",
        "outputId": "db8963ff-e0bf-478b-a20f-7b36643784bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "8c995c9019d04a5e900b6f29347a94d1",
            "d1c6b88e05154c76900d95017a266476",
            "deebd8712aae488384badbe49e44fba9",
            "021bbb687b3442dd82c4d2386478b695",
            "ef0d0e77a8e441e0bfb816bdd83b8da4",
            "61af859720844edfa7258c9d3125f1d0",
            "70e2d0ab2ca7483385769722e76a8a32",
            "18be198a32724eb6970bde37a97d3bfd"
          ]
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c995c9019d04a5e900b6f29347a94d1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tokenize the first sentence:\n",
            "['[CLS]', 'our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwOzX4VYVUyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "# In the original paper, the authors used a length of 512.\n",
        "MAX_LEN = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGry5dOZVUys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBBJgreQVUyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vviTqqtWVUyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7-Ev04XVUy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2020, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2020, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6O9yuCUVUy6",
        "colab_type": "text"
      },
      "source": [
        "### Introducing Deep Neural Nets\n",
        "\n",
        "A popular, simplified package for introducing deep neural networks is [Keras](https://keras.io). It is a high level package in that we don't bother with every detail or hyper-parameter associated with the neural network (e.g., regularizers), and can stack on layers directly. For a rapid tutorial on neural networks for text such as the LSTM or the Recurrent Neural Network, Colah's blog is a great start. [LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) is an article on LSTMs, and if you'd like to  learn about RNN, Andrej Karpathy does a great job in [this blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), in addition to our reading from the newest online verion of Jurafsky & Martin's review of deep learning methods in their book on speech and language processing, chapters [6,7,9,10](https://web.stanford.edu/~jurafsky/slp3/), and the [*Deep Learning*](https://www.deeplearningbook.org/) book by Goodfellow, Bengio & Courville.\n",
        "\n",
        "In the following cells we build a basic deep net that has an embedding layer and an LSTM to perform classification. This is to illustrate the process of using Keras, which is a very popular library for such work. It may not yield state of the art performance because it constrains the hyperparameters you can tune, but is nonetheless an useful tool and works well on some datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffshGSUiVUy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYoyyC1MVUy-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyFeUlOdVUzC",
        "colab_type": "code",
        "outputId": "fce293b0-30d3-4b25-b921-16d64d5a8be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6navuz5wVUzH",
        "colab_type": "code",
        "outputId": "6bd81971-9f29-45b9-e4a7-6b011781d852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        }
      },
      "source": [
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7695/7695 [==============================] - 60s 8ms/step - loss: 0.6137 - acc: 0.7012\n",
            "Epoch 2/10\n",
            "7695/7695 [==============================] - 44s 6ms/step - loss: 0.6090 - acc: 0.7038\n",
            "Epoch 3/10\n",
            "7695/7695 [==============================] - 44s 6ms/step - loss: 0.6082 - acc: 0.7038\n",
            "Epoch 4/10\n",
            "7695/7695 [==============================] - 43s 6ms/step - loss: 0.6093 - acc: 0.7038\n",
            "Epoch 5/10\n",
            "7695/7695 [==============================] - 43s 6ms/step - loss: 0.6089 - acc: 0.7038\n",
            "Epoch 6/10\n",
            "7695/7695 [==============================] - 43s 6ms/step - loss: 0.6084 - acc: 0.7038\n",
            "Epoch 7/10\n",
            "7695/7695 [==============================] - 43s 6ms/step - loss: 0.6086 - acc: 0.7038\n",
            "Epoch 8/10\n",
            "7695/7695 [==============================] - 43s 6ms/step - loss: 0.6086 - acc: 0.7038\n",
            "Epoch 9/10\n",
            "7695/7695 [==============================] - 43s 6ms/step - loss: 0.6082 - acc: 0.7038\n",
            "Epoch 10/10\n",
            "7695/7695 [==============================] - 43s 6ms/step - loss: 0.6084 - acc: 0.7038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "is1vOF3nVUzK",
        "colab_type": "text"
      },
      "source": [
        "We can see that the accuracy of this model isn't terrible, but it still hovers around 70%. Below there is code for a slightly modified neural network - how does this one perform? Note that in this model, I have added another LSTM layer. You are encouraged to explore the [Keras documentaion](https://keras.io/layers/about-keras-layers/) to explore what kind of layers you can add and how they change performances for different tasks. Different kinds of losses, optimizers, activations and layers can change the flavour of your net dramatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw-naQcmVUzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model_lstm2 = Sequential()\n",
        "# model_lstm2.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "# model_lstm2.add(LSTM(units))\n",
        "# model_lstm2.add(LSTM(units))\n",
        "# model_lstm2.add(Dense(1, activation='sigmoid'))\n",
        "# model_lstm2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# history_lstm2 = model_lstm2.fit(input_data_train, labels, epochs=10, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1vLcxrGVUzN",
        "colab_type": "text"
      },
      "source": [
        "### On with BERT!\n",
        "\n",
        "So while Neural Networks can do a good job with some kind of classification tasks, they don't perform too well on intent classification. Let us see how a bidirectional transformer embedding like BERT might do. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxW3Jd4RVUzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkAf_NbeVUzR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 16\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jgg3-YLVUzT",
        "colab_type": "text"
      },
      "source": [
        "## Loading our Models\n",
        "\n",
        "### Train Model\n",
        "Now that our input data is properly formatted, it's time to fine tune the BERT model.\n",
        "For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n",
        "We'll load BertForSequenceClassification. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
        "\n",
        "### Structure of Fine-Tuning Model\n",
        "\n",
        "As we've showed beforehand, the first token of every sequence is the special classification token ([CLS]). Unlike the hidden state vector corresponding to a normal word token, the hidden state corresponding to this special token is designated by the authors of BERT as an aggregate representation of the whole sentence used for classification tasks. As such, when we feed in an input sentence to our model during training, the output is the length 768 hidden state vector corresponding to this token. The additional layer that we've added on top consists of untrained linear neurons of size [hidden_state, number_of_labels], so [768,2], meaning that the output of BERT plus our classification layer is a vector of two numbers representing the \"score\" for \"grammatical/non-grammatical\" that are then fed into cross-entropy loss.\n",
        "\n",
        "### The Fine-Tuning Process\n",
        "\n",
        "Because the pre-trained BERT layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
        "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. OK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n",
        "\n",
        "Credit to Michel Kana's [tutorial](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03) and the [tutorial](https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=GuE5BqICAne2) by Chris McCormick and Nick Ryan who describe the workings of BERT and the way it is used by the ```transformers``` package. \n",
        "\n",
        "## WARNING: SHIFT TO A GPU ENABLED MACHINE (e.g., Google Colab)\n",
        "\n",
        "Note that you only want to run the following code if you have a GPU. Otherwise, rerun the **same** cells we just ran on the Colab file to train your model, download it to your local, and load it by running\n",
        "```model = BertForSequenceClassification.from_pretrained(\"my_model_directory\", num_labels=2)```.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xR9qWtaVUzX",
        "colab_type": "code",
        "outputId": "89f0732d-dd51-4c62-97d5-87bda1b642bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "308b0fc5e57d4046ad8bc39458dc830b",
            "1607f0ac607d42a591c7b2a701b6bd9a",
            "a975b7c65321451385ef1d5b6f9660a9",
            "c9e3ef0f6a854485812566b7fbf0ff65",
            "2df886bfdfc5425bb38b23efc60d7973",
            "ba3b06c35194467a8e4845b6e70e4a4e",
            "0af23832f98e483695fe2bb225ff3e83",
            "4cab99baf1304971abefa5c280ea23af",
            "ba42893a30964bba9abb0663751e1ae3",
            "2d57fabc87e24b4fbabc3f1551b38f23",
            "105008078c744a94944112eca6fb89e8",
            "8038d93b644e4447b19a768256a069a7",
            "75aca782860448ccb659085b63bf2bed",
            "0595c422e2b7423389ebcc4bab0beb03",
            "40980ba8bc794fd68ac8f639cec3da9e",
            "482ccf5af2c34510b2c7484f9c8aea9e"
          ]
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "308b0fc5e57d4046ad8bc39458dc830b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba42893a30964bba9abb0663751e1ae3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NFIAei1VUza",
        "colab_type": "code",
        "outputId": "794f2985-4d92-4e23-db9f-6c31855cd2b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLrKbf_uVUzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpiy2pb3VUzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCQ-n5M9VUzi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I0pQrY-VUzl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGMdNGsJVUzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWKNqqF8VUzr",
        "colab_type": "text"
      },
      "source": [
        "Note that the following cell can take upto 12 hours or longer if run without a GPU. The [Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) demonstrates how to fine-tune models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nitb5MgnVUzs",
        "colab_type": "code",
        "outputId": "d06b9a3f-a795-4cf0-c35e-2c1b3f8c413f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import random\n",
        "\n",
        "# # This training code is based on the `run_glue.py` script here:\n",
        "# # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# # Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# # Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# # For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "#     # ========================================\n",
        "#     #               Training\n",
        "#     # ========================================\n",
        "    \n",
        "#     # Perform one full pass over the training set.\n",
        "\n",
        "     print(\"\")\n",
        "     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "     print('Training...')\n",
        "\n",
        "#     # Measure how long the training epoch takes.\n",
        "     t0 = time.time()\n",
        "\n",
        "#     # Reset the total loss for this epoch.\n",
        "     total_loss = 0\n",
        "\n",
        "#     # Put the model into training mode. Don't be mislead--the call to \n",
        "#     # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "#     # `dropout` and `batchnorm` layers behave differently during training\n",
        "#     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "     model.train()\n",
        "\n",
        "#     # For each batch of training data...\n",
        "     for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "#         # Progress update every 40 batches.\n",
        "         if step % 40 == 0 and not step == 0:\n",
        "#             # Calculate elapsed time in minutes.\n",
        "             elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "#             # Report progress.\n",
        "             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "#         # Unpack this training batch from our dataloader. \n",
        "#         #\n",
        "#         # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "#         # `to` method.\n",
        "#         #\n",
        "#         # `batch` contains three pytorch tensors:\n",
        "#         #   [0]: input ids \n",
        "#         #   [1]: attention masks\n",
        "#         #   [2]: labels \n",
        "         b_input_ids = batch[0].to(device)\n",
        "         b_input_mask = batch[1].to(device)\n",
        "         b_labels = batch[2].to(device)\n",
        "\n",
        "#         # Always clear any previously calculated gradients before performing a\n",
        "#         # backward pass. PyTorch doesn't do this automatically because \n",
        "#         # accumulating the gradients is \"convenient while training RNNs\". \n",
        "#         # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "         model.zero_grad()        \n",
        "\n",
        "#         # Perform a forward pass (evaluate the model on this training batch).\n",
        "#         # This will return the loss (rather than the model output) because we\n",
        "#         # have provided the `labels`.\n",
        "#         # The documentation for this `model` function is here: \n",
        "#         # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "         outputs = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask, \n",
        "                     labels=b_labels)\n",
        "        \n",
        "#         # The call to `model` always returns a tuple, so we need to pull the \n",
        "#         # loss value out of the tuple.\n",
        "         loss = outputs[0]\n",
        "\n",
        "#         # Accumulate the training loss over all of the batches so that we can\n",
        "#         # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "#         # single value; the `.item()` function just returns the Python value \n",
        "#         # from the tensor.\n",
        "         total_loss += loss.item()\n",
        "\n",
        "#         # Perform a backward pass to calculate the gradients.\n",
        "         loss.backward()\n",
        "\n",
        "#         # Clip the norm of the gradients to 1.0.\n",
        "#         # This is to help prevent the \"exploding gradients\" problem.\n",
        "         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "#         # Update parameters and take a step using the computed gradient.\n",
        "#         # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "#         # modified based on their gradients, the learning rate, etc.\n",
        "         optimizer.step()\n",
        "\n",
        "#         # Update the learning rate.\n",
        "         scheduler.step()\n",
        "\n",
        "#     # Calculate the average loss over the training data.\n",
        "     avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "#     # Store the loss value for plotting the learning curve.\n",
        "     loss_values.append(avg_train_loss)\n",
        "\n",
        "     print(\"\")\n",
        "     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "     print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "#     # ========================================\n",
        "#     #               Validation\n",
        "#     # ========================================\n",
        "#     # After the completion of each training epoch, measure our performance on\n",
        "#     # our validation set.\n",
        "\n",
        "     print(\"\")\n",
        "     print(\"Running Validation...\")\n",
        "\n",
        "     t0 = time.time()\n",
        "\n",
        "#     # Put the model in evaluation mode--the dropout layers behave differently\n",
        "#     # during evaluation.\n",
        "     model.eval()\n",
        "\n",
        "#     # Tracking variables \n",
        "     eval_loss, eval_accuracy = 0, 0\n",
        "     nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "#     # Evaluate data for one epoch\n",
        "     for batch in validation_dataloader:\n",
        "        \n",
        "#         # Add batch to GPU\n",
        "         batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "#         # Unpack the inputs from our dataloader\n",
        "         b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "#         # Telling the model not to compute or store gradients, saving memory and\n",
        "#         # speeding up validation\n",
        "         with torch.no_grad():        \n",
        "\n",
        "#             # Forward pass, calculate logit predictions.\n",
        "#             # This will return the logits rather than the loss because we have\n",
        "#             # not provided labels.\n",
        "#             # token_type_ids is the same as the \"segment ids\", which \n",
        "#             # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "#             # The documentation for this `model` function is here: \n",
        "#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "             outputs = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask)\n",
        "        \n",
        "#         # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "#         # values prior to applying an activation function like the softmax.\n",
        "         logits = outputs[0]\n",
        "\n",
        "#         # Move logits and labels to CPU\n",
        "         logits = logits.detach().cpu().numpy()\n",
        "         label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "#         # Calculate the accuracy for this batch of test sentences.\n",
        "         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "#         # Accumulate the total accuracy.\n",
        "         eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "#         # Track the number of batches\n",
        "         nb_eval_steps += 1\n",
        "\n",
        "#     # Report the final accuracy for this validation run.\n",
        "     print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "     print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    481.    Elapsed: 0:00:10.\n",
            "  Batch    80  of    481.    Elapsed: 0:00:19.\n",
            "  Batch   120  of    481.    Elapsed: 0:00:29.\n",
            "  Batch   160  of    481.    Elapsed: 0:00:38.\n",
            "  Batch   200  of    481.    Elapsed: 0:00:47.\n",
            "  Batch   240  of    481.    Elapsed: 0:00:57.\n",
            "  Batch   280  of    481.    Elapsed: 0:01:06.\n",
            "  Batch   320  of    481.    Elapsed: 0:01:16.\n",
            "  Batch   360  of    481.    Elapsed: 0:01:25.\n",
            "  Batch   400  of    481.    Elapsed: 0:01:34.\n",
            "  Batch   440  of    481.    Elapsed: 0:01:44.\n",
            "  Batch   480  of    481.    Elapsed: 0:01:53.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:01:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.82\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    481.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    481.    Elapsed: 0:00:19.\n",
            "  Batch   120  of    481.    Elapsed: 0:00:28.\n",
            "  Batch   160  of    481.    Elapsed: 0:00:38.\n",
            "  Batch   200  of    481.    Elapsed: 0:00:47.\n",
            "  Batch   240  of    481.    Elapsed: 0:00:56.\n",
            "  Batch   280  of    481.    Elapsed: 0:01:06.\n",
            "  Batch   320  of    481.    Elapsed: 0:01:15.\n",
            "  Batch   360  of    481.    Elapsed: 0:01:25.\n",
            "  Batch   400  of    481.    Elapsed: 0:01:34.\n",
            "  Batch   440  of    481.    Elapsed: 0:01:43.\n",
            "  Batch   480  of    481.    Elapsed: 0:01:53.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:01:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.85\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    481.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    481.    Elapsed: 0:00:19.\n",
            "  Batch   120  of    481.    Elapsed: 0:00:28.\n",
            "  Batch   160  of    481.    Elapsed: 0:00:38.\n",
            "  Batch   200  of    481.    Elapsed: 0:00:47.\n",
            "  Batch   240  of    481.    Elapsed: 0:00:56.\n",
            "  Batch   280  of    481.    Elapsed: 0:01:06.\n",
            "  Batch   320  of    481.    Elapsed: 0:01:15.\n",
            "  Batch   360  of    481.    Elapsed: 0:01:25.\n",
            "  Batch   400  of    481.    Elapsed: 0:01:34.\n",
            "  Batch   440  of    481.    Elapsed: 0:01:43.\n",
            "  Batch   480  of    481.    Elapsed: 0:01:53.\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 0:01:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    481.    Elapsed: 0:00:09.\n",
            "  Batch    80  of    481.    Elapsed: 0:00:19.\n",
            "  Batch   120  of    481.    Elapsed: 0:00:28.\n",
            "  Batch   160  of    481.    Elapsed: 0:00:38.\n",
            "  Batch   200  of    481.    Elapsed: 0:00:47.\n",
            "  Batch   240  of    481.    Elapsed: 0:00:56.\n",
            "  Batch   280  of    481.    Elapsed: 0:01:06.\n",
            "  Batch   320  of    481.    Elapsed: 0:01:15.\n",
            "  Batch   360  of    481.    Elapsed: 0:01:24.\n",
            "  Batch   400  of    481.    Elapsed: 0:01:34.\n",
            "  Batch   440  of    481.    Elapsed: 0:01:43.\n",
            "  Batch   480  of    481.    Elapsed: 0:01:53.\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:01:53\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.86\n",
            "  Validation took: 0:00:04\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2g5lHnHVUzv",
        "colab_type": "code",
        "outputId": "50a8092a-8e97-4d50-c551-8e780f0ed535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        " import matplotlib.pyplot as plt\n",
        " %matplotlib inline\n",
        "\n",
        " import seaborn as sns\n",
        "\n",
        "# # Use plot styling from seaborn.\n",
        " sns.set(style='darkgrid')\n",
        "\n",
        "# # Increase the plot size and font size.\n",
        " sns.set(font_scale=1.5)\n",
        " plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# # Plot the learning curve.\n",
        " plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# # Label the plot.\n",
        " plt.title(\"Training loss\")\n",
        " plt.xlabel(\"Epoch\")\n",
        " plt.ylabel(\"Loss\")\n",
        "\n",
        " plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvAAAAGaCAYAAABpIXfbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKQHUYAXEREREelB/j8PQ0qpi4j5EQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UYQU_aHVUzy",
        "colab_type": "text"
      },
      "source": [
        "## COME BACK TO THIS NOTEBOOK to load and work with your trained model\n",
        "\n",
        "Once you tune your model on Colab (or on your own machine if you decided to do that instead), you load it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdIbrsw9VUzz",
        "colab_type": "code",
        "outputId": "7fb00d9c-a550-42a8-f641-6a68747daa17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "  model = BertForSequenceClassification.from_pretrained(\"model_save\", num_labels=2)\n",
        "except:\n",
        "  print(\"An exception occured..\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An exception occured..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGVXjktGVUz1",
        "colab_type": "text"
      },
      "source": [
        "### Holdout Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuXNkmb6VUz3",
        "colab_type": "code",
        "outputId": "2708146f-fcc3-450e-f36f-e447ea95161f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/week 8/data/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Fah0Ex2VUz-",
        "colab_type": "code",
        "outputId": "d840a48c-eb7e-4155-a7da-29c362bcf5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RYAuvCIVU0B",
        "colab_type": "code",
        "outputId": "e44f2cd9-f3a9-4a77-ba1b-7732b36ac7d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKubqQoeVU0E",
        "colab_type": "code",
        "outputId": "7675a07b-6cde-4948-b50e-9c66863157bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovJAaqTuVU0I",
        "colab_type": "code",
        "outputId": "941c43d6-bbbb-4fec-c81f-3ab092db282f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "matthews_set"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.049286405809014416,\n",
              " -0.21684543705982773,\n",
              " 0.4732058754737091,\n",
              " 0.30508307783296046,\n",
              " 0.44440090347500916,\n",
              " 0.7410010097502685,\n",
              " 0.4152273992686999,\n",
              " 0.0,\n",
              " 0.9165151389911681,\n",
              " 0.7530836820370708,\n",
              " 0.8459051693633014,\n",
              " 0.7419408268023742,\n",
              " 0.8150678894028793,\n",
              " 0.647150228929434,\n",
              " 0.4622501635210242,\n",
              " 0.5719694409972929,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqGi_o4VVU0L",
        "colab_type": "code",
        "outputId": "f027e6b1-b41a-43d7-e4c4-d29c4a652a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-NBvbJyVU0P",
        "colab_type": "text"
      },
      "source": [
        "That's pretty good performance. Note that we used [Matthews Correlation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html) to meausure this. It ranges from -1 to 1, with +1 being the best. The Google BERT model has a similar score too, so this model performed quite well. It took a long time though, approximately a day with no GPU. It would be significantly faster if a CUDA enabled machine ran this. Hence, we recommend that you run this on the Collab notebook.\n",
        "\n",
        "The following lines save the model to disk, if you would like to: note that we ran this in the colab file to save it to disk there as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOa4JCgFVU0Q",
        "colab_type": "code",
        "outputId": "41829671-bf6c-4a9f-853c-7b05e84b09dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        " import os\n",
        "\n",
        "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "\n",
        " output_dir = './model_save/'\n",
        "\n",
        "# # Create output directory if needed\n",
        " if not os.path.exists(output_dir):\n",
        "     os.makedirs(output_dir)\n",
        "\n",
        " print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# # They can then be reloaded using `from_pretrained()`\n",
        " model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        " model_to_save.save_pretrained(output_dir)\n",
        " tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# # Good practice: save your training arguments together with the trained model\n",
        "# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./model_save/vocab.txt',\n",
              " './model_save/special_tokens_map.json',\n",
              " './model_save/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QceaLQsVU0T",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:red\">*Exercise 1*</span>\n",
        "\n",
        "<span style=\"color:red\">Construct cells immediately below this that estimate a deep classification model with Keras (and LSTM) and also BERT in order to predict pre-established data labels relevant to your final project (as for week 3's homework). Which works better? Are the errors the same or different?\n",
        "\n",
        "<span style=\"color:red\">***Stretch***</span>: <span style=\"color:red\">Now alter the neural network by stacking network layers, adjusting the embedding dimension, compare its performance with your model above, and interpret why it might be different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ld_Tmuy2m3ty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = pd.read_csv(\"/content/drive/My Drive/week 8/emails.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eOsfIAnm3_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = df2.text.values\n",
        "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "labels = df2.spam.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJVf0V4Xn-Gy",
        "colab_type": "code",
        "outputId": "4b56487e-919d-4068-e146-515d13d4ac2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print (\"Tokenize the first sentence:\")\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            "['[CLS]', 'subject', ':', 'naturally', 'irresistible', 'your', 'corporate', 'identity', 'lt', 'is', 'really', 'hard', 'to', 'rec', '##oll', '##ect', 'a', 'company', ':', 'the', 'market', 'is', 'full', 'of', 'su', '##q', '##ges', '##tions', 'and', 'the', 'information', 'iso', '##ver', '##w', '##helm', '##in', '##q', ';', 'but', 'a', 'good', 'catch', '##y', 'logo', ',', 'st', '##yl', '##ish', 'stat', '##lone', '##ry', 'and', 'outstanding', 'website', 'will', 'make', 'the', 'task', 'much', 'easier', '.', 'we', 'do', 'not', 'promise', 'that', 'ha', '##vin', '##q', 'ordered', 'a', 'io', '##go', 'your', 'company', 'will', 'automatic', '##ail', '##y', 'become', 'a', 'world', 'ie', '##ade', '##r', ':', 'it', 'is', '##gui', '##te', 'ci', '##ear', 'that', 'without', 'good', 'products', ',', 'effective', 'business', 'organization', 'and', 'pr', '##actic', '##able', 'aim', 'it', 'will', 'be', 'hot', '##at', 'nowadays', 'market', ';', 'but', 'we', 'do', 'promise', 'that', 'your', 'marketing', 'efforts', 'will', 'become', 'much', 'more', 'effective', '.', 'here', 'is', 'the', 'list', 'of', 'clear', 'benefits', ':', 'creative', '##ness', ':', 'hand', '-', 'made', ',', 'original', 'logos', ',', 'specially', 'done', 'to', 'reflect', 'your', 'distinctive', 'company', 'image', '.', 'convenience', ':', 'logo', 'and', 'station', '##ery', 'are', 'provided', 'in', 'all', 'formats', ';', 'easy', '-', 'to', '-', 'use', 'content', 'management', 'system', 'lets', '##you', 'change', 'your', 'website', 'content', 'and', 'even', 'its', 'structure', '.', 'prompt', '##ness', ':', 'you', 'will', 'see', 'logo', 'drafts', 'within', 'three', 'business', 'days', '.', 'afford', '##ability', ':', 'your', 'marketing', 'break', '-', 'through', 'shouldn', \"'\", 't', 'make', 'gaps', 'in', 'your', 'budget', '.', '100', '%', 'satisfaction', 'guaranteed', ':', 'we', 'provide', 'unlimited', 'amount', 'of', 'changes', 'with', 'no', 'extra', 'fees', 'for', 'you', 'to', 'be', 'sure', '##tha', '##t', 'you', 'will', 'love', 'the', 'result', 'of', 'this', 'collaboration', '.', 'have', 'a', 'look', 'at', 'our', 'portfolio', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', 'not', 'interested', '.', '.', '.', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq5NiFhMoDR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8PZwC5KoHPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Po2g7F6oMUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih3D0UF_oQL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enKg3Q69oT3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=200, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=200, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiWMZftDobTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_in_size = tokenizer.vocab_size\n",
        "embedding_dim = 32\n",
        "unit = 100\n",
        "no_labels = len(np.unique(train_labels))\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieG6MEaxomxz",
        "colab_type": "code",
        "outputId": "ac4bca10-0a59-4fe9-eaae-91c637e5af61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(vocab_in_size, embedding_dim, input_length=MAX_LEN))\n",
        "model_lstm.add(LSTM(unit))\n",
        "model_lstm.add(Dense(no_labels, activation='softmax'))\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 128, 32)           976704    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 1,030,106\n",
            "Trainable params: 1,030,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "888UNx2mom43",
        "colab_type": "code",
        "outputId": "502b972f-ddf4-4b70-8c87-234e1b1779c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "# Because the size is small, I will try 100 epochs\n",
        "history_lstm = model_lstm.fit(train_inputs, train_labels, \n",
        "                              epochs=10,batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "5155/5155 [==============================] - 30s 6ms/step - loss: 0.4457 - acc: 0.7901\n",
            "Epoch 2/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.1595 - acc: 0.9451\n",
            "Epoch 3/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.1471 - acc: 0.9587\n",
            "Epoch 4/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.0891 - acc: 0.9684\n",
            "Epoch 5/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.0623 - acc: 0.9758\n",
            "Epoch 6/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.0188 - acc: 0.9961\n",
            "Epoch 7/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.0217 - acc: 0.9957\n",
            "Epoch 8/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.0159 - acc: 0.9965\n",
            "Epoch 9/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.0197 - acc: 0.9969\n",
            "Epoch 10/10\n",
            "5155/5155 [==============================] - 29s 6ms/step - loss: 0.0169 - acc: 0.9969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gR5HyGaonA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_gATToxpNDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH3sF-Z3pNLn",
        "colab_type": "code",
        "outputId": "56abcf35-ac01-4ddc-8940-e606245838c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS2KgAtXpNTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLEJaZpKpdkO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgmLlb2Vpdsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfxztm-zpjoa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8OqKWXzpjwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o4aZVnKppt5",
        "colab_type": "code",
        "outputId": "d2fb1a90-8cf5-4c11-9eae-f08da481ee7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        }
      },
      "source": [
        " import random\n",
        "\n",
        "# # This training code is based on the `run_glue.py` script here:\n",
        "# # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# # Set the seed value all over the place to make this reproducible.\n",
        " seed_val = 42\n",
        "\n",
        " random.seed(seed_val)\n",
        " np.random.seed(seed_val)\n",
        " torch.manual_seed(seed_val)\n",
        " torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# # Store the average loss after each epoch so we can plot them.\n",
        " loss_values = []\n",
        "\n",
        "# # For each epoch...\n",
        " for epoch_i in range(0, epochs):\n",
        "    \n",
        "#     # ========================================\n",
        "#     #               Training\n",
        "#     # ========================================\n",
        "    \n",
        "#     # Perform one full pass over the training set.\n",
        "\n",
        "     print(\"\")\n",
        "     print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "     print('Training...')\n",
        "\n",
        "#     # Measure how long the training epoch takes.\n",
        "     t0 = time.time()\n",
        "\n",
        "#     # Reset the total loss for this epoch.\n",
        "     total_loss = 0\n",
        "\n",
        "#     # Put the model into training mode. Don't be mislead--the call to \n",
        "#     # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "#     # `dropout` and `batchnorm` layers behave differently during training\n",
        "#     # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "     model.train()\n",
        "\n",
        "#     # For each batch of training data...\n",
        "     for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "#         # Progress update every 40 batches.\n",
        "         if step % 40 == 0 and not step == 0:\n",
        "#             # Calculate elapsed time in minutes.\n",
        "             elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "#             # Report progress.\n",
        "             print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "#         # Unpack this training batch from our dataloader. \n",
        "#         #\n",
        "#         # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "#         # `to` method.\n",
        "#         #\n",
        "#         # `batch` contains three pytorch tensors:\n",
        "#         #   [0]: input ids \n",
        "#         #   [1]: attention masks\n",
        "#         #   [2]: labels \n",
        "         b_input_ids = batch[0].to(device)\n",
        "         b_input_mask = batch[1].to(device)\n",
        "         b_labels = batch[2].to(device)\n",
        "\n",
        "#         # Always clear any previously calculated gradients before performing a\n",
        "#         # backward pass. PyTorch doesn't do this automatically because \n",
        "#         # accumulating the gradients is \"convenient while training RNNs\". \n",
        "#         # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "         model.zero_grad()        \n",
        "\n",
        "#         # Perform a forward pass (evaluate the model on this training batch).\n",
        "#         # This will return the loss (rather than the model output) because we\n",
        "#         # have provided the `labels`.\n",
        "#         # The documentation for this `model` function is here: \n",
        "#         # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "         outputs = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask, \n",
        "                     labels=b_labels)\n",
        "        \n",
        "#         # The call to `model` always returns a tuple, so we need to pull the \n",
        "#         # loss value out of the tuple.\n",
        "         loss = outputs[0]\n",
        "\n",
        "#         # Accumulate the training loss over all of the batches so that we can\n",
        "#         # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "#         # single value; the `.item()` function just returns the Python value \n",
        "#         # from the tensor.\n",
        "         total_loss += loss.item()\n",
        "\n",
        "#         # Perform a backward pass to calculate the gradients.\n",
        "         loss.backward()\n",
        "\n",
        "#         # Clip the norm of the gradients to 1.0.\n",
        "#         # This is to help prevent the \"exploding gradients\" problem.\n",
        "         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "#         # Update parameters and take a step using the computed gradient.\n",
        "#         # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "#         # modified based on their gradients, the learning rate, etc.\n",
        "         optimizer.step()\n",
        "\n",
        "#         # Update the learning rate.\n",
        "         scheduler.step()\n",
        "\n",
        "#     # Calculate the average loss over the training data.\n",
        "     avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "#     # Store the loss value for plotting the learning curve.\n",
        "     loss_values.append(avg_train_loss)\n",
        "\n",
        "     print(\"\")\n",
        "     print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "     print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "#     # ========================================\n",
        "#     #               Validation\n",
        "#     # ========================================\n",
        "#     # After the completion of each training epoch, measure our performance on\n",
        "#     # our validation set.\n",
        "\n",
        "     print(\"\")\n",
        "     print(\"Running Validation...\")\n",
        "\n",
        "     t0 = time.time()\n",
        "\n",
        "#     # Put the model in evaluation mode--the dropout layers behave differently\n",
        "#     # during evaluation.\n",
        "     model.eval()\n",
        "\n",
        "#     # Tracking variables \n",
        "     eval_loss, eval_accuracy = 0, 0\n",
        "     nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "#     # Evaluate data for one epoch\n",
        "     for batch in validation_dataloader:\n",
        "        \n",
        "#         # Add batch to GPU\n",
        "         batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "#         # Unpack the inputs from our dataloader\n",
        "         b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "#         # Telling the model not to compute or store gradients, saving memory and\n",
        "#         # speeding up validation\n",
        "         with torch.no_grad():        \n",
        "\n",
        "#             # Forward pass, calculate logit predictions.\n",
        "#             # This will return the logits rather than the loss because we have\n",
        "#             # not provided labels.\n",
        "#             # token_type_ids is the same as the \"segment ids\", which \n",
        "#             # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "#             # The documentation for this `model` function is here: \n",
        "#             # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "             outputs = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask)\n",
        "        \n",
        "#         # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "#         # values prior to applying an activation function like the softmax.\n",
        "         logits = outputs[0]\n",
        "\n",
        "#         # Move logits and labels to CPU\n",
        "         logits = logits.detach().cpu().numpy()\n",
        "         label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "#         # Calculate the accuracy for this batch of test sentences.\n",
        "         tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "#         # Accumulate the total accuracy.\n",
        "         eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "#         # Track the number of batches\n",
        "         nb_eval_steps += 1\n",
        "\n",
        "#     # Report the final accuracy for this validation run.\n",
        "     print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "     print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        " print(\"\")\n",
        " print(\"Training complete!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    162.    Elapsed: 0:00:17.\n",
            "  Batch    80  of    162.    Elapsed: 0:00:34.\n",
            "  Batch   120  of    162.    Elapsed: 0:00:50.\n",
            "  Batch   160  of    162.    Elapsed: 0:01:07.\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:01:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    162.    Elapsed: 0:00:17.\n",
            "  Batch    80  of    162.    Elapsed: 0:00:34.\n",
            "  Batch   120  of    162.    Elapsed: 0:00:50.\n",
            "  Batch   160  of    162.    Elapsed: 0:01:07.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:01:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    162.    Elapsed: 0:00:17.\n",
            "  Batch    80  of    162.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    162.    Elapsed: 0:00:50.\n",
            "  Batch   160  of    162.    Elapsed: 0:01:07.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:01:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    162.    Elapsed: 0:00:17.\n",
            "  Batch    80  of    162.    Elapsed: 0:00:33.\n",
            "  Batch   120  of    162.    Elapsed: 0:00:50.\n",
            "  Batch   160  of    162.    Elapsed: 0:01:07.\n",
            "\n",
            "  Average training loss: 0.00\n",
            "  Training epcoh took: 0:01:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxpEzjyoHLBD",
        "colab_type": "code",
        "outputId": "9e0162b3-8baf-46d4-853f-84e9a81ba744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#df = pd.read_csv(\"../data/cola_public/raw/out_of_domain_dev.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Create sentence and label lists\n",
        "sentences = df.sentence.values\n",
        "labels = df.label.values\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
        "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(input_ids)\n",
        "prediction_masks = torch.tensor(attention_masks)\n",
        "prediction_labels = torch.tensor(labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test sentences: 516\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuEqajMQHLN8",
        "colab_type": "code",
        "outputId": "6823f93c-cb71-4811-cea9-c910a87d49ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 516 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJrctEa6IRkS",
        "colab_type": "code",
        "outputId": "eba2ad22-2b81-4086-80e5-3c383687f55a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive samples: 354 of 516 (68.60%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apfAqv-pIU4F",
        "colab_type": "code",
        "outputId": "b43e067b-c5e5-4f23-85af-2e8382812c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "    # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "    # in to a list of 0s and 1s.\n",
        "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "    # Calculate and store the coef for this batch.  \n",
        "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "    matthews_set.append(matthews)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtmBjOo0IX8M",
        "colab_type": "code",
        "outputId": "e415368c-4af8-4608-bce8-c9c0ca778667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "\n",
        "matthews_set"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2135744251723958,\n",
              " 0.17407765595569785,\n",
              " -0.21713222235566895,\n",
              " 0.16151457061744964,\n",
              " -0.0849411985729376,\n",
              " -0.16976428667549412,\n",
              " 0.041344911529736156,\n",
              " 0.0,\n",
              " 0.040128617695256406,\n",
              " 0.16870980440527691,\n",
              " 0.11235088294097073,\n",
              " 0.14907119849998599,\n",
              " 0.18367958959266126,\n",
              " 0.12403473458920847,\n",
              " 0.12403473458920847,\n",
              " -0.3567530340063379,\n",
              " 0.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv9i1Qm0Ic7J",
        "colab_type": "code",
        "outputId": "e253dd90-e456-4e80-cce7-5a36091a1516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MCC: 0.044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piLOFcte0i_7",
        "colab_type": "text"
      },
      "source": [
        "The LSTM model did not actually perform very well, but given that BERT couldn't even run, it was still better. The problem could be that the data size was too small or that labelling at the time was not very accurate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajUetvTuVU0U",
        "colab_type": "text"
      },
      "source": [
        "## Embeddings, Context Words\n",
        "\n",
        "We saw how a bootstrapped BERT model performed so much better than a model trained from scatch. Because BERT's method of capturing context is bidirectional, meaning that words can now have different word embedding values based on their location within a sentence. Let us use the same BERT model to capture sentence and word embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vNd5HgPVU0V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O405vSF7VU0X",
        "colab_type": "text"
      },
      "source": [
        "Let's go through the sentence format for the BERT model, as well as how our vocabulary looks like. Note that you have to use the BERT tokenizer to use the BERT model because of the similar vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aw0yGIeUVU0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt6xqV4uVU0d",
        "colab_type": "code",
        "outputId": "360c59be-dceb-4ab1-a14f-739f226ec32f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "print (tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIbYnyYEVU0g",
        "colab_type": "text"
      },
      "source": [
        "BERTS model uses a WordPiece technique to do its tokenizing, as described in the paper. That's why the word embedding is split up the way it is.\n",
        "A quick peek at what the voabulary looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0JTP9aGVU0g",
        "colab_type": "code",
        "outputId": "e096cc00-ce7f-4af3-f59d-87f7fced2d43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "list(tokenizer.vocab.keys())[6000:6030]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['peninsula',\n",
              " 'adults',\n",
              " 'novels',\n",
              " 'emerged',\n",
              " 'vienna',\n",
              " 'metro',\n",
              " 'debuted',\n",
              " 'shoes',\n",
              " 'tamil',\n",
              " 'songwriter',\n",
              " 'meets',\n",
              " 'prove',\n",
              " 'beating',\n",
              " 'instance',\n",
              " 'heaven',\n",
              " 'scared',\n",
              " 'sending',\n",
              " 'marks',\n",
              " 'artistic',\n",
              " 'passage',\n",
              " 'superior',\n",
              " '03',\n",
              " 'significantly',\n",
              " 'shopping',\n",
              " '##tive',\n",
              " 'retained',\n",
              " '##izing',\n",
              " 'malaysia',\n",
              " 'technique',\n",
              " 'cheeks']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7BEr_pLVU0k",
        "colab_type": "code",
        "outputId": "00039df9-0631-4c92-cda1-6b774cda4fde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
        "\n",
        "# Add the special tokens.\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Split the sentence into tokens.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Map the token strings to their vocabulary indeces.\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "# Display the words with their indices.\n",
        "for tup in zip(tokenized_text, indexed_tokens):\n",
        "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS]           101\n",
            "after         2,044\n",
            "stealing     11,065\n",
            "money         2,769\n",
            "from          2,013\n",
            "the           1,996\n",
            "bank          2,924\n",
            "vault        11,632\n",
            ",             1,010\n",
            "the           1,996\n",
            "bank          2,924\n",
            "robber       27,307\n",
            "was           2,001\n",
            "seen          2,464\n",
            "fishing       5,645\n",
            "on            2,006\n",
            "the           1,996\n",
            "mississippi   5,900\n",
            "river         2,314\n",
            "bank          2,924\n",
            ".             1,012\n",
            "[SEP]           102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi28IAogVU0n",
        "colab_type": "text"
      },
      "source": [
        "### Segment ID\n",
        "\n",
        "BERT is trained on and expects sentence pairs, using 1s and 0s to distinguish between the two sentences. That is, for each token in “tokenized_text,” we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence.\n",
        "\n",
        "If you want to process two sentences, assign each word in the first sentence plus the ‘[SEP]’ token a 0, and all tokens of the second sentence a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avaV3HWtVU0n",
        "colab_type": "code",
        "outputId": "5a9cf1ae-7f33-4a05-f4bd-29611de030de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "segments_ids = [1] * len(tokenized_text)\n",
        "\n",
        "print (segments_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dffdlhBVU0p",
        "colab_type": "text"
      },
      "source": [
        "Like we did for classification, we now convert these segments to tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfCr0P6zVU0q",
        "colab_type": "text"
      },
      "source": [
        "The embedding layer is the hidden state layer, and this is what we pick up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsMgYwMxVU0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhQ0_3ySVU0x",
        "colab_type": "code",
        "outputId": "d809589a-a782-4875-a2d8-6041024331ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_embedding = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model_embedding.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cghr2UKlVU0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output = model_embedding(tokens_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrfUJQGYVU01",
        "colab_type": "code",
        "outputId": "fc32dc15-9562-40db-f82e-2446c8ff04f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(output[0][0][0]), len(output[1][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GE6b7dsVU04",
        "colab_type": "text"
      },
      "source": [
        "### Understanding the Output\n",
        "\n",
        "This kind of forward pass returns us the last layer of the net, which we will use to make our vectors. The first object returned contains the batch number, followed by each of the tokens and their vector values. The second object contains a vector value, which I suspect is the sentence vector of the tokens. \n",
        "\n",
        "The first index is the batch size, and our batch size is 1, so we just choose the 0th index and work with that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLiCEaJvVU05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings, sentence_embedding = output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkrOD632VU07",
        "colab_type": "code",
        "outputId": "13471192-2373-4701-d61c-834513c2c7b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word_embeddings[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ostPnwbTVU0-",
        "colab_type": "code",
        "outputId": "6bb5db1b-ad15-413b-c07b-5dd0a61e8ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "word_embeddings[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
              "        [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
              "        [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
              "        ...,\n",
              "        [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
              "        [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
              "        [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]],\n",
              "       grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byjfFSSJVU1A",
        "colab_type": "text"
      },
      "source": [
        "Let’s take a quick look at the range of values for a given layer and token.\n",
        "\n",
        "You’ll find that the range is fairly similar for all layers and tokens, with the majority of values falling between [-2, 2], and a small smattering of values around -10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO4lBgYIVU1A",
        "colab_type": "code",
        "outputId": "cbcb2e04-1424-459f-c893-b25e5ecac04d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "source": [
        "vec = word_embeddings[0][0]\n",
        "vec = vec.detach().numpy()\n",
        "# Plot the values as a histogram to show their distribution.\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.hist(vec, bins=200)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAAJJCAYAAACUMFKbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5DddX3/8Rdks7u5LCTghloyyj2k\nRIRgsQGGUUklRGLSSgI6BLEWUdERiLXBYseKLQwkKUxBKEHSkDJiRWFhFHVqGaczREaNSFNWLBm1\nRAxZopJNwl6S7O+P/HabJYG95ORzcrKPxwwz7Pd7Lu+zn708c87Z7/eQnp6engAAUMSh1R4AAGAk\nEV8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCACiortoDvJ7f/W5rdu7cdRiyI48cn02btlR5IobD\n2tUm61a7rF1tsm6168gjxw/p8gd0fO3c2dMXX70fU5usXW2ybrXL2tUm6zYyeNkRAKAg8QUAUJD4\nAgAoSHwBABQkvgAAChJfAAAFiS8AgIIGPM7Xk08+mcsuu2yv+771rW/l+OOP7/t4zZo1ueWWW/LM\nM89k/PjxueCCC7Jo0aKMGTOmchMDANSwQR9k9YMf/GBOOeWUftuOOuqovv9vbW3N5ZdfnhNOOCGL\nFy/Ohg0bcu+992b9+vW56667KjcxAEANG3R8nXnmmZk5c+Zr7l+2bFkmTJiQVatWZdy4cUmSyZMn\n5/rrr8/q1aszY8aMfZ8WAKDGDek9X1u2bMn27dv3uv2JJ57IvHnz+sIrSebOnZuxY8fmscce2/dJ\nAQAOAoN+5uuv/uqvsm3bttTV1eXtb397/vqv/zpTpkxJkjz77LPZvn17pk2b1u869fX1mTp1alpb\nWys7NQBAjRowvkaPHp3zzz8/5557biZOnJhnn3029957bz7wgQ/kwQcfzLHHHpu2trYkSXNz8x7X\nb25uzlNPPTWs4V59lvDm5qZh3Q7VZ+1qk3WrXdauNlm3kWHA+Jo+fXqmT5/e9/F5552Xd73rXXnf\n+96X22+/PUuXLk1HR0eSXc90vVpDQ0Pf/qHatGlL3xnem5ub0tbWPqzbobqsXW2ybrXL2tUm61a7\nhhrNwzrO18knn5wZM2bkBz/4QZKksbExSdLV1bXHZTs7O/v2AwCMdMM+yOob3/jGvPzyy0n+7+XG\n3pcfd9fW1pZJkyYN924AAA4qw46v559/PhMnTkySnHTSSamrq8vatWv7Xaarqyutra2ZOnXqvk0J\nAHCQGDC+fvvb3+6x7Uc/+lGefPLJnHPOOUmSpqamzJgxIy0tLdm6dWvf5VpaWrJt27bMmjWrgiMD\nANSuAd9wf/XVV2fMmDE5/fTTM3HixPzP//xPvvrVr2bixIn55Cc/2Xe5a665JpdcckkWLlyY+fPn\nZ8OGDVmxYkXOPffcnHXWWfv1QQAA1IpDenp6el7vAvfdd18effTR/O///m+2bNmSI444Iuecc04+\n+clP5g//8A/7XfZHP/pRlixZ0ndux9mzZ+faa6/N2LFjhzWcv3Y8OFi72mTdape1q03WrXYN9a8d\nB4yvahJfBwdrV5usW+2ydrXJutWuIoeaAABgeMQXAEBBgz63IwAHjqbDxqSxYdeP8I7O7Wnf/EqV\nJwIGS3wB1KDGhrrMWdSSJHl06dx4pxDUDi87AgAUJL4AAAoSXwAABYkvAICCxBcAQEHiCwCgIPEF\nAFCQ+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCxBcAQEHiCwCgIPEFAFCQ+AIAKEh8AQAUJL4AAAoS\nXwAABYkvAICCxBcAQEHiCwCgIPEFAFCQ+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCxBcAQEF11R4A\ngMprOmxMGht2/Yjv6Nye9s2vVHkioJf4AjgINTbUZc6iliTJo0vnpr3K8wD/x8uOAAAFiS8AgILE\nFwBAQeILAKAg8QUAUJD4AgAoSHwBABQkvgAAChJfAAAFiS8AgILEFwBAQeILAKAg8QUAUJD4AgAo\nSHwBABQkvgAAChJfAAAFiS8AgILEFwBAQXXVHgCA6mk6bEwaG3b9Kujo3J72za9UeSI4+IkvgBGs\nsaEucxa1JEkeXTo37VWeB0YCLzsCABQkvgAAChJfAAAFiS8AgILEFwBAQeILAKAg8QUAUJD4AgAo\nSHwBABQkvgAOIk2Hjan2CMAAxBfAQWT30wUBBybxBQBQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCA\ngsQXAEBB4gsAoCDxBQBQUF21BwBg33R170hzc9OA+zs6t6d98ysFJwP2xjNfADWufvSozFnU8pqn\nFerd39jg39twIBBfAAAFiS8AgILEFwBAQeILAKAg8QUAUJD4AgAoSHwBABQkvgAACnLEPYAa0nTY\nGAdLhRrnmS+AGtLYUPeaR7IHaoP4AgAoSHwBABQkvgAAChpWfC1fvjxTpkzJ3Llz99i3Zs2avP/9\n789b3/rWnH322fniF7+YV155ZZ8HBQA4GAz5T2ba2tpy5513ZuzYsXvsa21tzeWXX54TTjghixcv\nzoYNG3Lvvfdm/fr1ueuuuyoyMABALRtyfC1dujTTpk1LT09PNm/e3G/fsmXLMmHChKxatSrjxo1L\nkkyePDnXX399Vq9enRkzZlRmagCAGjWklx2ffvrpPPLII7nuuuv22Ldly5Y88cQTmTdvXl94Jcnc\nuXMzduzYPPbYY/s+LQBAjRt0fPX09OSGG27IvHnzMnXq1D32P/vss9m+fXumTZvWb3t9fX2mTp2a\n1tbWfZ8WAKDGDTq+Hn744Tz33HO5+uqr97q/ra0tSdLc3LzHvubm5mzcuHGYIwIAHDwG9Z6vLVu2\nZOnSpfnIRz6SSZMm7fUyHR0dSXY90/VqDQ0NffuH4sgjx/f7uLm5aci3wYHB2tUm63bwGWhNrXl1\n+fyPDIOKrzvvvDOjR4/Ohz70ode8TGNjY5Kkq6trj32dnZ19+4di06Yt2bmzJ8muL8i2tvYh3wbV\nZ+1qk3U7MO3rL+dXr+mrb8+aV4/vudo11O/LAeNr48aNWblyZT71qU/lpZde6tve2dmZ7u7urF+/\nPk1NTX0vN/a+/Li7tra213zGDABgJBnwPV+bNm1Kd3d3lixZkvPOO6/vv5/+9KdZt25dzjvvvCxf\nvjwnnXRS6urqsnbt2n7X7+rqSmtr617fpA8AMNIM+MzX5MmTc8cdd+yx/dZbb822bdvy2c9+Nscc\nc0yampoyY8aMtLS05Morr+w73ERLS0u2bduWWbNmVX56AIAaM2B8NTU1ZebMmXtsX7lyZUaNGtVv\n3zXXXJNLLrkkCxcuzPz587Nhw4asWLEi5557bs4666zKTg4AUIMqemLtU045JStWrEh9fX1uvPHG\nfO1rX8uCBQty2223VfJuAABq1pBPL9Rr1apVe93+tre9LQ888MCwBwIAOJhV9JkvAABen/gCAChI\nfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCA\ngsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBBddUeAIDymg4bk8YG\nvwKgGjzzBTACNTbUZc6ilmqPASOS+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCxBcAQEHiCwCgIPEF\nAFCQ+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCxBcAQEHiCwCgIPEFAFCQ+AIAKEh8AQAUJL4AAAoS\nXwAABYkvAICCxBcAQEHiCwCgIPEFAFCQ+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCxBcAQEF11R4A\ngANL02Fj0tiw69dDR+f2tG9+pcoTwcFFfAHQT2NDXeYsakmSPLp0btqrPA8cbLzsCABQkPgCAChI\nfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCA\ngsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgC\nAChIfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFFQ30AX+67/+K3fd\ndVeeeeaZbNq0KU1NTTn55JNz1VVXZfr06f0uu2bNmtxyyy155plnMn78+FxwwQVZtGhRxowZs98e\nAABALRkwvp5//vns2LEj8+fPT3Nzc9rb2/Poo4/m0ksvzfLly3P22WcnSVpbW3P55ZfnhBNOyOLF\ni7Nhw4bce++9Wb9+fe666679/kAAAGrBgPE1e/bszJ49u9+297///Zk5c2buu+++vvhatmxZJkyY\nkFWrVmXcuHFJksmTJ+f666/P6tWrM2PGjP0wPgBAbRnWe77GjBmTI444Ips3b06SbNmyJU888UTm\nzZvXF15JMnfu3IwdOzaPPfZYZaYFAKhxAz7z1WvLli3p6urK73//+zz88MP5+c9/nquuuipJ8uyz\nz2b79u2ZNm1av+vU19dn6tSpaW1trezUAAA1atDx9dnPfjbf+c53kiSjR4/OJZdcko9+9KNJkra2\ntiRJc3PzHtdrbm7OU089Nazhjjxy/Ktuq2lYt0P1WbvaZN0OPgOt6d72+zoox+d6ZBh0fF111VW5\n+OKLs2HDhrS0tKSrqyvd3d2pr69PR0dHkl3PdL1aQ0ND3/6h2rRpS3bu7Emy6wuyra19WLdDdVm7\n2mTdDkz7+su5d01f63ba2tr32OfroAzfc7VrqN+Xg37P15QpU3L22Wfnfe97X7785S/nv//7v3Pd\nddclSRobG5MkXV1de1yvs7Ozbz8AwEg3rDfcjx49Ouedd16++93vpqOjo+/lxt6XH3fX1taWSZMm\n7duUAAAHiWEf4b6joyM9PT3ZunVrTjrppNTV1WXt2rX9LtPV1ZXW1tZMnTp1nwcFADgYDBhfv/3t\nb/fYtmXLlnznO9/JG9/4xhx55JFpamrKjBkz0tLSkq1bt/ZdrqWlJdu2bcusWbMqOzUAQI0a8A33\nV199dRoaGnL66aenubk5v/nNb/KNb3wjGzZsyLJly/oud8011+SSSy7JwoULM3/+/GzYsCErVqzI\nueeem7POOmu/PggAgFoxYHy9973vTUtLS1atWpXNmzenqakpp512Wm6++eaceeaZfZc75ZRTsmLF\niixZsiQ33nhjxo8fnwULFuTaa6/drw8AAKCWDBhfF110US666KJB3djb3va2PPDAA/s8FADAwWrY\nb7gHAGDoxBcAQEHiCwCgIPEFAFCQ+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCxBcAQEHiCwCgIPEF\nAFDQgCfWBqCcpsPGpLFh14/mjs7tad/8SpUnAipNfAEcQBob6jJnUUuS5NGlc9Ne5XmAyvOyIwBA\nQeILAKAg8QUAUJD4AgAoSHwBABQkvgAAChJfAAAFiS8AgILEFwBAQY5wD3CA2/2UQ0Dt88wXwAGu\n95RDvacdAmqb+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCxBcAQEHiCwCgIPEFAFCQ+AIAKMj5KgBG\niK7uHWlubhrWdTo6t6d98yv7aTIYWTzzBTBC1I8eNeTTFPVex7kloXLEFwBAQeILAKAg8QUAUJD4\nAgAoSHwBABQkvgAAChJfAAAFiS8AgILEFwBAQeILAKAg8QUAUJD4AgAoSHwBABQkvgAAChJfAAAF\niS8AgILEFwBAQeILAKAg8QUAUJD4AgAoSHwBABQkvgAAChJfAAAFiS8AgILEFwBAQeILAKAg8QUA\nUJD4AgAoSHwBABRUV+0BAEaapsPGpLGhLh2d29O++ZV+23bX1b0jzc1N1RgR2I888wVQWGNDXeYs\naukXW73bdlc/etQe24DaJ74AAAoSXwAABYkvAICCxBcAQEHiCwCgIPEFAFCQ+AIAKEh8AQAUJL4A\nAApyeiGAKnH6IBiZPPMFUCW9pw9yCiEYWcQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFCS+AAAKEl8A\nAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFFQ30AWefvrpPPTQQ3nyySfzwgsvZMKECTn9\n9NNz9dVX581vfnO/y65Zsya33HJLnnnmmYwfPz4XXHBBFi1alDFjxuy3BwAAUEsGjK977rkna9as\nyaxZszJlypS0tbXl/vvvz7x58/Lggw/m+OOPT5K0trbm8ssvzwknnJDFixdnw4YNuffee7N+/frc\ndddd+/2BAADUggHj6/LLL8+SJUtSX1/ft2327NmZM2dOli9fnptuuilJsmzZskyYMCGrVq3KuHHj\nkiSTJ0/O9ddfn9WrV2fGjBn76SEAANSOAd/zNX369H7hlSTHHHNMTjzxxKxbty5JsmXLljzxxBOZ\nN29eX3glydy5czN27Ng89thjFR4bAKA2DesN9z09PXnppZcyceLEJMmzzz6b7du3Z9q0af0uV19f\nn6lTp6a1tXXfJwUAOAgMK74eeeSRvPjii7nggguSJG1tbUmS5ubmPS7b3NycjRs37sOIAAAHjwHf\n8/Vq69atyxe+8IWcccYZmTt3bpKko6MjSfZ4eTJJGhoa+vYP1ZFHju/3cXNz07Buh+qzdrXJuo08\nr7fmvh72P5/jkWFI8dXW1pYrr7wyhx9+eG677bYceuiuJ84aGxuTJF1dXXtcp7Ozs2//UG3atCU7\nd/Yk2fUF2dbWPqzbobqsXW2ybvvPgfwLtq2t/TXn8/Wwf/meq11D/Z4edHy1t7fniiuuSHt7e77y\nla/0e4mx9/97X37cXVtbWyZNmjSkoQAADlaDes9XZ2dnPvrRj+aXv/xl/vmf/znHHXdcv/0nnXRS\n6urqsnbt2n7bu7q60tramqlTp1ZuYgCAGjZgfO3YsSNXX311nnrqqdx222057bTT9rhMU1NTZsyY\nkZaWlmzdurVve0tLS7Zt25ZZs2ZVdmoAgBo14MuON910U/7jP/4j73znO/P73/8+LS0tffvGjRuX\nmTNnJkmuueaaXHLJJVm4cGHmz5+fDRs2ZMWKFTn33HNz1lln7b9HAABQQwaMr5/97GdJkscffzyP\nP/54v31HH310X3ydcsopWbFiRZYsWZIbb7wx48ePz4IFC3Lttdfuh7EBAGrTgPG1atWqQd/Y2972\ntjzwwAP7NBAAwMFsWAdZBQBgeMQXAEBB4gsAoCDxBUCSpKt7xwF99H04WIgvAJIk9aNHZc6iloEv\nCOwT8QUAUJD4AgAoSHwBABQkvgAAChJfAAAFiS8AgILEFwBAQeILAKCgumoPAHAwazpsTBob6tLR\nuT3tm1+p9jgV1fvYkhyUjw/2F898AexHjQ11mbOopS9SDia9j+1gfXywv4gvAICCxBcAQEHiCwCg\nIPEFAFCQ+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCHJIYgAF1de9Ic3NTEqcSgn0lvgAYUP3oUZmz\nqCVJ8ujSuWmv8jxQy7zsCABQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQ\nkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFCS+AAAKEl8A\nAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoCDx\nBQBQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoKC6ag8AMBJ0de9Ic3NTtcfYb3of\nX0fn9rRvfqXa48ABzTNfAAXUjx6VOYtaMmdRS7VH2S96H19jg3/Tw0DEFwBAQeILAKAg8QUAUJD4\nAgAoSHwBABQkvgAAChJfAAAFiS8AgILEFwBAQeILAKAg8QUAUJD4AgAoSHwBABQkvgAAChJfAAAF\niS8AgILEFwBAQeILAKAg8QUAUFBdtQcA4ODR1b0jzc1NSZKOzu1p3/xKlSeCA4/4AqBi6kePypxF\nLUmSR5fOTXuV54EDkZcdAQAKEl8AAAWJLwCAggYVXxs3bsySJUuycOHCnH766ZkyZUqefPLJvV72\ne9/7Xv7sz/4sb3nLW/KOd7wjt99+e7Zv317RoQEAatWg4usXv/hFli9fnhdffDFTpkx5zct9//vf\nz1VXXZXDDz88n/vc5zJz5szccccdufHGGys2MABALRvUXzuecsop+cEPfpCJEyfm3//933PVVVft\n9XI333xz/uiP/ihf/vKXM2rUqCTJuHHjcvfdd2fhwoU55phjKjY4AEAtGtQzX+PHj8/EiRNf9zLP\nPfdcnnvuuVx88cV94ZUkH/jAB7Jz585897vf3bdJAQAOAhV7w/0zzzyTJJk2bVq/7UcddVT+4A/+\noG8/AMBIVrH4amtrS5I0Nzfvsa+5uTkbN26s1F0BANSsih3hvqOjI0lSX1+/x76Ghoa88srQTzFx\n5JHj+33ce8oKao+1q03Wjdcy2K8NX0ND4/M1MlQsvhobG5MkXV1de+zr7Ozs2z8UmzZtyc6dPUl2\nfUG2tTlRRS2ydrXJulXGwfrLtK2tfVCPzdfQ4Pmeq11D/T6v2MuOvS839r78uLu2trZMmjSpUncF\nAFCzKhZfU6dOTZKsXbu23/YXX3wxGzZs6NsPADCSVSy+TjzxxBx33HH56le/mh07dvRt/8pXvpJD\nDz007373uyt1VwAANWvQ7/n60pe+lCRZt25dkqSlpSU//vGPc9hhh+XSSy9NknzmM5/Jxz72sXz4\nwx/O7Nmz8/Of/zz3339/Lr744hx77LH7YXwAgNoy6Pi67bbb+n389a9/PUly9NFH98XXO9/5ztx+\n++25/fbbc8MNN+SII47Ixz72sXz84x+v4MgAALVr0PH17LPPDupyM2fOzMyZM4c9EADAwaxi7/kC\nAGBg4gsAoCDxBQBQkPgCAChIfAEAFCS+AAAKEl8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCAChI\nfAEAFFRX7QEAalnTYWPS2LDrR2lH5/a0b36lyhMBBzrxBbAPGhvqMmdRS5Lk0aVz017leYADn5cd\nAQAKEl8AAAWJLwCAgsQXAEBB4gsAoCDxBQBQkPgCAChIfAEAFOQgqwAV0tW9I83NTY50///1fj6S\npLNrRxrqR/ncQDzzBVAx9aNHZc6ilr7TDY10vZ+POYta0lDvcwO9xBcAQEHiCwCgIPEFAFCQ+AIA\nKEh8AQAUJL4AAAoSXwAABYkvAICCxBcAQEHiC4Ah2f20QcDQiS8AhqT3tEHA8IgvAICCxBcAQEHi\nCwCgIPEFAFCQ+AIAKEh8AQAUJL4AAAoSXwAABdVVewAARo7dj47f0bk97ZtfqfJEUJ74AqCY3Y+O\n/+jSuWmv8jxQDV52BAAoSHwBABQkvgAAChJfAAAFiS8AgILEFwBAQeILAKAg8QUAUJD4AgAoyBHu\nAV5H02Fj0tiw60dl7+lwdt8GMFR+egC8jsaGuj1Oh/PqbQBD4WVHAICCxBcAQEHiCwCgIPEFAFCQ\n+AIAKEh8AQAUJL4AAAoSXwAABYkvAICCHOEeoMK6unekubmp2mMc8Ho/T72nbUr+73ROu2/rtbdT\nPUEt8swXQIXVjx6VOYta+k5BxN71fp52P09m76mb9nbuzN59r7UfaoX4AgAoSHwBABQkvgAAChJf\nAAAFiS8AgILEFwBAQeILAKAg8QUAUJCj1EGNe70jgrPLcI6Mvvt1ejlyPVAJnvmCGvd6RwRnl+Ec\nGb33OrvrPSI7wL4QXwAABYkvAICCxBcAQEHiCwCgIPEFAFCQ+AIAKEh8AQAUJL4AAAoSXwAABY3o\nQ2IP55QjDE8tfK6Hc5qefTm1Ty2dFqjE+u1+H51dO9JQP2q/3FfvKYJ2v+1aWouD0d5O27T7tt6v\nh31lnUeWA3m9R/QzX8M55QjDUwuf6+GcpmdfTu1TS6cFKrF+u99HQ/2o/XZfvacI2v22a2ktDka9\na7L7qZt239b79bCvrPPIciCv94iOLwCA0sQXAEBB4gsAoKCKx1dXV1duueWWnHPOOTn11FOzYMGC\nrF69utJ3AwBQkyoeX4sXL87KlSvz3ve+N3/zN3+TQw89NFdccUV+8pOfVPquAABqTkXj6+mnn843\nv/nNfPrTn85nPvOZXHzxxVm5cmXe+MY3ZsmSJZW8KwCAmlTR+Pr2t7+d0aNHZ/78+X3bGhoactFF\nF+XHP/5xNm7cWMm7AwCoORU9+EVra2uOPfbYjBs3rt/2U089NT09PWltbc2kSZMGfXuHHnrI635c\nCZMmjtmvt8//qYXPde+MQ5lvONepxHX3x+282u63V2L9dr+PSj+mgW779bbt7bp72zac61R624Ew\nQ6m5qvk9t78cqHPVqgN1vQ/p6enpqdSNXXjhhTnqqKPy5S9/ud/25557Lu95z3vyxS9+sd+zYgAA\nI01FX3bs6OjI6NGj99je0NCQJOns7Kzk3QEA1JyKxldjY2O6u7v32N4bXb0RBgAwUlU0vpqbm/f6\npvq2trYkGdL7vQAADkYVja+TTz45v/jFL7J169Z+23/605/27QcAGMkqGl+zZs1Kd3d3vva1r/Vt\n6+rqyje+8Y1Mnz49Rx11VCXvDgCg5lT0UBNvfetbM2vWrCxZsiRtbW1505velIceeigvvPBCbrzx\nxkreFQBATarooSaSXW+uv/XWW/Poo4/m5ZdfzpQpU3LttdfmrLPOquTdAADUpIrHFwAAr63iJ9YG\nAOC1iS8AgIJqOr7+8z//MwsXLsz06dMzffr0zJ8/P9///verPRaD9PnPfz5TpkzJxz/+8WqPwgBW\nr16d6667Lueff37e+ta3ZubMmfnc5z7Xdww/qq+rqyu33HJLzjnnnJx66qlZsGBBVq9eXe2xeB1P\nP/10/u7v/i6zZ8/Oaaedlne84x255ppr8qtf/araozFEy5cvz5QpUzJ37txBXb6if+1Y0r/927/l\nb//2b/Pud787n/nMZ7Jjx44899xz2bBhQ7VHYxB+9rOf5cEHH3TWgxpxyy235OWXX86sWbNyzDHH\n5Pnnn8+//uu/5vHHH09LS8sw6jMAAAb/SURBVEuOPPLIao844i1evDjf/e53c9lll+XNb35zHnro\noVxxxRVZtWpVTj/99GqPx17cc889WbNmTWbNmpUpU6akra0t999/f+bNm5cHH3wwxx9/fLVHZBDa\n2tpy5513ZuzYsYO+Tk2+4f7555/Pe97znlx77bW5/PLLqz0Ow7Bw4cJMnjw5Tz75ZE4++eR86Utf\nqvZIvI4f/vCHOeOMM3LooYf223bppZfmE5/4RD75yU9WcTqefvrpzJ8/P9ddd13fz8TOzs5ceOGF\nmTRpUu6///7qDsherVmzJtOmTUt9fX3ftl/+8peZM2dO3vOe9+Smm26q4nQM1uLFi/PCCy+kp6cn\nmzdvTktLy4DXqcmXHR944IEcdthhueyyy9LT05MtW7ZUeySG4LHHHsvatWtzzTXXVHsUBumP//iP\n+4VX77YJEyZk3bp1VZqKXt/+9rczevTozJ8/v29bQ0NDLrroovz4xz/e62nfqL7p06f3C68kOeaY\nY3LiiSf6vqoRTz/9dB555JFcd911Q7peTcbX6tWr85a3vCX33Xdf/uRP/iRnnHFGzj777KxYsaLa\nozGAjo6O3HzzzfnLv/xL5/qscVu3bs3WrVszceLEao8y4rW2tubYY4/NuHHj+m0/9dRT09PTk9bW\n1ipNxlD19PTkpZde8n1VA3p6enLDDTdk3rx5mTp16pCuW5Pv+frVr36VX//61/nBD36Qq666KpMn\nT05LS0tuuummHHLIIV6KPIDdc8896enpyYc//OFqj8I+WrlyZbq7u3PBBRdUe5QRr62tba+nb2tu\nbk4Sz3zVkEceeSQvvviiVwZqwMMPP5znnnsud9xxx5CvW/X42rlzZ7q7uwd12d43Z2/bti07d+7M\nP/7jP2b27NlJkvPPPz8LFizIXXfdlYULF2bUqFH7bWaGt24vvPBCli9fnhtuuCGNjY37czxex3DW\n7tV++MMf5o477siFF16YM888s5LjMQwdHR0ZPXr0Htt716+zs7P0SAzDunXr8oUvfCFnnHHGoP9q\njurYsmVLli5dmo985CPDehWn6vH1wx/+MJdddtmgLrt69eocccQRaWxsTHd3d84///y+fYccckgu\nvPDC/MM//EN+8Ytf5IQTTthfI5PhrdvNN9+ck046KXPmzNnP0/F6hrN2u1u3bl0+8YlPZMqUKbnh\nhhv2x4gMUe/PxFfrjS5/VXzga2try5VXXpnDDz88t9122x7vseTAcuedd2b06NH50Ic+NKzrVz2+\njjvuuEGfdHv8+PFJdj2Vvm3btj2e3XrDG96QJHn55ZcrOyR7GOq6rV27No899liWLFmSX//61337\ntm/fno6Ojqxfvz4TJkzoW2P2n+F8z/X6zW9+kw9/+MNpamrK3XffPaQ/rWb/aW5u3utLi73HYfP+\nygNbe3t7rrjiirS3t+crX/lK38vFHJg2btyYlStX5lOf+lReeumlvu2dnZ3p7u7O+vXr09TUlMMP\nP/w1b6Pq8dXc3Jw///M/H9J1TjnllHznO99JV1dXv78U6T3G16v/pU7lDXXdetfm05/+9B77Xnzx\nxZx33nn5/Oc/n/e///0Vm5G9G873XJL87ne/y1/8xV+kq6srK1eu7PvHDtV38sknZ9WqVdm6dWu/\nN93/9Kc/7dvPgamzszMf/ehH88tf/jL/8i//kuOOO67aIzGATZs2pbu7O0uWLMmSJUv22H/eeefl\niiuu2Ovvu15Vj6/hmDVrVr71rW/l4YcfzoIFC5Ik3d3defjhh3P00UfnmGOOqe6A7OHUU0/d65sS\nP/e5z2Xy5Mm58sor/YI4gG3bti0f+chH8uKLL+a+++7Lm9/85mqPxG5mzZqVe++9N1/72tf6/uCo\nq6sr3/jGNzJ9+vS9vhmf6tuxY0euvvrqPPXUU/nSl76U0047rdojMQiTJ0/e6++zW2+9Ndu2bctn\nP/vZATukJg+y2tPTk8suuyw/+clPcumll2by5Mn55je/mZ/85CdZtmxZ35vwOfC9613vcpDVGvDx\nj3883/ve9/K+970vb3/72/vte8Mb3pCzzz67SpPR61Of+lS+973v5YMf/GDe9KY35aGHHsratWuz\ncuXKnHHGGdUej734+7//+9x333155zvfucdfDY8bNy4zZ86s0mQMx8KFCwd9kNWafObrkEMOyZ13\n3plly5blkUceyebNm3PiiSfmn/7pn/Knf/qn1R4PDjo/+9nPkiRf//rX8/Wvf73fvjPPPFN8HQBu\nvvnm3HrrrWlpacnLL7+cKVOm5O677xZeB7De76vHH388jz/+eL99Rx99tPg6iNXkM18AALXK37IC\nABQkvgAAChJfAAAFiS8AgILEFwBAQeILAKAg8QUAUJD4AgAoSHwBABQkvgAACvp/PcWsUY20a1oA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4d_2KqlVU1C",
        "colab_type": "text"
      },
      "source": [
        "These values are grouped by layer - we can use the permute function to make it grouped by each individual token instead. Let us look at what the later looks like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPGyyBVxVU1D",
        "colab_type": "text"
      },
      "source": [
        "### Word Vectors\n",
        "\n",
        "So each of those tokens have embedding values - let us try and compare them with each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgibaKhyVU1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "token_vecs = []\n",
        "# For each token in the sentence...\n",
        "for embedding in word_embeddings[0]:\n",
        "    cat_vec = embedding.detach().numpy()\n",
        "    # Use `cat_vec` to represent `token`.\n",
        "    token_vecs.append(cat_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFq_xHzuVU1H",
        "colab_type": "code",
        "outputId": "5c4fd04f-8d22-4734-a545-313425618a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " token_vecs[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-4.96443480e-01, -1.83083862e-01, -5.23144543e-01,  5.25867820e-01,\n",
              "         5.07567286e-01,  1.63771078e-01,  2.03400478e-01,  3.17120552e-01,\n",
              "        -5.31788357e-02, -1.73957199e-01,  1.56624570e-01, -2.91166812e-01,\n",
              "        -4.70112801e-01,  6.43650353e-01,  1.01684511e-01,  4.04032469e-02,\n",
              "        -2.23800391e-01,  4.66817111e-01,  7.84237146e-01, -2.29057565e-01,\n",
              "        -1.18187547e-01, -1.04612954e-01,  2.05792636e-01,  1.56229854e-01,\n",
              "        -3.35294753e-02, -1.64334595e-01, -3.00018519e-01, -9.57756490e-02,\n",
              "        -9.04611200e-02,  3.83675098e-01,  5.08683249e-02,  5.76396957e-02,\n",
              "        -1.03359692e-01, -8.35441649e-01,  1.45372763e-01, -3.95008534e-01,\n",
              "         4.80392948e-02, -1.43011183e-01,  4.68772948e-02,  3.04917902e-01,\n",
              "        -3.79327983e-01,  8.92916843e-02, -2.46660322e-01,  4.72975671e-02,\n",
              "         2.10815221e-01, -6.77734256e-01, -3.22645330e+00, -7.80529007e-02,\n",
              "        -2.20757365e-01, -2.99443126e-01,  8.09336007e-02,  2.65430123e-01,\n",
              "        -1.86352000e-01,  3.39112550e-01, -2.95594126e-01,  1.01933919e-01,\n",
              "        -2.37938061e-01,  3.40985388e-01,  1.73496604e-01,  6.41646460e-02,\n",
              "         2.12839916e-01, -1.02881238e-01,  6.68158606e-02, -2.90524334e-01,\n",
              "        -5.86458802e-01, -1.40753668e-02,  9.18535888e-02,  2.60409743e-01,\n",
              "        -3.46390866e-02,  1.13274097e-01, -4.20777500e-01,  2.92496430e-03,\n",
              "         1.86433062e-01, -1.15658678e-01,  3.70337874e-01,  2.28536472e-01,\n",
              "         1.01425931e-01,  9.77128074e-02, -4.22397792e-01,  2.88411826e-02,\n",
              "        -1.39281049e-01,  4.90712643e-01,  3.30015808e-01,  3.56563747e-01,\n",
              "         6.02087192e-02, -1.37127697e-01, -4.12591189e-01, -4.09708060e-02,\n",
              "         1.91048384e-01,  6.27252400e-01, -4.45484698e-01,  2.80430198e-01,\n",
              "        -1.61857128e-01,  2.41101548e-01, -1.14539579e-01,  3.70033801e-01,\n",
              "        -2.78448015e-02,  5.79997785e-02,  3.52648348e-02,  2.06929669e-01,\n",
              "         3.24672051e-02, -3.77573147e-02,  2.44730096e-02, -5.84430635e-01,\n",
              "         3.75072539e-01, -9.73653123e-02,  3.37925218e-02, -3.39242429e-01,\n",
              "        -8.83096680e-02, -2.36735368e+00, -2.44131267e-01,  1.68392465e-01,\n",
              "         1.92312166e-01, -8.40701699e-01,  1.19921125e-01,  6.06657267e-01,\n",
              "         5.76106429e-01, -8.18123817e-02, -3.63107532e-01, -6.47368431e-02,\n",
              "         1.74297184e-01,  2.11351097e-01,  2.39222422e-01,  3.49824913e-02,\n",
              "        -1.17697924e-01, -1.35092273e-01,  2.89797178e-03,  1.00747474e-01,\n",
              "         5.02799377e-02,  1.39894143e-01,  1.58913359e-01,  3.33098695e-02,\n",
              "        -7.40756318e-02, -2.71180093e-01, -4.39655513e-01,  2.46955410e-01,\n",
              "        -3.88266370e-02,  2.53998697e-01, -5.09628594e-01, -1.18572265e-01,\n",
              "        -3.43539953e-01, -6.48569167e-01, -2.96182895e+00, -1.19460717e-01,\n",
              "         1.78044736e-01,  5.04842877e-01,  4.15470898e-01,  3.66170108e-01,\n",
              "         1.89367216e-02,  3.48847777e-01,  2.14875862e-01,  1.12747349e-01,\n",
              "        -1.22770779e-01,  1.13436125e-01,  5.68660855e-01,  6.73133850e-01,\n",
              "         2.12556317e-01, -6.23406172e-02, -4.28857165e-04,  3.46716851e-01,\n",
              "         8.73257339e-01,  3.91273320e-01, -5.11024892e-01, -8.81830379e-02,\n",
              "        -3.18105906e-01,  6.06252491e-01,  5.81328988e-01, -4.57720365e-03,\n",
              "        -4.93934155e-01,  2.34801829e-01,  3.47466707e-01,  4.05476451e-01,\n",
              "         6.58729732e-01, -1.09048560e-01,  4.64046448e-01,  3.14285532e-02,\n",
              "        -3.29006374e-01,  3.73139948e-01,  3.19356769e-01, -2.35989451e-01,\n",
              "        -5.52734077e-01,  3.43990743e-01,  5.05866587e-01,  2.14512706e-01,\n",
              "         1.72335729e-01, -9.51977521e-02, -9.23417322e-03, -1.93522230e-01,\n",
              "        -6.30806625e-01,  6.20591976e-02, -3.20225246e-02, -3.21920365e-01,\n",
              "        -1.29654616e-01, -2.76218534e-01, -8.27538073e-02, -1.65851191e-01,\n",
              "         2.17253640e-01, -4.60050732e-01,  1.11732241e-02, -2.27732421e-03,\n",
              "        -5.08396685e-01,  1.49069384e-01,  1.54944304e-02, -2.92756017e-02,\n",
              "        -2.41469428e-01,  3.55890679e+00,  2.38879129e-01,  1.67586058e-01,\n",
              "         2.10127175e-01,  2.13954166e-01, -2.59500854e-02, -2.61487097e-01,\n",
              "        -1.28857762e-01,  2.22951192e-02,  5.55173978e-02,  3.39359373e-01,\n",
              "        -5.46073727e-02, -1.74488530e-01,  3.18549395e-01, -2.23722577e-01,\n",
              "         6.94601312e-02,  6.50311649e-01, -5.74242413e-01,  2.16190323e-01,\n",
              "        -2.73740888e-01,  5.77723920e-01, -6.46596774e-02,  5.53966880e-01,\n",
              "        -1.97370470e-01, -1.45811939e+00, -1.81662589e-02, -2.75657654e-01,\n",
              "        -7.71310866e-01, -2.34643221e-01, -7.76636004e-01, -9.06061158e-02,\n",
              "         1.02008931e-01, -3.24134640e-02, -4.07866240e-01, -6.61822200e-01,\n",
              "         1.55555874e-01,  1.60321936e-01, -7.62926787e-02,  1.76264644e-01,\n",
              "         2.40890518e-01,  3.29033971e-01,  2.78907955e-01,  2.30351627e-01,\n",
              "        -1.11648962e-01,  8.92521292e-02, -3.11395466e-01,  6.65649533e-01,\n",
              "         1.41427457e-01, -1.82343259e-01,  3.08778077e-01,  1.47625152e-02,\n",
              "         9.06254202e-02,  1.61282286e-01, -3.39173675e-01, -3.43498707e-01,\n",
              "         2.24695392e-02, -2.60535930e-03, -2.00275123e-01,  3.26455444e-01,\n",
              "        -6.27784550e-01,  1.27019165e-02, -7.24707767e-02, -4.45065588e-01,\n",
              "         5.87643683e-02, -9.89589170e-02,  2.31020689e-01,  1.32828265e-01,\n",
              "        -3.63016695e-01, -2.72315121e+00, -8.08233544e-02, -2.72067249e-01,\n",
              "         2.45523930e-01,  1.77642822e-01,  2.64398530e-02, -1.89721718e-01,\n",
              "         5.20491064e-01,  2.57500648e-01, -3.94501358e-01,  1.98993683e-02,\n",
              "         2.23253429e-01, -5.22476137e-01, -4.00060833e-01, -3.04477453e-01,\n",
              "         1.53196022e-01, -3.36015552e-01, -3.94786298e-01,  1.19548760e-01,\n",
              "        -4.02424961e-01,  1.78518429e-01, -9.36367884e-02, -2.89806575e-02,\n",
              "         3.68312657e-01,  5.53866386e-01,  4.30189013e-01, -2.76178718e-01,\n",
              "        -4.23182607e-01,  5.99044979e-01,  2.65435092e-02,  1.91163085e-02,\n",
              "        -1.91422522e-01,  1.08966909e-01,  7.78571516e-02, -2.34300330e-01,\n",
              "        -3.96282959e+00,  1.02426708e+00,  5.99136949e-02, -8.55004340e-02,\n",
              "         5.02325892e-01,  2.04032183e-01,  3.51960212e-01, -3.17075849e-01,\n",
              "        -3.89082253e-01,  4.70760018e-01,  4.26983647e-02, -3.79290342e-01,\n",
              "         3.03365082e-01,  1.87844709e-01,  3.48927587e-01, -3.80804390e-01,\n",
              "         3.68183732e-01, -4.36544031e-01,  3.78773898e-01,  2.68184453e-01,\n",
              "        -1.02129191e-01, -4.94785756e-01, -2.75882930e-01, -5.91668785e-02,\n",
              "         3.41152579e-01,  7.68275112e-02, -1.03112352e+00, -1.85486928e-01,\n",
              "         1.67060882e-01,  2.39740387e-02,  2.47842938e-01, -3.65591168e-01,\n",
              "        -2.93118119e-01, -1.29541293e-01,  1.26234025e-01, -9.07170102e-02,\n",
              "        -2.74631083e-01, -1.08266219e-01,  2.33825862e-01,  5.80821812e-01,\n",
              "        -3.29433054e-01,  6.93497777e-01, -8.13419223e-02,  7.20322490e-01,\n",
              "         1.01221502e+00,  4.04893368e-01, -3.17409560e-02, -2.60511607e-01,\n",
              "         1.80479009e-02,  2.52404392e-01,  2.69827783e-01,  1.37816563e-01,\n",
              "         1.01267648e+00,  5.00430167e-01, -7.46682193e-03, -3.21982533e-01,\n",
              "         3.29819262e-01, -2.53469229e-01, -3.22095454e-01,  2.98502743e-01,\n",
              "         3.12732041e-01, -6.23661220e-01,  4.27779853e-01, -1.97812125e-01,\n",
              "         6.07190847e-01, -7.74273455e-01,  6.67049170e-01, -3.99002552e-01,\n",
              "         7.89432004e-02, -2.88655460e-01, -7.32143745e-02,  5.30469239e-01,\n",
              "        -2.34022588e-01, -1.41889918e+00,  7.40583092e-02,  2.44443417e-01,\n",
              "         6.31262124e-01, -1.24745384e-01, -1.79785118e-03, -1.23219378e-01,\n",
              "        -2.05152363e-01,  1.99713036e-01,  2.49915183e-01,  3.43321830e-01,\n",
              "        -3.64774197e-01, -1.24131218e-01,  3.42789799e-01,  2.20201403e-01,\n",
              "        -5.82586884e-01, -1.41372189e-01,  2.93624401e-01, -5.85335754e-02,\n",
              "         2.27580816e-01,  1.93610147e-03, -4.95776206e-01, -1.26534654e-02,\n",
              "         3.64383638e-01, -9.04033840e-01,  4.27582473e-01,  3.69858108e-02,\n",
              "        -2.64229059e-01, -2.99502730e-01, -1.12824686e-01, -1.59777656e-01,\n",
              "        -2.99163848e-01, -5.11969924e-01,  3.49827498e-01,  5.66638649e-01,\n",
              "        -2.36470461e-01,  2.19377607e-01, -2.56921560e-01, -3.64015490e-01,\n",
              "         3.59212816e-01,  2.25017533e-01,  9.33584094e-01,  2.55949259e-01,\n",
              "         4.32752341e-01,  6.76164627e-01,  5.66852354e-02,  2.27903262e-01,\n",
              "         3.21117043e-01, -4.47684852e-03, -2.51164973e-01, -7.23802149e-02,\n",
              "        -1.96479246e-01,  2.16304645e-01,  2.00830996e-01, -2.41959065e-01,\n",
              "        -5.57882851e-03,  8.04368183e-02,  3.82947475e-01, -1.18751779e-01,\n",
              "        -5.09832501e-01, -3.66203010e-01,  7.99692422e-02, -1.29492566e-01,\n",
              "        -3.73944081e-02, -3.55980635e-01,  2.85069615e-01,  5.11699855e-01,\n",
              "         1.03834487e-01,  3.16135027e-02, -3.85969341e-01, -1.39063761e-01,\n",
              "        -3.33120614e-01,  1.53776407e-01,  2.71703869e-01,  4.88241225e-01,\n",
              "        -8.79787356e-02,  3.10290456e-01, -2.76035458e-01,  4.46957141e-01,\n",
              "         1.11639507e-01, -5.48905551e-01, -2.81095088e-01, -3.66535294e-03,\n",
              "         7.32386410e-02, -4.89853360e-02, -3.25320721e-01,  4.09820735e-01,\n",
              "         4.24422532e-01,  1.54733002e-01, -1.91867566e+00,  4.66618448e-01,\n",
              "         2.85239681e-03, -7.39024254e-03, -4.67612445e-01, -1.21222667e-01,\n",
              "        -1.23870470e-01,  6.69878185e-01, -4.07640249e-01, -7.27496371e-02,\n",
              "        -1.61380321e-01,  4.96555805e-01,  2.90475816e-01, -9.47676152e-02,\n",
              "        -7.97316357e-02,  1.23873375e-01, -4.80703324e-01, -2.03851521e-01,\n",
              "         2.63725430e-01,  2.69227266e-01, -5.31632423e-01,  4.17398930e-01,\n",
              "         9.72760916e-02, -5.89068115e-01, -6.50447533e-02, -6.52156770e-02,\n",
              "        -4.44527358e-01,  5.09543538e-01, -2.07915772e-02,  7.02583432e-01,\n",
              "         7.02653229e-02,  2.93231960e-02, -9.89862204e-01, -4.19287026e-01,\n",
              "         5.24407588e-02,  3.35501015e-01,  4.34899598e-01,  1.98769838e-01,\n",
              "        -4.09349836e-02, -1.30154401e-01, -4.85866159e-01,  3.10661852e-01,\n",
              "         3.36619198e-01, -1.92345828e-02,  4.74303454e-01,  1.07067473e-01,\n",
              "        -2.26985559e-01,  7.19780982e-01, -7.78148025e-02, -1.70015201e-01,\n",
              "        -4.97130871e-01, -4.85323608e-01, -8.94770473e-02,  1.40165105e-01,\n",
              "         5.59836745e-01,  9.49454010e-02,  2.36711755e-01, -4.08689678e-02,\n",
              "        -3.96002531e-01,  1.31414076e-02,  2.45329365e-01, -2.86562383e-01,\n",
              "        -4.79656011e-02,  2.90178210e-01,  2.43968800e-01, -3.92223746e-01,\n",
              "         1.72341257e-01,  1.14619575e-01, -1.44450217e-01,  3.46212775e-01,\n",
              "         5.00817418e-01,  3.15456778e-01, -2.68809646e-01,  1.97000086e-01,\n",
              "        -3.59111309e-01,  2.12863013e-02, -9.41918865e-02,  6.87831566e-02,\n",
              "         1.28863439e-01,  1.46477669e-01, -2.65381653e-02,  3.27169955e-01,\n",
              "         4.96643066e-01,  5.64202428e-01,  3.11837435e-01,  5.85194863e-02,\n",
              "        -1.16228700e-01, -3.08481395e-01,  1.10124908e-01,  2.85096943e-01,\n",
              "         4.69592437e-02,  1.90518066e-01, -2.89468467e-01, -1.50390178e-01,\n",
              "         1.02881812e-01, -1.14472650e-01,  5.34636788e-02, -4.79551740e-02,\n",
              "         1.15898021e-01, -2.81709939e-01, -4.50495481e-01,  1.96445391e-01,\n",
              "         5.64683497e-01,  8.13148096e-02,  2.25618809e-01,  8.65780711e-01,\n",
              "         5.20760834e-01,  8.34164545e-02, -1.47655711e-01, -1.80678904e-01,\n",
              "        -2.66575873e-01, -1.34432390e-01, -2.20966473e-01,  4.27959234e-01,\n",
              "         4.15386796e-01, -6.49855494e-01, -3.76740210e-02, -1.20948210e-01,\n",
              "         1.29465735e+00,  1.97849959e-01, -6.80984259e-02, -3.89368325e-01,\n",
              "         2.49668255e-01, -1.00826532e-01,  1.24903843e-01,  5.53097785e-01,\n",
              "        -3.72584462e-01,  9.48317647e-02,  1.48878351e-01,  7.28109360e-01,\n",
              "         3.68199736e-01, -5.24742842e-01,  9.83190358e-01,  3.32874328e-01,\n",
              "        -4.83081862e-02, -7.85611868e-02, -4.35715079e-01,  8.36143270e-02,\n",
              "        -3.55440557e-01,  5.30741453e-01, -1.64875850e-01,  3.44813794e-01,\n",
              "        -4.89469051e-01, -1.99053541e-01, -1.16049580e-01, -2.65161935e-02,\n",
              "         5.55195324e-02,  5.62756598e-01, -6.82370603e-01, -1.11470938e-01,\n",
              "         9.04736891e-02,  1.23436250e-01, -3.95493269e-01, -4.20348167e-01,\n",
              "         4.24524516e-01,  8.90348032e-02, -6.25508368e-01, -8.82904753e-02,\n",
              "        -7.64678195e-02, -7.34592497e-01,  5.99547863e-01, -1.07045002e-01,\n",
              "        -5.65828979e-02,  7.93955266e-01,  1.96331292e-01,  3.31262976e-01,\n",
              "        -1.14866301e-01,  5.93512416e-01,  6.45156875e-02, -6.27890587e-01,\n",
              "        -2.45792061e-01,  7.66151994e-02, -3.16679955e-01, -1.78656280e-01,\n",
              "        -2.63119698e-01, -4.53189313e-01, -1.21909454e-01,  1.94516256e-01,\n",
              "         4.03121896e-02,  1.98828563e-01,  1.40138669e-03, -7.61329681e-02,\n",
              "         2.13804990e-01, -2.17629984e-01, -3.55108172e-01,  9.16010812e-02,\n",
              "         4.96091872e-01, -1.07609227e-01, -7.00106204e-01,  3.83604705e-01,\n",
              "         2.32985690e-01,  1.84037000e-01,  7.56520510e-01, -7.59451985e-02,\n",
              "        -7.61282295e-02, -3.34319174e-01, -7.00818717e-01, -2.38774252e+00,\n",
              "         2.95531124e-01, -3.82994339e-02,  8.12778950e-01,  2.43406352e-02,\n",
              "         4.95631933e-01,  2.43997574e-01, -5.86022258e-01,  3.79765302e-01,\n",
              "        -4.25535530e-01, -6.94953799e-02,  7.28130281e-01,  5.58093369e-01,\n",
              "        -5.53037897e-02, -3.01736128e-02,  1.58413481e-02, -1.40765449e-02,\n",
              "        -2.15868086e-01,  4.86262031e-02,  3.57963182e-02, -1.47060528e-01,\n",
              "        -1.22490339e-01, -5.03728271e-01, -7.43456602e-01, -1.05349374e+00,\n",
              "        -4.14616138e-01,  5.27961589e-02, -3.10070850e-02,  2.59459969e-02,\n",
              "         3.31803590e-01, -5.97370088e-01,  7.09472835e-01, -5.71904657e-03,\n",
              "         1.05884477e-01, -3.04838568e-02, -3.26949984e-01, -4.75982010e-01,\n",
              "         4.49603312e-02,  4.93685216e-01,  2.09894970e-01, -1.16388030e-01,\n",
              "         8.74377608e-01, -3.03938836e-01,  2.94406265e-01, -4.05707687e-01,\n",
              "        -3.84139746e-01,  5.35743356e-01, -5.80612046e-04,  3.72212082e-01,\n",
              "        -7.76851416e-01,  3.62775892e-01, -3.14780697e-02,  3.80594954e-02,\n",
              "         2.08832085e-01,  1.91814050e-01,  3.19398195e-01, -1.52141586e-01,\n",
              "         3.10364276e-01, -1.41056836e-01, -3.69151831e-01, -8.89910683e-02,\n",
              "        -7.67164379e-02, -4.55742963e-02,  1.79730073e-01,  4.31620210e-01,\n",
              "        -4.39400345e-01, -2.20771909e-01,  1.96780458e-01,  2.16226187e-02,\n",
              "        -6.17437288e-02, -1.36844277e-01,  1.32920548e-01, -1.10373218e-02,\n",
              "         6.14536107e-01,  1.79993510e-01,  7.56564289e-02, -9.47412569e-03,\n",
              "         2.48992234e-01,  7.75964081e-01, -3.71776193e-01,  4.26526070e-01,\n",
              "         7.76777416e-03, -3.46336246e-01, -3.44775140e-01,  1.63073182e-01,\n",
              "        -5.61948252e+00,  1.89674869e-01, -2.42430329e-01,  6.89353123e-02,\n",
              "         3.44795138e-01, -3.97522002e-01, -1.82985514e-01, -2.08219647e-01,\n",
              "         8.09381083e-02, -1.89693227e-01, -2.54664660e-01,  6.69617057e-01,\n",
              "        -3.35124694e-02, -1.90211147e-01,  3.73797536e-01,  3.96444112e-01],\n",
              "       dtype=float32),\n",
              " array([-1.32271156e-01, -2.76224881e-01, -3.49536479e-01,  4.03017104e-01,\n",
              "         6.26723826e-01,  9.28573787e-01,  3.54269564e-01,  4.18787211e-01,\n",
              "         9.30652171e-02, -9.84617710e-01,  4.73236442e-01, -2.97742248e-01,\n",
              "        -1.24097683e-01,  2.70189047e-01,  1.54146880e-01, -2.66527802e-01,\n",
              "         3.56231809e-01,  4.41532701e-01, -1.34754077e-01, -5.75517237e-01,\n",
              "         1.08425058e-01, -7.23930120e-01, -4.75830793e-01,  3.31279069e-01,\n",
              "         1.70513764e-01,  7.47568086e-02, -1.25391304e-01, -4.84085530e-01,\n",
              "        -3.34793240e-01,  3.18960309e-01,  1.67563170e-01, -1.74110189e-01,\n",
              "         1.81034714e-01, -4.92088944e-01, -5.09673417e-01, -2.27387860e-01,\n",
              "         8.41640159e-02, -4.98517556e-03, -1.10939495e-01,  8.39043140e-01,\n",
              "        -4.36505258e-01,  1.52007297e-01, -1.69744581e-01,  5.51617026e-01,\n",
              "         6.71651363e-01, -4.45955962e-01,  2.79277205e-01, -1.88808501e-01,\n",
              "         2.32515961e-01, -5.38897336e-01, -5.10967255e-01,  2.43614591e-03,\n",
              "        -2.96133697e-01,  3.17265511e-01, -6.38440102e-02,  4.93794858e-01,\n",
              "        -2.21035823e-01,  1.01996288e-01,  7.04156637e-01, -3.93560261e-01,\n",
              "        -1.37390167e-01,  2.24527270e-01,  4.64707017e-01, -4.99322683e-01,\n",
              "         7.55620003e-02, -5.11340238e-02, -6.78536817e-02, -2.00146094e-01,\n",
              "         2.82011002e-01, -3.23401652e-02, -1.77242860e-01,  3.05818737e-01,\n",
              "        -2.40304530e-01, -2.16611043e-01,  1.95988998e-01,  7.69588292e-01,\n",
              "         2.13049725e-01, -2.69718647e-01, -4.66057688e-01,  2.05401674e-01,\n",
              "         1.42435804e-01,  8.40206981e-01, -5.71672738e-01, -4.29926038e-01,\n",
              "         1.96607150e-02,  5.90040028e-01, -6.20843351e-01,  7.23027587e-01,\n",
              "         1.02181464e-01,  3.99611652e-01, -4.81092453e-01,  9.41113681e-02,\n",
              "        -2.70582944e-01,  2.78613687e-01, -1.49985462e-01, -5.57238758e-01,\n",
              "         4.98367958e-02,  1.64478660e-01,  5.06558776e-01,  9.38035920e-02,\n",
              "         2.33579874e-02, -8.43971789e-01, -2.70406574e-01, -1.39122277e-01,\n",
              "        -4.58737127e-02,  1.55872870e-02, -2.16542065e-01,  3.61698598e-01,\n",
              "        -8.35394561e-02, -1.11489177e+00,  1.05560198e-01, -1.90597489e-01,\n",
              "        -1.07546657e-01,  6.79896250e-02, -4.93548095e-01,  7.01865017e-01,\n",
              "         9.77630794e-01, -5.50273433e-02, -5.26210606e-01, -3.10342222e-01,\n",
              "        -2.15762436e-01,  7.53905475e-01,  2.20329389e-01, -4.28744406e-02,\n",
              "        -4.01366860e-01, -1.24226600e-01,  2.30846167e-01, -2.86576986e-01,\n",
              "         3.98806721e-01, -7.29917347e-01, -3.19225043e-01, -7.08768442e-02,\n",
              "         1.42596692e-01, -2.64794528e-01, -2.94227928e-01,  3.97498548e-01,\n",
              "        -7.63494223e-02,  7.70957321e-02, -6.89568043e-01,  2.93097615e-01,\n",
              "        -1.30118445e-01, -4.24389571e-01, -4.59551841e-01, -4.22760248e-01,\n",
              "        -5.12658536e-01,  3.34186971e-01,  1.18509330e-01, -1.13549061e-01,\n",
              "        -2.47999862e-01,  5.13390303e-01, -2.41997674e-01, -1.83663711e-01,\n",
              "        -2.95867175e-01, -1.94649622e-01,  1.60251155e-01,  1.97319314e-01,\n",
              "        -2.70580370e-02,  1.82698473e-01,  1.72905639e-01, -2.45043203e-01,\n",
              "         3.75830114e-01,  1.08341563e+00, -6.25474632e-01, -1.33061586e-02,\n",
              "        -6.53316975e-01,  8.08957070e-02,  4.29184474e-02, -2.39557281e-01,\n",
              "         5.76656759e-02,  2.10583150e-01, -4.91286904e-01, -1.81781933e-01,\n",
              "        -7.39361644e-02, -1.45407677e-01,  3.03629994e-01,  3.20882976e-01,\n",
              "        -3.55335444e-01, -1.96698889e-01, -1.36933988e-02, -4.19830292e-01,\n",
              "        -5.62474966e-01,  3.82471830e-01,  1.82312578e-01, -1.85468420e-01,\n",
              "        -2.76065975e-01, -3.68280500e-01, -4.92694527e-01, -7.80783296e-02,\n",
              "        -3.08708727e-01,  1.66826665e-01,  1.04066916e-01, -2.19494730e-01,\n",
              "        -4.83528018e-01, -2.71749675e-01,  1.88229904e-01,  3.38715091e-02,\n",
              "        -2.63271719e-01, -7.84515500e-01, -3.32744330e-01, -8.54791701e-02,\n",
              "        -7.27089703e-01,  2.66811341e-01, -1.85057640e-01,  2.80903932e-02,\n",
              "        -6.05118163e-02,  8.97986770e-01,  4.75269407e-02,  6.48471490e-02,\n",
              "         3.42790276e-01,  4.92968440e-01, -4.72047590e-02,  2.30691150e-01,\n",
              "        -7.47160316e-01, -2.16639370e-01,  2.11964071e-01, -9.44225937e-02,\n",
              "        -5.99904060e-01,  3.38490605e-01,  1.53465196e-01, -1.21621050e-01,\n",
              "         3.58155102e-01,  5.83503246e-01, -2.82410890e-01,  2.67158955e-01,\n",
              "         3.17844629e-01,  8.29297006e-01, -6.14570856e-01,  2.92351216e-01,\n",
              "        -1.61440209e-01, -7.27593422e-01, -1.92487285e-01,  5.01498058e-02,\n",
              "        -3.32505889e-02,  4.66805071e-01, -7.22967029e-01, -3.76539111e-01,\n",
              "        -7.15700209e-01,  2.36026421e-01,  3.47878665e-01, -3.98118615e-01,\n",
              "         1.18620276e-01,  2.42490694e-01, -9.58711132e-02,  6.44086957e-01,\n",
              "        -2.02521607e-01, -3.49965383e-04,  5.60712039e-01,  3.03883314e-01,\n",
              "        -7.07619965e-01,  1.40789762e-01, -3.94581705e-01, -4.08961356e-01,\n",
              "        -4.14609462e-01, -6.09467745e-01,  3.93854529e-01, -4.59907144e-01,\n",
              "         3.94885056e-02, -1.64484084e-02, -3.40016276e-01,  3.54338974e-01,\n",
              "         2.53990203e-01, -9.28773209e-02,  6.00696921e-01, -2.70708740e-01,\n",
              "        -1.49809182e-01, -3.86055708e-01,  9.44549218e-02, -6.16303720e-02,\n",
              "         1.43041819e-01,  9.81624275e-02,  1.20906822e-01, -1.41587260e-03,\n",
              "         1.64063647e-01, -9.46563423e-01, -1.63440108e-02, -1.32014617e-01,\n",
              "        -8.75466168e-02, -3.95578332e-02, -7.51173720e-02,  2.01573506e-01,\n",
              "         1.35553509e-01,  3.97066958e-02,  2.31597766e-01,  1.74125090e-01,\n",
              "         6.45092845e-01, -1.11492956e+00, -1.32211521e-01, -2.31433481e-01,\n",
              "        -6.13544583e-02, -1.89839721e-01, -6.15327537e-01,  3.49148124e-01,\n",
              "        -4.40027684e-01,  3.84873331e-01, -2.19111010e-01, -4.53095347e-01,\n",
              "         1.20437048e-01,  2.68600762e-01,  7.85424784e-02,  7.08677992e-02,\n",
              "         1.13993399e-01,  3.45346987e-01, -6.57072008e-01,  6.35023773e-01,\n",
              "        -2.30344728e-01, -1.31338716e-01,  1.75359547e-02,  7.16063023e-01,\n",
              "        -5.42305040e+00,  4.23695177e-01,  4.09041375e-01,  3.15198004e-01,\n",
              "         1.29853770e-01, -7.12497234e-01,  9.42538917e-01, -2.72713333e-01,\n",
              "        -4.00160313e-01,  7.66285419e-01, -5.90511411e-02, -4.14465606e-01,\n",
              "         5.08884750e-02,  1.38051212e-01, -1.16002010e-02, -1.37665451e-01,\n",
              "         5.39106369e-01, -4.33914334e-01,  3.63891035e-01,  6.43019319e-01,\n",
              "         6.88606262e-01, -6.00446425e-02, -1.16337538e-01, -3.08656096e-01,\n",
              "         3.25150788e-01,  4.77419376e-01, -4.54944700e-01, -6.54206127e-02,\n",
              "         9.13445279e-02,  4.51677412e-01,  2.24319547e-02, -1.44284919e-01,\n",
              "         5.54149225e-03,  1.06026423e+00, -5.79998887e-04,  1.56451404e-01,\n",
              "        -2.82135665e-01, -5.54678515e-02,  1.35904133e-01,  9.08228755e-02,\n",
              "         6.05974831e-02, -2.35050067e-01,  1.12193696e-01,  5.77416062e-01,\n",
              "         1.14307666e+00,  3.59919131e-01,  5.06114364e-01, -3.45119476e-01,\n",
              "         4.57524449e-01,  1.76062927e-01,  1.18032813e-01, -3.73644084e-01,\n",
              "         1.10688806e-01,  4.01054263e-01,  6.79848969e-01,  1.04622036e-01,\n",
              "         4.08744395e-01,  4.61059988e-01,  1.27946228e-01, -2.98194051e-01,\n",
              "        -4.56017166e-01, -6.20150805e-01,  2.22165920e-02,  1.07944690e-01,\n",
              "         2.94060588e-01,  1.60428986e-01,  3.09496105e-01,  7.37164691e-02,\n",
              "         6.36799216e-01, -2.49339804e-01,  3.03555489e-01,  2.19950095e-01,\n",
              "         7.98778906e-02, -9.15838540e-01, -5.39704487e-02,  4.35074240e-01,\n",
              "         7.11460233e-01, -1.23804130e-01,  1.73383981e-01,  1.33817032e-01,\n",
              "        -9.23782706e-01, -4.65191603e-01,  3.51583630e-01,  7.02984333e-02,\n",
              "        -3.53394270e-01, -3.62109065e-01, -1.15908183e-01,  7.10371360e-02,\n",
              "        -2.46097818e-01,  4.63281423e-01,  2.41951674e-01,  9.19201016e-01,\n",
              "        -2.86450922e-01,  8.58807266e-01,  2.90502876e-01,  1.48754671e-01,\n",
              "         2.26685002e-01, -8.62955928e-01, -8.76683462e-03, -4.15223569e-01,\n",
              "        -5.44910491e-01, -9.44470048e-01,  3.20000798e-01,  6.67602718e-02,\n",
              "         3.37314466e-03, -3.02337706e-01, -1.12398364e-01,  5.77003807e-02,\n",
              "         1.99970886e-01,  2.96740741e-01, -3.40555638e-01, -7.89728463e-01,\n",
              "         6.20862603e-01,  3.00545305e-01,  1.72430314e-02,  3.76259089e-01,\n",
              "         1.01092644e-01,  3.11605275e-01, -1.56202659e-01,  9.14367810e-02,\n",
              "         2.17637762e-01,  2.74977297e-01, -9.00154352e-01, -4.97933418e-01,\n",
              "         3.70942414e-01, -5.44997677e-03,  2.75399655e-01,  1.64888039e-01,\n",
              "        -3.12841237e-01,  1.17703438e-01, -2.56873816e-01, -4.95934963e-01,\n",
              "        -5.42847812e-01,  2.83561319e-01,  4.63587552e-01, -2.41912946e-01,\n",
              "        -3.69193852e-01, -1.65456676e+00,  2.92848080e-01,  4.47169095e-01,\n",
              "        -2.55920589e-01, -2.49434620e-01, -2.85494924e-01,  1.44084811e-01,\n",
              "        -2.49225393e-01,  1.09684475e-01,  4.38927412e-01,  6.24771863e-02,\n",
              "         3.70379001e-01,  4.56568331e-01, -2.46032760e-01,  1.53862298e-01,\n",
              "         2.13023111e-01, -4.70522016e-01,  1.37807757e-01,  1.54564515e-01,\n",
              "         3.76786202e-01, -1.15511015e-01, -4.78796922e-02,  3.02484959e-01,\n",
              "         1.40589148e-01,  5.39406121e-01,  1.33446664e-01, -1.85265079e-01,\n",
              "         1.66532665e-03,  4.15849477e-01, -1.28711149e-01,  7.05950335e-02,\n",
              "         5.02292812e-01,  7.54311085e-01,  1.50793254e-01, -7.34010786e-02,\n",
              "         6.12449408e-01, -3.27927798e-01,  2.68610120e-01, -5.49288809e-01,\n",
              "        -8.59172717e-02,  1.26787618e-01, -4.07478124e-01, -1.29679471e-01,\n",
              "        -2.84324348e-01,  5.08168519e-01, -3.52618188e-01,  1.80538237e-01,\n",
              "         2.15180948e-01,  3.09327319e-02, -4.44892585e-01,  6.65544450e-01,\n",
              "         4.17323187e-02,  4.42294955e-01,  7.77338296e-02,  4.75789100e-01,\n",
              "        -1.16257392e-01,  2.12593839e-01, -3.11166763e-01, -1.14692338e-01,\n",
              "        -4.76032376e-01, -5.35992272e-02,  1.20802090e-01,  6.39665306e-01,\n",
              "        -2.73973197e-01, -5.46355657e-02, -2.37852354e-02, -6.76626265e-01,\n",
              "         2.71363229e-01, -2.83259898e-01,  9.45568234e-02,  1.41934261e-01,\n",
              "        -2.69321889e-01,  4.11148399e-01, -2.32700825e-01,  2.21237272e-01,\n",
              "        -7.72422296e-04, -5.66776931e-01, -4.39310908e-01, -3.89989197e-01,\n",
              "         1.54823124e-01,  5.54409802e-01,  5.95034100e-02,  2.21061513e-01,\n",
              "         3.39884341e-01,  3.71722966e-01,  2.41826236e-01,  5.78544810e-02,\n",
              "        -2.76876211e-01,  2.20986217e-01, -3.44271213e-02, -2.42103189e-01,\n",
              "         4.68639195e-01, -2.27571234e-01,  2.51852155e-01, -9.27403331e-01,\n",
              "        -3.16990018e-02,  3.80084544e-01,  8.73845220e-02, -4.45627749e-01,\n",
              "         2.25205764e-01, -7.88019747e-02,  2.96379209e-01,  2.63870955e-01,\n",
              "        -1.45308143e-02, -2.56757647e-01,  4.50775534e-01,  4.10025418e-01,\n",
              "         2.40673140e-01,  3.38988245e-01, -2.03613520e-01,  3.01018447e-01,\n",
              "         1.09130755e-01,  1.58609197e-01,  4.40304816e-01,  4.00204062e-01,\n",
              "         5.41583359e-01, -1.24801092e-01,  1.11221291e-01, -5.79016685e-01,\n",
              "        -6.99561000e-01, -5.34762032e-02, -1.59253664e-02,  2.47346580e-01,\n",
              "         1.59837991e-01, -5.99131286e-01, -4.57206309e-01,  2.48615772e-01,\n",
              "         4.64766830e-01, -5.13905048e-01,  4.12802279e-01,  2.62435973e-01,\n",
              "        -4.52407181e-01, -1.16570108e-01, -6.49783373e-01, -8.31199586e-02,\n",
              "        -1.14375561e-01,  1.52529189e-02,  7.71051869e-02,  4.11616892e-01,\n",
              "         6.91190541e-01, -7.10293174e-01, -1.08267343e+00,  3.85944284e-02,\n",
              "         7.49824047e-01,  3.33052993e-01,  4.25717026e-01, -3.77678156e-01,\n",
              "        -6.61336124e-01, -2.57438689e-01,  9.47287604e-02,  5.07377565e-01,\n",
              "        -7.27701306e-01, -2.53510743e-01,  7.56217003e-01,  6.53069735e-01,\n",
              "         7.99407303e-01,  1.24601491e-01,  7.13163972e-01,  3.94054642e-03,\n",
              "        -7.91481659e-02,  2.63799012e-01, -3.57233554e-01,  2.92308629e-01,\n",
              "        -7.15017736e-01,  3.38784337e-01, -1.57962054e-01,  1.28017869e-02,\n",
              "        -3.05788428e-01, -5.65196648e-02,  1.20749868e-01, -1.87306821e-01,\n",
              "         6.68815494e-01, -2.34477460e-01, -7.44249895e-02, -3.48899812e-02,\n",
              "        -1.75021693e-01, -1.80507883e-01, -7.21915305e-01, -7.18379140e-01,\n",
              "         5.36204875e-01, -6.95014775e-01, -1.29244804e-01,  1.28594995e-01,\n",
              "         2.06915066e-01, -7.67479599e-01,  7.56661668e-02,  1.56128988e-01,\n",
              "         8.04778576e-01,  6.82251811e-01, -1.81257829e-01,  5.24880886e-01,\n",
              "        -7.56786346e-01, -1.99339867e-01,  4.91953678e-02, -2.92971551e-01,\n",
              "        -7.97262862e-02,  4.71347094e-01, -5.46738446e-01,  3.58563550e-02,\n",
              "         1.33926079e-01, -7.82266378e-01,  1.26400083e-01, -6.30783260e-01,\n",
              "         1.06702733e+00,  2.54181296e-01,  1.81079552e-01,  7.01062143e-01,\n",
              "         4.26786840e-01,  8.06295723e-02, -1.00780092e-01,  2.45071292e-01,\n",
              "         4.11095083e-01, -8.21130648e-02,  2.09951192e-01,  3.58823270e-01,\n",
              "         3.77353817e-01, -6.73767149e-01,  5.38425326e-01,  4.64907428e-03,\n",
              "        -4.23596017e-02, -1.57803763e-02, -6.90244734e-01, -1.33976245e+00,\n",
              "         3.31471652e-01, -4.92014021e-01,  1.07000053e+00, -5.59639454e-01,\n",
              "         3.64662528e-01, -3.71794075e-01, -2.38216609e-01, -1.56648263e-01,\n",
              "        -1.27729923e-01,  4.24870521e-01,  2.98236310e-01,  3.30356479e-01,\n",
              "         1.56143978e-02, -4.64355126e-02, -1.33965714e-02,  5.68775415e-01,\n",
              "         1.35371700e-01, -4.35414165e-01,  4.04244512e-02, -6.74046203e-02,\n",
              "         2.88020045e-01,  2.21068352e-01, -4.54652756e-01, -1.14181077e+00,\n",
              "        -8.07972699e-02, -4.00904089e-01, -2.64644265e-01, -4.85874623e-01,\n",
              "        -8.63908529e-02, -2.45655879e-01,  1.25153035e-01, -6.73853979e-02,\n",
              "        -6.66692713e-03, -5.80305934e-01, -1.37431890e-01, -2.41330639e-01,\n",
              "         3.32134485e-01,  4.17547762e-01,  4.20334071e-01, -2.63944685e-01,\n",
              "         6.59762740e-01,  2.86443561e-01, -2.99244951e-02, -2.15789586e-01,\n",
              "        -7.17347383e-01,  5.55515349e-01, -6.03112757e-01,  1.19803876e-01,\n",
              "        -1.17464852e+00,  7.16081411e-02,  7.08448768e-01, -3.35564971e-01,\n",
              "        -4.28348988e-01,  1.25241771e-01,  4.25944000e-01,  1.11239515e-02,\n",
              "         4.25340474e-01, -2.02169567e-01, -4.04619306e-01, -7.62695521e-02,\n",
              "         7.62027279e-02,  4.60989803e-01,  1.39674067e-01,  6.57401264e-01,\n",
              "         1.70608535e-01, -5.89526951e-01, -1.08002396e-02,  7.95005858e-02,\n",
              "         1.41207814e-01,  7.59584829e-02, -2.58985776e-02, -2.92161584e-01,\n",
              "         1.05102934e-01, -1.41121924e-01,  4.22796875e-01, -7.17003763e-01,\n",
              "        -2.42287576e-01,  8.31394553e-01, -5.77315986e-01,  5.42825818e-01,\n",
              "         2.22090378e-01,  8.57340991e-02,  2.94266194e-01, -4.72475380e-01,\n",
              "        -2.24675584e+00, -2.29122788e-01, -1.04525971e+00, -2.32706610e-02,\n",
              "         4.86585349e-01, -1.22184753e-01,  3.07392299e-01, -8.22704017e-01,\n",
              "         4.03955013e-01,  2.13603422e-01,  5.24978518e-01,  2.28779435e-01,\n",
              "         2.69511551e-01, -4.56658244e-01,  3.78646791e-01, -1.09613948e-01],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kotd8Mn5VU1L",
        "colab_type": "text"
      },
      "source": [
        "Another method to create the vectors is to sum the last four layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEBRn0odVU1M",
        "colab_type": "text"
      },
      "source": [
        "### Sentence Vector\n",
        "\n",
        "To get a single vector for our entire sentence we have multiple application-dependent strategies - we could just average all the tokens in our sentence. We can also use this oppurtunity to see if the second vector returned is a sentence vector too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMd-757OVU1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_embedding_0 = sentence_embedding.detach().numpy()[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0O3kfk7VU1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_embedding_1 = np.mean(token_vecs, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SehqzeBQVU1T",
        "colab_type": "code",
        "outputId": "2c6f4d6c-5844-4c26-c4d1-ff592e7cd90b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(sentence_embedding_0), len(sentence_embedding_1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcfairLsVU1V",
        "colab_type": "text"
      },
      "source": [
        "Remember that the power of these vectors is how they are context dependant - our sentence had multiple uses of the word bank. Let us see the index and the word of the sentence and check the context accordingly. We'll then print the simlarity values for the similar and different meanings and see how it turns out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml0xM8nRVU1W",
        "colab_type": "code",
        "outputId": "9db9510b-4878-44dd-c240-ee29ed8ecc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "for i, token_str in enumerate(tokenized_text):\n",
        "    print(i, token_str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [CLS]\n",
            "1 after\n",
            "2 stealing\n",
            "3 money\n",
            "4 from\n",
            "5 the\n",
            "6 bank\n",
            "7 vault\n",
            "8 ,\n",
            "9 the\n",
            "10 bank\n",
            "11 robber\n",
            "12 was\n",
            "13 seen\n",
            "14 fishing\n",
            "15 on\n",
            "16 the\n",
            "17 mississippi\n",
            "18 river\n",
            "19 bank\n",
            "20 .\n",
            "21 [SEP]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUB9ovToVU1Y",
        "colab_type": "code",
        "outputId": "d4b2583d-1345-48b4-b293-befa499b3daf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print('First 5 vector values for each instance of \"bank\".')\n",
        "print('')\n",
        "print(\"bank vault   \", str(token_vecs[6][:5]))\n",
        "print(\"bank robber  \", str(token_vecs[10][:5]))\n",
        "print(\"river bank   \", str(token_vecs[19][:5]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 vector values for each instance of \"bank\".\n",
            "\n",
            "bank vault    [ 0.90010476 -0.53804135 -0.16690874  0.22416204  0.6896583 ]\n",
            "bank robber   [ 0.7977133  -0.52172726 -0.19836937  0.18898545  0.59409314]\n",
            "river bank    [ 0.29608968 -0.2856336  -0.03818354  0.16736214  0.77126276]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOy3xx6tVU1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwj1OAo9VU1h",
        "colab_type": "code",
        "outputId": "55c99f7f-d4ae-4861-d05b-163e9edf65db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "\n",
        "# Calculate the cosine similarity between the word bank \n",
        "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
        "diff_bank = 1 - cosine(token_vecs[10], token_vecs[19])\n",
        "\n",
        "# Calculate the cosine similarity between the word bank\n",
        "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
        "same_bank = 1 - cosine(token_vecs[10], token_vecs[6])\n",
        "\n",
        "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
        "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector similarity for  *similar*  meanings:  0.95\n",
            "Vector similarity for *different* meanings:  0.70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVb0lI0CVU1j",
        "colab_type": "text"
      },
      "source": [
        "This makes sense! Let us see if the mean value of all the tokens and what we think is the sentence vector is the same thing, by checking their cosine distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2M1-Z-tVU1j",
        "colab_type": "code",
        "outputId": "e9856545-cb19-48e5-c1c5-b85a000a021b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "1 - cosine(sentence_embedding_0, sentence_embedding_1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.00831323117017746"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8wiyDhwVU1l",
        "colab_type": "text"
      },
      "source": [
        "That is good - it seems it is indeed the sentence vector, so we can now write two functions which calculate the word and sentence vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8k3TSioVU1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_vector(text, word_id, model, tokenizer):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    word_embeddings, sentence_embeddings = model(tokens_tensor)   \n",
        "    vector = word_embeddings[0][word_id].detach().numpy()\n",
        "    return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UmKNkkVVU1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_10 = word_vector(text, 6, model_embedding, tokenizer)\n",
        "word_6 = word_vector(text, 10, model_embedding, tokenizer)\n",
        "word_19 = word_vector(text, 19, model_embedding, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQt35yBZVU1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentence_vector(text, model, tokenizer, method=\"average\"):\n",
        "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "    tokenized_text = tokenizer.tokenize(marked_text)\n",
        "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "    word_embeddings, sentence_embeddings = model(tokens_tensor)\n",
        "    token_vecs = []\n",
        "    \n",
        "    for embedding in word_embeddings[0]:\n",
        "        cat_vec = embedding.detach().numpy()\n",
        "        token_vecs.append(cat_vec)\n",
        "        \n",
        "    if method == \"average\":\n",
        "        sentence_embedding = np.mean(token_vecs, axis=0)\n",
        "    if method == \"model\":\n",
        "        sentence_embedding = sentence_embeddings\n",
        "    # do something\n",
        "    return sentence_embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4JHr5G2VU1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sen_vec_0 = sentence_vector(text, model_embedding, tokenizer)\n",
        "sen_vec_1 = sentence_vector(text, model_embedding, tokenizer, method=\"model\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af950QCSVU1s",
        "colab_type": "text"
      },
      "source": [
        "### Similarity metrics\n",
        "It is worth noting that word-level similarity comparisons are not appropriate with BERT embeddings because these embeddings are contextually dependent, meaning that the word vector changes depending on the sentence it appears in. This enables direct sensitivity to polysemy so that, e.g., your representation encodes river “bank” and not a financial institution “bank”. Nevertheless, it makes direct word-to-word similarity comparisons less valuable. For sentence embeddings, however, similarity comparison is still valid such that one can query, for example, a single sentence against a dataset of other sentences in order to find the most similar. Depending on the similarity metric used, the resulting similarity values will be less informative than the relative ranking of similarity outputs as some similarity metrics make assumptions about the vector space (equally-weighted dimensions, for example) that do not hold for our 768-dimensional vector space.\n",
        "\n",
        "### Using the Vectors\n",
        "\n",
        "Without fine-tuning, BERT features may be less useful than plain GloVe or word2vec.\n",
        "They start to be interesting when you fine-tune a classifier on top of BERT. \n",
        "\n",
        "### Using Transformers Pipelines\n",
        "\n",
        "The context vectors make the other pipeline functions which transformers has built in a lot more powerful. \n",
        "\n",
        "### NOTE\n",
        "The pipeline functionality in transformers is currently being worked on and might be broken, so it is an optional part of the exercise. Do try to uncomment the lines of code and try to see if it works, though! If you have managed to get transformers v2.5.1 installed, it will work - I managed to get it to work sometimes, it can be annoying to get it to work but when it works it works well.\n",
        "\n",
        "Consider the following: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_HnfL50VU1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " from transformers import pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrOPsVnDVU1w",
        "colab_type": "code",
        "outputId": "49ba63ed-70a4-4f93-f20d-02907a31cabf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "9def3917127043e4ad7801a58505edb0",
            "222ff29243194de0abcbb29adf0e4fa8",
            "20602626f93f485d992bde68adf73a4f",
            "2f25b6b7a2fe4d139d19a0c9ee820809",
            "8ae8fff832ff41cf878336cd209b52a6",
            "75ee0d70f7264635bda48feeb4720548",
            "f0d9e8f8d1d844d48ee9d3f397c2d5cd",
            "b2b452f5071047eca41d788db8f58c98",
            "878bacd9fa1349cd971ff4090198b4c9",
            "754f1baba7c24561be6ef7512425e45f",
            "7abec9c901c34dc9a33ff5be98c43a6b",
            "844a6c424d924475b7d1f532028d07cc",
            "89110e75334d4766a09c12019f38b036",
            "e8d97221a9c44d02aa835c401929dd89",
            "2986f635e05c45399bd03b7cdf29fe95",
            "5ac5cff3edb849fa95c353a008ca67f3",
            "e73f130af51840dc851c4b2f415087d5",
            "a41ca2afa42a4611aa6f6019c5c781fb",
            "89fbd3b381544cf58f0e1fb459d82b1e",
            "7e4b9c20c7cb488ebe4444c114664d02",
            "503d74b3dc334228acecbe62972a4104",
            "3dec6473721a4b4d9656169518dcd19d",
            "9f28272d7f2b4e509a1ec48784ffd441",
            "161d2b5c9c2c4cfea9e2cc00d235d2e1",
            "8a1bb4a33f5f466eae890e1b441393c7",
            "f58615c4d84a4780a797889feed1288b",
            "7c1f53d7a828433ca968a46e51340fd7",
            "674fa38e3e574b50b41fde5639326b10",
            "a5f702c8462a4e229644a9cf20ecab46",
            "7a209549fc824e7e96f7cd767754d400",
            "c90d9e6aeec84ffbb4c04824255abdf3",
            "2701bfb6728a417da08e3c33e37c22a7"
          ]
        }
      },
      "source": [
        "# Allocate a pipeline for sentiment-analysis\n",
        " nlp_sentiment = pipeline('sentiment-analysis')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9def3917127043e4ad7801a58505edb0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=546, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "878bacd9fa1349cd971ff4090198b4c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=754, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e73f130af51840dc851c4b2f415087d5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a1bb4a33f5f466eae890e1b441393c7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=267844284, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsIQX7SHVU1x",
        "colab_type": "code",
        "outputId": "e24d2e0c-2f08-4f0f-d509-8a69beba2d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " nlp_sentiment(\"This BERT model is so good at classifiying sentiment, I love it.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9997735}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8PU-MADVU1z",
        "colab_type": "text"
      },
      "source": [
        "We get a strong positive sentiment, which we'd expect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1bFKDHXVU1z",
        "colab_type": "code",
        "outputId": "2dc0321e-3a0c-4de2-c6ce-6e06ba86ee67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " nlp_sentiment(\"I'm so sad that I have to spend this weekend just doing HW and readings.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9997195}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXVzTJbKVU13",
        "colab_type": "text"
      },
      "source": [
        "Negative label, bingo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCIOxAv3VU13",
        "colab_type": "code",
        "outputId": "4d9d449d-937a-4335-8b87-7884420a8d94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163,
          "referenced_widgets": [
            "781480d1155f43d1b7b67e010ec3f1bf",
            "328095142a6b450ab4715bd2ad7a477c",
            "77d78ba207fa4655a2084c4d85d2e38f",
            "91cbde568aad4e6895230c6fd9dbbccb",
            "0cd431ea7d7744c0bcc9e106b38541fb",
            "251eb2ffa9044967bd8ea42621c63722",
            "ff67ad2cbd814659ad84fd688be2583e",
            "22486f6d84d84257ac568d71e597c126",
            "a4b6aedc2d534e599ab3812e376cdb4b",
            "8bda4a79dce34f2684f97ae87782dc7f",
            "a11170460d264e81a12ff97781ff78d9",
            "6ca1bee0ede847508b99ac3524bf0022",
            "36e001801d61431f8607da1bafebc99a",
            "78e4ad1550b148ecbad7d292f3c7a578",
            "f8f671674c944185b2a5273411906e29",
            "37312bd099284fa887ff89175db55b97",
            "3a32831ca071418d9ba8a9d884160a97",
            "314bcd23f53b4041b76058bbd44f4393",
            "c29256b9ab894819a714dba4e0a0a3b6",
            "46e4083a3b69468aabdf8831ef8a6c73",
            "d51d2a79baf04cac9cad1b6d2f06df62",
            "262e1bc51cd54271bdc8053927911b1c",
            "add0aa1680d7455c99d77f36a4559843",
            "c3af8e6022b5494a9ccf0ae3a5aec650"
          ]
        }
      },
      "source": [
        "# Allocate a pipeline for question-answering\n",
        " nlp_question = pipeline('question-answering')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "781480d1155f43d1b7b67e010ec3f1bf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4b6aedc2d534e599ab3812e376cdb4b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=555, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a32831ca071418d9ba8a9d884160a97",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=265481570, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUjhbwbVVU15",
        "colab_type": "code",
        "outputId": "295c19da-5b7f-4db0-c9cb-cf1c5bca43b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        " nlp_question({\n",
        "     'question': 'What is my favorite thing to do on weekends ?',\n",
        "     'context': 'There is nothing I like more than analysing complex textual data all weekend '\n",
        " })"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convert squad examples to features: 100%|██████████| 1/1 [00:00<00:00, 330.83it/s]\n",
            "add example index and unique id: 100%|██████████| 1/1 [00:00<00:00, 3429.52it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answer': 'analysing complex textual data',\n",
              " 'end': 64,\n",
              " 'score': 0.783282314829421,\n",
              " 'start': 34}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIemccNrVU17",
        "colab_type": "text"
      },
      "source": [
        "It's also great at question-answering tasks!\n",
        "We can also extract features, as we manually did before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8dahP2mVU17",
        "colab_type": "code",
        "outputId": "e190ef0c-86c1-400e-8dff-efd12a46266f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114,
          "referenced_widgets": [
            "6bd4e8071ea04f54a1236db97e5d3b8b",
            "0b163c10db3b4ee4aa07325b21cbccf6",
            "dcb487c18d014542aa28bc0bb9114782",
            "93b8c107ea4f4af49b802efef342e433",
            "df5de5efae5c4308946c6573b57991a5",
            "00428507d1c84f95bce9f6e551dde84e",
            "554f2d962e4649fcb7f9ec78c60eb806",
            "77641d2ac64b4e9497df53e7b8e29853",
            "1cbd3348f2c94964a0031b7e9edf0445",
            "4a9e05693be04e5e88f7e379bd59a62f",
            "559691b1f0cc47f1bda7a0a164810704",
            "63a9af6b96aa4a72973876868eae0089",
            "d04b386c8e064f81b2e40a6ee98c405d",
            "67972abd5b424557bac2f8bb18adf50f",
            "684a5d67ae2f4b96937dd54e7c96a9b8",
            "ee8b93da6b4a464394a11b4a21ad9e7e"
          ]
        }
      },
      "source": [
        " nlp_feature = pipeline('feature-extraction')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bd4e8071ea04f54a1236db97e5d3b8b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1cbd3348f2c94964a0031b7e9edf0445",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=267967963, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JPo6XpJVU19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " vec = nlp_feature(\"Just sitting here exploring data all day long\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LijCssHVU1_",
        "colab_type": "code",
        "outputId": "2443785a-e92d-4b91-f19b-4b9f1ffc5b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        " len(vec[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Oq4KH4VU2A",
        "colab_type": "code",
        "outputId": "ce0b2ada-7c39-456e-d42e-3b17ce215731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " vec"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[[0.14189067482948303,\n",
              "   -0.12390559166669846,\n",
              "   0.09115814417600632,\n",
              "   -0.08900467306375504,\n",
              "   -0.05150013789534569,\n",
              "   -0.4635638892650604,\n",
              "   0.18017995357513428,\n",
              "   0.6232033371925354,\n",
              "   -0.3349599242210388,\n",
              "   -0.2881890833377838,\n",
              "   0.07291894406080246,\n",
              "   -0.17141906917095184,\n",
              "   -0.1449853479862213,\n",
              "   0.2984652817249298,\n",
              "   -0.12272806465625763,\n",
              "   0.040193960070610046,\n",
              "   0.08573563396930695,\n",
              "   0.1481054723262787,\n",
              "   0.2565165162086487,\n",
              "   0.0700564906001091,\n",
              "   0.060992516577243805,\n",
              "   -0.1371365189552307,\n",
              "   -0.023561697453260422,\n",
              "   -0.07952208071947098,\n",
              "   -0.06407859921455383,\n",
              "   -0.19339139759540558,\n",
              "   -0.09276492148637772,\n",
              "   0.045680366456508636,\n",
              "   0.07748443633317947,\n",
              "   0.07563905417919159,\n",
              "   -0.1420525163412094,\n",
              "   0.08205928653478622,\n",
              "   -0.25743886828422546,\n",
              "   -0.1854376345872879,\n",
              "   0.038040947169065475,\n",
              "   -0.21186859905719757,\n",
              "   0.10410626232624054,\n",
              "   -0.12047628313302994,\n",
              "   -0.12784768640995026,\n",
              "   0.04564565047621727,\n",
              "   -0.08709012717008591,\n",
              "   -0.011292720213532448,\n",
              "   -0.09043078124523163,\n",
              "   -0.11281097680330276,\n",
              "   0.10099746286869049,\n",
              "   -0.19919289648532867,\n",
              "   -2.1137187480926514,\n",
              "   -0.020534982904791832,\n",
              "   -0.11475207656621933,\n",
              "   -0.13262872397899628,\n",
              "   0.25711068511009216,\n",
              "   -0.14274241030216217,\n",
              "   0.1874479055404663,\n",
              "   0.291480153799057,\n",
              "   0.350800484418869,\n",
              "   0.4904603362083435,\n",
              "   -0.08307281881570816,\n",
              "   0.41149190068244934,\n",
              "   -0.13529111444950104,\n",
              "   0.15215656161308289,\n",
              "   0.32757410407066345,\n",
              "   -0.06796900928020477,\n",
              "   -0.3228010833263397,\n",
              "   -0.08485888689756393,\n",
              "   -0.17013870179653168,\n",
              "   0.06888911128044128,\n",
              "   -0.03870970755815506,\n",
              "   0.06222761422395706,\n",
              "   -0.15760016441345215,\n",
              "   0.4677022397518158,\n",
              "   -0.3461166322231293,\n",
              "   -0.17324316501617432,\n",
              "   0.25126221776008606,\n",
              "   0.06703227013349533,\n",
              "   -0.061183445155620575,\n",
              "   0.06951253861188889,\n",
              "   0.034664053469896317,\n",
              "   0.17166775465011597,\n",
              "   -0.03999583050608635,\n",
              "   0.1383817493915558,\n",
              "   0.027338813990354538,\n",
              "   0.5210862159729004,\n",
              "   0.24462856352329254,\n",
              "   0.23632848262786865,\n",
              "   -0.05639230087399483,\n",
              "   0.1803535670042038,\n",
              "   -0.5635473132133484,\n",
              "   -0.32705235481262207,\n",
              "   0.2466888576745987,\n",
              "   0.308651864528656,\n",
              "   -0.11022034287452698,\n",
              "   -0.12073197960853577,\n",
              "   0.1916385442018509,\n",
              "   0.1914253532886505,\n",
              "   0.3524211347103119,\n",
              "   -0.3255392909049988,\n",
              "   -0.09741540998220444,\n",
              "   0.0630158856511116,\n",
              "   0.24461884796619415,\n",
              "   0.2853175699710846,\n",
              "   0.17951811850070953,\n",
              "   -0.14545120298862457,\n",
              "   0.17531538009643555,\n",
              "   -0.25332146883010864,\n",
              "   0.291865736246109,\n",
              "   -0.1147962138056755,\n",
              "   0.01644848845899105,\n",
              "   -0.2873721420764923,\n",
              "   0.03805327042937279,\n",
              "   -2.441610336303711,\n",
              "   0.10115007311105728,\n",
              "   -0.02449466474354267,\n",
              "   0.005987229757010937,\n",
              "   -0.16471382975578308,\n",
              "   -0.14718914031982422,\n",
              "   0.08064853399991989,\n",
              "   0.2286129742860794,\n",
              "   -0.041901107877492905,\n",
              "   0.14204296469688416,\n",
              "   0.03552348166704178,\n",
              "   -0.2660551965236664,\n",
              "   0.3736701011657715,\n",
              "   0.008669876493513584,\n",
              "   -0.17582769691944122,\n",
              "   -0.09769915789365768,\n",
              "   0.27662453055381775,\n",
              "   -0.146589994430542,\n",
              "   -0.36558735370635986,\n",
              "   -0.10719699412584305,\n",
              "   0.09608837217092514,\n",
              "   0.34678661823272705,\n",
              "   0.44581979513168335,\n",
              "   0.12781216204166412,\n",
              "   -0.17231576144695282,\n",
              "   -0.3146626353263855,\n",
              "   0.12957319617271423,\n",
              "   0.36432552337646484,\n",
              "   -0.17451295256614685,\n",
              "   -0.21629463136196136,\n",
              "   0.07522779703140259,\n",
              "   -0.12373341619968414,\n",
              "   0.09256156533956528,\n",
              "   -2.797110080718994,\n",
              "   0.40121474862098694,\n",
              "   0.3705047369003296,\n",
              "   0.07605580985546112,\n",
              "   0.04045340046286583,\n",
              "   -0.06630239635705948,\n",
              "   0.11095736175775528,\n",
              "   0.08714142441749573,\n",
              "   0.04562394320964813,\n",
              "   0.026222871616482735,\n",
              "   -0.09973537176847458,\n",
              "   -0.06977388262748718,\n",
              "   -0.18034259974956512,\n",
              "   0.0684581771492958,\n",
              "   -0.45805177092552185,\n",
              "   -0.1821487993001938,\n",
              "   0.25059857964515686,\n",
              "   0.1201295480132103,\n",
              "   0.1219378262758255,\n",
              "   -0.22829392552375793,\n",
              "   -0.14787143468856812,\n",
              "   -0.04511437192559242,\n",
              "   -0.13493959605693817,\n",
              "   0.03614917770028114,\n",
              "   0.42456838488578796,\n",
              "   0.2764202356338501,\n",
              "   0.22307664155960083,\n",
              "   0.03614028915762901,\n",
              "   -0.11818433552980423,\n",
              "   0.13772816956043243,\n",
              "   0.39649924635887146,\n",
              "   -0.2583000063896179,\n",
              "   -0.04830144718289375,\n",
              "   0.01639113575220108,\n",
              "   0.13549917936325073,\n",
              "   0.26198846101760864,\n",
              "   0.006679609417915344,\n",
              "   -0.05919340252876282,\n",
              "   -0.013114934787154198,\n",
              "   0.473270982503891,\n",
              "   0.1145349070429802,\n",
              "   0.03079173155128956,\n",
              "   0.09757624566555023,\n",
              "   -0.0767911821603775,\n",
              "   0.24749702215194702,\n",
              "   -0.15404701232910156,\n",
              "   -0.257613867521286,\n",
              "   0.32242652773857117,\n",
              "   0.011831823736429214,\n",
              "   -0.2098160833120346,\n",
              "   0.1994401067495346,\n",
              "   -0.0011042621918022633,\n",
              "   0.3713664710521698,\n",
              "   -0.05984899029135704,\n",
              "   -0.07729873061180115,\n",
              "   -0.5751102566719055,\n",
              "   0.22309164702892303,\n",
              "   0.06476012617349625,\n",
              "   -0.15482284128665924,\n",
              "   -0.05584566295146942,\n",
              "   -0.08775703608989716,\n",
              "   0.08447933942079544,\n",
              "   -0.3391747772693634,\n",
              "   3.77706241607666,\n",
              "   0.08900029212236404,\n",
              "   -0.1689547300338745,\n",
              "   0.1249833032488823,\n",
              "   0.25135573744773865,\n",
              "   -0.23799803853034973,\n",
              "   0.19314800202846527,\n",
              "   -0.09071096032857895,\n",
              "   -0.0657348483800888,\n",
              "   0.0312188770622015,\n",
              "   0.023515287786722183,\n",
              "   0.25586625933647156,\n",
              "   0.13599836826324463,\n",
              "   -0.1302916705608368,\n",
              "   0.0080567030236125,\n",
              "   0.12279867380857468,\n",
              "   0.20432330667972565,\n",
              "   -0.1989934891462326,\n",
              "   0.44520050287246704,\n",
              "   -0.11259984970092773,\n",
              "   0.16985063254833221,\n",
              "   0.08140995353460312,\n",
              "   0.0797911062836647,\n",
              "   -0.04773546755313873,\n",
              "   -1.1796177625656128,\n",
              "   0.07820679992437363,\n",
              "   -0.1744629144668579,\n",
              "   -0.056143924593925476,\n",
              "   0.2717457413673401,\n",
              "   -0.15645352005958557,\n",
              "   -0.0779256746172905,\n",
              "   0.03904725983738899,\n",
              "   -0.024818647652864456,\n",
              "   0.07453455775976181,\n",
              "   0.01453505177050829,\n",
              "   -0.19642966985702515,\n",
              "   0.022992171347141266,\n",
              "   0.4081132411956787,\n",
              "   0.28501322865486145,\n",
              "   -0.07469592988491058,\n",
              "   0.49267083406448364,\n",
              "   0.2357170283794403,\n",
              "   0.04317593201994896,\n",
              "   -0.06692785769701004,\n",
              "   -0.1662720888853073,\n",
              "   0.3012932538986206,\n",
              "   0.006175084970891476,\n",
              "   0.026484403759241104,\n",
              "   -0.3685031235218048,\n",
              "   0.04501931741833687,\n",
              "   -0.18027807772159576,\n",
              "   -0.1471213549375534,\n",
              "   -0.026414887979626656,\n",
              "   -0.275400847196579,\n",
              "   0.1097494587302208,\n",
              "   -0.22290709614753723,\n",
              "   -0.11364620923995972,\n",
              "   0.08221229910850525,\n",
              "   -0.030703015625476837,\n",
              "   -0.5620907545089722,\n",
              "   -0.2979554831981659,\n",
              "   0.13429825007915497,\n",
              "   -0.19699957966804504,\n",
              "   0.1328638345003128,\n",
              "   0.03327305242419243,\n",
              "   -0.01831149123609066,\n",
              "   -0.0200374573469162,\n",
              "   -0.3602771461009979,\n",
              "   -3.63580584526062,\n",
              "   -0.0695691779255867,\n",
              "   -0.08991328626871109,\n",
              "   0.15681524574756622,\n",
              "   0.21509985625743866,\n",
              "   0.015525151044130325,\n",
              "   0.10404017567634583,\n",
              "   0.313425213098526,\n",
              "   0.3558163046836853,\n",
              "   -0.3473048806190491,\n",
              "   0.5014413595199585,\n",
              "   0.18775241076946259,\n",
              "   -0.1679747849702835,\n",
              "   0.25570714473724365,\n",
              "   -0.23252691328525543,\n",
              "   0.10884799808263779,\n",
              "   -0.08309311419725418,\n",
              "   -0.01151359360665083,\n",
              "   0.012671248987317085,\n",
              "   -0.08796339482069016,\n",
              "   0.005888774991035461,\n",
              "   0.1797654777765274,\n",
              "   -0.11733394116163254,\n",
              "   0.24883806705474854,\n",
              "   -0.2787546217441559,\n",
              "   0.009068524464964867,\n",
              "   -0.23081320524215698,\n",
              "   -0.1598641723394394,\n",
              "   -0.04999156668782234,\n",
              "   0.11388672888278961,\n",
              "   0.029125699773430824,\n",
              "   -0.19876255095005035,\n",
              "   0.1220359206199646,\n",
              "   -0.07379250228404999,\n",
              "   -0.2844364643096924,\n",
              "   -2.6703314781188965,\n",
              "   -0.07504609227180481,\n",
              "   -0.28740957379341125,\n",
              "   -0.04683247208595276,\n",
              "   -0.13761316239833832,\n",
              "   -0.10873081535100937,\n",
              "   0.25434449315071106,\n",
              "   -0.19275808334350586,\n",
              "   -0.342020720243454,\n",
              "   -0.12357362359762192,\n",
              "   0.02500792406499386,\n",
              "   0.0824543833732605,\n",
              "   0.11978372186422348,\n",
              "   0.3692665994167328,\n",
              "   0.4365362524986267,\n",
              "   0.043107978999614716,\n",
              "   0.21161743998527527,\n",
              "   0.04201553389430046,\n",
              "   0.13710100948810577,\n",
              "   0.22452011704444885,\n",
              "   0.030232273042201996,\n",
              "   -0.09338121861219406,\n",
              "   0.09719786047935486,\n",
              "   -0.03243439272046089,\n",
              "   0.12845759093761444,\n",
              "   0.4037132263183594,\n",
              "   -0.47691595554351807,\n",
              "   0.10475622117519379,\n",
              "   -0.22200565040111542,\n",
              "   0.0056738764978945255,\n",
              "   0.07900483906269073,\n",
              "   -0.34552985429763794,\n",
              "   -7.518361962866038e-05,\n",
              "   -0.013653535395860672,\n",
              "   -0.20257633924484253,\n",
              "   -0.0689961314201355,\n",
              "   0.05335073918104172,\n",
              "   0.23100386559963226,\n",
              "   0.5370945334434509,\n",
              "   0.2892773151397705,\n",
              "   -0.17156915366649628,\n",
              "   0.40652045607566833,\n",
              "   0.12200430780649185,\n",
              "   0.26002123951911926,\n",
              "   0.6548904776573181,\n",
              "   0.26176029443740845,\n",
              "   0.2224261611700058,\n",
              "   -0.14559441804885864,\n",
              "   -0.16454051434993744,\n",
              "   0.1452081948518753,\n",
              "   -0.005631265230476856,\n",
              "   0.09310908615589142,\n",
              "   1.178512454032898,\n",
              "   -0.1556803584098816,\n",
              "   0.004577021114528179,\n",
              "   -0.40215224027633667,\n",
              "   0.5373492240905762,\n",
              "   0.29735782742500305,\n",
              "   -0.08571943640708923,\n",
              "   -0.028071222826838493,\n",
              "   0.41571933031082153,\n",
              "   -0.3545486032962799,\n",
              "   0.07484594732522964,\n",
              "   -0.1962364763021469,\n",
              "   -0.02884351648390293,\n",
              "   -0.25330984592437744,\n",
              "   0.2777813673019409,\n",
              "   -0.34765955805778503,\n",
              "   0.09802307933568954,\n",
              "   0.14054705202579498,\n",
              "   -0.0008707377128303051,\n",
              "   0.30697914958000183,\n",
              "   -0.10163167119026184,\n",
              "   -0.9693422913551331,\n",
              "   -0.06378757208585739,\n",
              "   0.2646535336971283,\n",
              "   -0.09572897106409073,\n",
              "   0.053155455738306046,\n",
              "   0.14277450740337372,\n",
              "   0.26404887437820435,\n",
              "   -0.21338798105716705,\n",
              "   -0.1499396711587906,\n",
              "   -0.18730929493904114,\n",
              "   0.22815710306167603,\n",
              "   -0.20437559485435486,\n",
              "   -0.48528245091438293,\n",
              "   -0.2832508683204651,\n",
              "   -0.024929523468017578,\n",
              "   -0.267251193523407,\n",
              "   0.06694118678569794,\n",
              "   -0.13197511434555054,\n",
              "   0.15867727994918823,\n",
              "   0.3092917203903198,\n",
              "   0.1755565106868744,\n",
              "   0.3069901466369629,\n",
              "   -0.09701453894376755,\n",
              "   0.27842724323272705,\n",
              "   -0.8621044158935547,\n",
              "   0.19633960723876953,\n",
              "   -0.22644491493701935,\n",
              "   0.12367922067642212,\n",
              "   -0.2019204944372177,\n",
              "   -0.054795440286397934,\n",
              "   0.18924130499362946,\n",
              "   -0.20102722942829132,\n",
              "   -0.19321253895759583,\n",
              "   -0.226711705327034,\n",
              "   0.2725568115711212,\n",
              "   -0.19538508355617523,\n",
              "   0.20754662156105042,\n",
              "   -0.169674351811409,\n",
              "   0.16014984250068665,\n",
              "   0.14345911145210266,\n",
              "   0.15589816868305206,\n",
              "   0.8449886441230774,\n",
              "   -0.13603085279464722,\n",
              "   0.053615178912878036,\n",
              "   0.24068966507911682,\n",
              "   -0.1678590178489685,\n",
              "   0.20468440651893616,\n",
              "   0.11762485653162003,\n",
              "   0.2877519428730011,\n",
              "   0.001887910533696413,\n",
              "   -0.132967010140419,\n",
              "   -0.18435439467430115,\n",
              "   -0.1923152655363083,\n",
              "   0.01637326180934906,\n",
              "   -0.34637799859046936,\n",
              "   -0.2072051614522934,\n",
              "   0.012300962582230568,\n",
              "   0.08587922900915146,\n",
              "   0.22372977435588837,\n",
              "   -0.12342480570077896,\n",
              "   -0.47368767857551575,\n",
              "   -0.10674317926168442,\n",
              "   -0.2860183119773865,\n",
              "   -0.09750880300998688,\n",
              "   -0.10591991990804672,\n",
              "   0.4197397232055664,\n",
              "   0.1100495308637619,\n",
              "   0.4281109869480133,\n",
              "   0.224747896194458,\n",
              "   0.0015341397374868393,\n",
              "   0.41923144459724426,\n",
              "   -0.27890127897262573,\n",
              "   0.5886233448982239,\n",
              "   -0.042046092450618744,\n",
              "   0.06105789169669151,\n",
              "   -0.005588216707110405,\n",
              "   0.3022272288799286,\n",
              "   -0.27704015374183655,\n",
              "   -0.19603373110294342,\n",
              "   -0.11507018655538559,\n",
              "   -0.05349508672952652,\n",
              "   0.3971266746520996,\n",
              "   0.012008024379611015,\n",
              "   -0.00465360376983881,\n",
              "   -0.1993948072195053,\n",
              "   0.14083895087242126,\n",
              "   -0.0043094768188893795,\n",
              "   0.0032925885170698166,\n",
              "   0.06677978485822678,\n",
              "   -1.1960668563842773,\n",
              "   0.34319502115249634,\n",
              "   0.11449320614337921,\n",
              "   0.01673436537384987,\n",
              "   0.057170458137989044,\n",
              "   -0.09412831813097,\n",
              "   0.06575683504343033,\n",
              "   0.31443220376968384,\n",
              "   0.07002022862434387,\n",
              "   0.1507374495267868,\n",
              "   -0.11785666644573212,\n",
              "   -0.0962434858083725,\n",
              "   -0.2372543066740036,\n",
              "   0.11956365406513214,\n",
              "   -0.01845107041299343,\n",
              "   0.014626482501626015,\n",
              "   -0.1685343086719513,\n",
              "   0.07488313317298889,\n",
              "   -0.017560189589858055,\n",
              "   -0.055007629096508026,\n",
              "   0.06020866706967354,\n",
              "   0.3215947449207306,\n",
              "   0.1810104101896286,\n",
              "   -0.004348181653767824,\n",
              "   0.06847719848155975,\n",
              "   -0.013363025151193142,\n",
              "   -0.02583661675453186,\n",
              "   0.393423467874527,\n",
              "   0.09830386191606522,\n",
              "   0.3078402876853943,\n",
              "   0.05159403011202812,\n",
              "   -0.20729993283748627,\n",
              "   -0.485577255487442,\n",
              "   -0.1979215294122696,\n",
              "   0.3564087748527527,\n",
              "   0.24817989766597748,\n",
              "   0.12402079254388809,\n",
              "   -0.0616481751203537,\n",
              "   0.08690887689590454,\n",
              "   -0.011069528758525848,\n",
              "   -0.3721299171447754,\n",
              "   0.29203206300735474,\n",
              "   0.01165013387799263,\n",
              "   -0.07840316742658615,\n",
              "   0.5851749777793884,\n",
              "   0.1886526346206665,\n",
              "   -0.05195017158985138,\n",
              "   0.10395750403404236,\n",
              "   -0.02229904569685459,\n",
              "   -0.33299049735069275,\n",
              "   -0.2513653039932251,\n",
              "   -0.19071516394615173,\n",
              "   -0.1261555701494217,\n",
              "   0.05102440342307091,\n",
              "   0.13642776012420654,\n",
              "   -0.048247650265693665,\n",
              "   -0.018592162057757378,\n",
              "   0.008767927065491676,\n",
              "   -0.22478671371936798,\n",
              "   0.09383633732795715,\n",
              "   0.11110734939575195,\n",
              "   -0.2884538173675537,\n",
              "   -0.21345040202140808,\n",
              "   0.01924070529639721,\n",
              "   -0.1969987004995346,\n",
              "   -0.4873712956905365,\n",
              "   -0.22288930416107178,\n",
              "   -0.19702817499637604,\n",
              "   -0.04108433425426483,\n",
              "   0.037911608815193176,\n",
              "   0.21005336940288544,\n",
              "   -0.1622978299856186,\n",
              "   -0.23629768192768097,\n",
              "   0.21396847069263458,\n",
              "   -0.28499871492385864,\n",
              "   0.2648273706436157,\n",
              "   0.0659249946475029,\n",
              "   0.14575672149658203,\n",
              "   0.031459446996450424,\n",
              "   -0.285205215215683,\n",
              "   0.11509369313716888,\n",
              "   -0.4486059844493866,\n",
              "   -0.240940660238266,\n",
              "   0.18610231578350067,\n",
              "   -0.15505048632621765,\n",
              "   0.25462573766708374,\n",
              "   -0.26098552346229553,\n",
              "   -0.10706204175949097,\n",
              "   0.1184205561876297,\n",
              "   -0.11190126091241837,\n",
              "   -0.16902422904968262,\n",
              "   -0.22297239303588867,\n",
              "   0.0028958653565496206,\n",
              "   -0.08206244558095932,\n",
              "   -0.04077563434839249,\n",
              "   0.07998069375753403,\n",
              "   0.22379229962825775,\n",
              "   -0.1712985783815384,\n",
              "   0.11806809157133102,\n",
              "   -0.06212461739778519,\n",
              "   -0.2654877305030823,\n",
              "   0.3395913541316986,\n",
              "   0.2996240258216858,\n",
              "   0.2713463008403778,\n",
              "   0.01431035902351141,\n",
              "   0.2836616337299347,\n",
              "   0.359130859375,\n",
              "   -0.016698449850082397,\n",
              "   -0.1080389991402626,\n",
              "   -0.05379290133714676,\n",
              "   -0.1495130956172943,\n",
              "   -0.02516566962003708,\n",
              "   -0.1653147041797638,\n",
              "   0.10242285579442978,\n",
              "   0.05050210282206535,\n",
              "   -0.22047652304172516,\n",
              "   -0.04320557042956352,\n",
              "   -0.22592058777809143,\n",
              "   2.0817763805389404,\n",
              "   0.44672074913978577,\n",
              "   0.11490708589553833,\n",
              "   -0.11260263621807098,\n",
              "   0.14487610757350922,\n",
              "   -0.03374588117003441,\n",
              "   -0.08374080061912537,\n",
              "   0.41037142276763916,\n",
              "   -0.059109847992658615,\n",
              "   0.2217170000076294,\n",
              "   -0.11393207311630249,\n",
              "   0.2520650327205658,\n",
              "   -0.17494545876979828,\n",
              "   0.1741197407245636,\n",
              "   0.40919339656829834,\n",
              "   0.11452330648899078,\n",
              "   0.22857354581356049,\n",
              "   -0.3063148260116577,\n",
              "   -0.22487607598304749,\n",
              "   -0.03724738582968712,\n",
              "   -0.4895467162132263,\n",
              "   0.26384589076042175,\n",
              "   0.2721390426158905,\n",
              "   -0.0015764262061566114,\n",
              "   -0.04628855362534523,\n",
              "   0.3256835639476776,\n",
              "   0.15255863964557648,\n",
              "   -0.11119676381349564,\n",
              "   0.27868595719337463,\n",
              "   0.1644754260778427,\n",
              "   0.03267400339245796,\n",
              "   0.0920703262090683,\n",
              "   0.12340312451124191,\n",
              "   0.3943895101547241,\n",
              "   -0.3725792169570923,\n",
              "   -0.22407026588916779,\n",
              "   -0.025055600330233574,\n",
              "   -0.16032405197620392,\n",
              "   -0.15286153554916382,\n",
              "   0.12003812193870544,\n",
              "   0.10215044766664505,\n",
              "   -0.42095687985420227,\n",
              "   0.3861692547798157,\n",
              "   -0.08763957768678665,\n",
              "   -0.11373280733823776,\n",
              "   0.4380153715610504,\n",
              "   -0.10041720420122147,\n",
              "   -0.35476920008659363,\n",
              "   0.29637691378593445,\n",
              "   0.2003798931837082,\n",
              "   -0.003579714335501194,\n",
              "   0.01750635914504528,\n",
              "   -0.33239153027534485,\n",
              "   0.021644270047545433,\n",
              "   0.013727294281125069,\n",
              "   -0.15821781754493713,\n",
              "   0.09430892765522003,\n",
              "   0.042382653802633286,\n",
              "   -0.07403727620840073,\n",
              "   -0.08196861296892166,\n",
              "   -0.05646141991019249,\n",
              "   0.13998720049858093,\n",
              "   0.2817685604095459,\n",
              "   0.22388853132724762,\n",
              "   0.24760125577449799,\n",
              "   0.10161265730857849,\n",
              "   -0.1558239609003067,\n",
              "   0.06966249644756317,\n",
              "   0.06744486838579178,\n",
              "   -0.04855260252952576,\n",
              "   0.06390447914600372,\n",
              "   0.32811713218688965,\n",
              "   0.42348232865333557,\n",
              "   0.01286397222429514,\n",
              "   0.15842853486537933,\n",
              "   -0.08838482201099396,\n",
              "   0.46624040603637695,\n",
              "   0.1612309217453003,\n",
              "   -0.05961588770151138,\n",
              "   -2.7199811935424805,\n",
              "   0.2381746917963028,\n",
              "   0.07918205112218857,\n",
              "   -0.08866891264915466,\n",
              "   -0.1732357293367386,\n",
              "   0.3579196631908417,\n",
              "   0.3832072913646698,\n",
              "   -0.08317911624908447,\n",
              "   -0.01786012388765812,\n",
              "   -0.08431630581617355,\n",
              "   0.2706400156021118,\n",
              "   0.15971128642559052,\n",
              "   0.40344977378845215,\n",
              "   0.024495605379343033,\n",
              "   0.07221902906894684,\n",
              "   -0.010796286165714264,\n",
              "   0.05347433313727379,\n",
              "   -0.21729566156864166,\n",
              "   -0.10284766554832458,\n",
              "   -0.11736199259757996,\n",
              "   -0.09312638640403748,\n",
              "   0.36695384979248047,\n",
              "   0.03739142045378685,\n",
              "   -0.2933957576751709,\n",
              "   -0.33690401911735535,\n",
              "   0.24429713189601898,\n",
              "   -0.1464226096868515,\n",
              "   -0.2505648136138916,\n",
              "   0.011083722114562988,\n",
              "   0.1415240466594696,\n",
              "   -0.05228041112422943,\n",
              "   0.2195643037557602,\n",
              "   -0.3492546081542969,\n",
              "   -0.14026711881160736,\n",
              "   -0.024928713217377663,\n",
              "   0.038213323801755905,\n",
              "   -0.25521355867385864,\n",
              "   -0.10916654765605927,\n",
              "   0.029009254649281502,\n",
              "   0.30063724517822266,\n",
              "   -0.15047940611839294,\n",
              "   0.5533100962638855,\n",
              "   -0.13361461460590363,\n",
              "   0.2450127899646759,\n",
              "   0.03126019984483719,\n",
              "   -0.08708380162715912,\n",
              "   0.4189358949661255,\n",
              "   -0.4359220266342163,\n",
              "   0.06549248099327087,\n",
              "   -0.36129119992256165,\n",
              "   -0.0024675740860402584,\n",
              "   0.020101625472307205,\n",
              "   0.3303860127925873,\n",
              "   -0.030635641887784004,\n",
              "   0.18530938029289246,\n",
              "   -0.000640014884993434,\n",
              "   0.34903860092163086,\n",
              "   0.03211066871881485,\n",
              "   -0.06552394479513168,\n",
              "   -0.114411361515522,\n",
              "   0.11997158080339432,\n",
              "   0.00420392444357276,\n",
              "   0.02772914431989193,\n",
              "   -0.07427041977643967,\n",
              "   0.1780521720647812,\n",
              "   -0.13183005154132843,\n",
              "   -0.27580225467681885,\n",
              "   0.008217395283281803,\n",
              "   0.013235715217888355,\n",
              "   -0.16991828382015228,\n",
              "   -0.31521040201187134,\n",
              "   -0.31308984756469727,\n",
              "   0.22926612198352814,\n",
              "   0.1697162240743637,\n",
              "   0.05662497878074646,\n",
              "   -0.10752061754465103,\n",
              "   0.3375282287597656,\n",
              "   0.04558660835027695,\n",
              "   0.18099331855773926,\n",
              "   0.05683476850390434,\n",
              "   -0.196914941072464,\n",
              "   -0.15452636778354645,\n",
              "   0.08786531537771225,\n",
              "   -0.20492954552173615,\n",
              "   0.007906562648713589,\n",
              "   -7.39279842376709,\n",
              "   -0.0728866457939148,\n",
              "   -0.14927849173545837,\n",
              "   -0.05244627222418785,\n",
              "   -0.06724908202886581,\n",
              "   -0.24248917400836945,\n",
              "   -0.04998267814517021,\n",
              "   -0.06269968301057816,\n",
              "   0.06842708587646484,\n",
              "   -0.21331319212913513,\n",
              "   0.25745245814323425,\n",
              "   -0.04642234370112419,\n",
              "   -0.035301025956869125,\n",
              "   -0.047993771731853485,\n",
              "   0.4119162857532501,\n",
              "   0.15952686965465546],\n",
              "  [0.5480239987373352,\n",
              "   -0.14845852553844452,\n",
              "   0.008829821832478046,\n",
              "   0.3182567358016968,\n",
              "   -0.23657819628715515,\n",
              "   -0.3616679310798645,\n",
              "   -0.38439980149269104,\n",
              "   1.0074431896209717,\n",
              "   -0.14436623454093933,\n",
              "   -0.3434999883174896,\n",
              "   -0.18949905037879944,\n",
              "   -0.5735831260681152,\n",
              "   -0.3061425983905792,\n",
              "   0.5645259022712708,\n",
              "   -0.42823535203933716,\n",
              "   0.380318284034729,\n",
              "   0.5713322162628174,\n",
              "   -0.05658929795026779,\n",
              "   0.2236579954624176,\n",
              "   0.7389624118804932,\n",
              "   0.04447418078780174,\n",
              "   -0.1088845506310463,\n",
              "   -0.41995111107826233,\n",
              "   0.21291352808475494,\n",
              "   -0.14594806730747223,\n",
              "   -0.4835052490234375,\n",
              "   0.13144278526306152,\n",
              "   -0.049426112323999405,\n",
              "   -0.1357567310333252,\n",
              "   0.04840163514018059,\n",
              "   0.16814036667346954,\n",
              "   -0.475136935710907,\n",
              "   -0.5965958833694458,\n",
              "   -0.8206644654273987,\n",
              "   -0.2998701333999634,\n",
              "   -0.30570104718208313,\n",
              "   -0.10300768166780472,\n",
              "   0.3123209774494171,\n",
              "   -0.321205198764801,\n",
              "   -0.3408326506614685,\n",
              "   0.2904420495033264,\n",
              "   -0.6943031549453735,\n",
              "   -0.09934498369693756,\n",
              "   0.017194315791130066,\n",
              "   0.32339829206466675,\n",
              "   -0.48505404591560364,\n",
              "   0.20475155115127563,\n",
              "   -0.02827206626534462,\n",
              "   0.4003779888153076,\n",
              "   -0.9029915928840637,\n",
              "   -0.019333330914378166,\n",
              "   -0.03179347142577171,\n",
              "   0.42069244384765625,\n",
              "   0.4073099195957184,\n",
              "   0.12199503928422928,\n",
              "   0.8148741722106934,\n",
              "   -0.10859866440296173,\n",
              "   0.22361233830451965,\n",
              "   -0.15852533280849457,\n",
              "   0.22044023871421814,\n",
              "   -0.017716489732265472,\n",
              "   0.11685805767774582,\n",
              "   0.3159812092781067,\n",
              "   -0.66177898645401,\n",
              "   -0.05075566843152046,\n",
              "   0.5818215608596802,\n",
              "   0.1814865916967392,\n",
              "   -0.3395249545574188,\n",
              "   -0.36153900623321533,\n",
              "   0.24978752434253693,\n",
              "   -0.40384137630462646,\n",
              "   -0.5412755012512207,\n",
              "   -0.12734287977218628,\n",
              "   0.11237272620201111,\n",
              "   -0.13595187664031982,\n",
              "   -0.2485722005367279,\n",
              "   0.07609234005212784,\n",
              "   0.16767844557762146,\n",
              "   0.046384647488594055,\n",
              "   0.06914081424474716,\n",
              "   0.4479329288005829,\n",
              "   1.0837955474853516,\n",
              "   -0.7081340551376343,\n",
              "   0.48198068141937256,\n",
              "   -0.05634799599647522,\n",
              "   -0.005013145040720701,\n",
              "   -1.1245074272155762,\n",
              "   -0.1600923389196396,\n",
              "   0.09298793971538544,\n",
              "   0.4354359805583954,\n",
              "   -0.01545698195695877,\n",
              "   -0.01118644792586565,\n",
              "   0.23997873067855835,\n",
              "   0.042224351316690445,\n",
              "   0.06052356958389282,\n",
              "   -0.9535394906997681,\n",
              "   -0.2481563240289688,\n",
              "   0.24265557527542114,\n",
              "   0.8001957535743713,\n",
              "   0.6220465898513794,\n",
              "   0.011891989037394524,\n",
              "   -0.3501274585723877,\n",
              "   0.032588522881269455,\n",
              "   -0.6158797144889832,\n",
              "   -0.04741177335381508,\n",
              "   -0.45724356174468994,\n",
              "   0.1288263499736786,\n",
              "   0.5110241770744324,\n",
              "   0.17902573943138123,\n",
              "   -1.1052603721618652,\n",
              "   -0.047722142189741135,\n",
              "   -0.4204765260219574,\n",
              "   0.04895848408341408,\n",
              "   -0.6583161950111389,\n",
              "   0.014520907774567604,\n",
              "   0.4129714369773865,\n",
              "   0.05576372519135475,\n",
              "   -0.362303227186203,\n",
              "   -0.25437361001968384,\n",
              "   0.15321561694145203,\n",
              "   0.018677547574043274,\n",
              "   0.46428391337394714,\n",
              "   -0.11465934664011002,\n",
              "   0.41186845302581787,\n",
              "   -0.07520899176597595,\n",
              "   0.17173752188682556,\n",
              "   -0.267568439245224,\n",
              "   -0.0700724869966507,\n",
              "   -0.250768780708313,\n",
              "   -0.7456349730491638,\n",
              "   0.5662550330162048,\n",
              "   0.858272910118103,\n",
              "   0.28296422958374023,\n",
              "   -0.7885467410087585,\n",
              "   -0.057116881012916565,\n",
              "   0.053421154618263245,\n",
              "   0.5970461964607239,\n",
              "   -0.16704894602298737,\n",
              "   -0.38569194078445435,\n",
              "   0.10397668927907944,\n",
              "   0.5304443836212158,\n",
              "   0.24595262110233307,\n",
              "   -0.17630931735038757,\n",
              "   -0.00856153666973114,\n",
              "   0.6471470594406128,\n",
              "   -0.0167695265263319,\n",
              "   -0.10213322192430496,\n",
              "   -0.6444417834281921,\n",
              "   -0.003705247538164258,\n",
              "   0.4623793959617615,\n",
              "   -0.5517070889472961,\n",
              "   -0.04227351024746895,\n",
              "   0.030941490083932877,\n",
              "   -0.3187817335128784,\n",
              "   -0.1955258548259735,\n",
              "   0.5770872235298157,\n",
              "   -0.820560097694397,\n",
              "   -0.18129247426986694,\n",
              "   0.4571714997291565,\n",
              "   0.1658918708562851,\n",
              "   0.31876900792121887,\n",
              "   -0.260405033826828,\n",
              "   -0.33094167709350586,\n",
              "   0.5149271488189697,\n",
              "   -0.6850183010101318,\n",
              "   0.1945311576128006,\n",
              "   0.23228740692138672,\n",
              "   0.41867703199386597,\n",
              "   0.2772769331932068,\n",
              "   0.33522218465805054,\n",
              "   -0.3940621614456177,\n",
              "   0.010492801666259766,\n",
              "   0.6582840085029602,\n",
              "   -0.20806418359279633,\n",
              "   0.03445298224687576,\n",
              "   -0.13958235085010529,\n",
              "   0.41147831082344055,\n",
              "   -0.09118283540010452,\n",
              "   0.48556241393089294,\n",
              "   0.23231889307498932,\n",
              "   -0.44360366463661194,\n",
              "   1.1419861316680908,\n",
              "   0.09372560679912567,\n",
              "   -0.0324031226336956,\n",
              "   -0.3611435294151306,\n",
              "   -0.28055456280708313,\n",
              "   0.6646414995193481,\n",
              "   -0.3219013810157776,\n",
              "   -0.0690692812204361,\n",
              "   0.7029904723167419,\n",
              "   0.61078941822052,\n",
              "   -0.30704838037490845,\n",
              "   0.09244384616613388,\n",
              "   0.011981724761426449,\n",
              "   1.1350396871566772,\n",
              "   -0.2195884883403778,\n",
              "   -0.3348301947116852,\n",
              "   -1.1814132928848267,\n",
              "   -0.04448913410305977,\n",
              "   0.14514265954494476,\n",
              "   -0.11236689984798431,\n",
              "   -0.2702992260456085,\n",
              "   0.4639214277267456,\n",
              "   0.2995874285697937,\n",
              "   -0.7404305934906006,\n",
              "   1.6404900550842285,\n",
              "   -0.07661794871091843,\n",
              "   -0.6446247100830078,\n",
              "   -0.11050016433000565,\n",
              "   0.3464439809322357,\n",
              "   -0.14852555096149445,\n",
              "   0.43366801738739014,\n",
              "   0.09360993653535843,\n",
              "   -0.2528285086154938,\n",
              "   0.09893481433391571,\n",
              "   -0.13983017206192017,\n",
              "   0.4022027254104614,\n",
              "   0.14470109343528748,\n",
              "   -0.013543271459639072,\n",
              "   -0.027514994144439697,\n",
              "   0.17938154935836792,\n",
              "   0.1265350580215454,\n",
              "   -0.5491384863853455,\n",
              "   1.192535400390625,\n",
              "   0.2579999268054962,\n",
              "   0.9066522717475891,\n",
              "   -0.06440901756286621,\n",
              "   -0.11334648728370667,\n",
              "   0.02370004914700985,\n",
              "   -0.21765293180942535,\n",
              "   -0.17692790925502777,\n",
              "   -0.42949366569519043,\n",
              "   0.18602807819843292,\n",
              "   0.4538586735725403,\n",
              "   0.017947405576705933,\n",
              "   -0.5317066311836243,\n",
              "   -0.38800615072250366,\n",
              "   0.299435019493103,\n",
              "   -0.11485077440738678,\n",
              "   0.23200419545173645,\n",
              "   -0.4571692943572998,\n",
              "   -0.4663323760032654,\n",
              "   0.16227169334888458,\n",
              "   0.21900500357151031,\n",
              "   -0.13548941910266876,\n",
              "   0.5678836107254028,\n",
              "   0.13193799555301666,\n",
              "   -0.47831639647483826,\n",
              "   -0.6644124984741211,\n",
              "   -0.05098404362797737,\n",
              "   -0.4612865149974823,\n",
              "   -0.5400318503379822,\n",
              "   0.1240227073431015,\n",
              "   -1.1062145233154297,\n",
              "   -0.15981930494308472,\n",
              "   -0.06416032463312149,\n",
              "   -0.36119773983955383,\n",
              "   0.08349576592445374,\n",
              "   -0.40901127457618713,\n",
              "   0.8476132154464722,\n",
              "   -0.3364635109901428,\n",
              "   -0.4259801208972931,\n",
              "   0.37512314319610596,\n",
              "   -0.1228988915681839,\n",
              "   -0.8794047832489014,\n",
              "   -0.44595861434936523,\n",
              "   -0.16557392477989197,\n",
              "   0.25461122393608093,\n",
              "   0.6274900436401367,\n",
              "   0.5192663073539734,\n",
              "   -0.029077371582388878,\n",
              "   -0.4047282338142395,\n",
              "   0.09279453754425049,\n",
              "   -0.7053220272064209,\n",
              "   0.059838779270648956,\n",
              "   -0.1862182468175888,\n",
              "   0.41132986545562744,\n",
              "   -0.07263423502445221,\n",
              "   -0.32159119844436646,\n",
              "   0.22097526490688324,\n",
              "   0.032898612320423126,\n",
              "   0.7478379607200623,\n",
              "   0.0944834053516388,\n",
              "   0.5862072110176086,\n",
              "   -0.019327770918607712,\n",
              "   -0.5197621583938599,\n",
              "   0.2877610921859741,\n",
              "   0.4449092745780945,\n",
              "   -0.744621992111206,\n",
              "   -0.15034131705760956,\n",
              "   -0.1747675985097885,\n",
              "   0.5562825798988342,\n",
              "   -0.29728829860687256,\n",
              "   -0.3447123169898987,\n",
              "   0.05215439945459366,\n",
              "   -0.2754984200000763,\n",
              "   0.45380449295043945,\n",
              "   -0.5570628046989441,\n",
              "   -0.07316984981298447,\n",
              "   -0.08189557492733002,\n",
              "   0.1631869673728943,\n",
              "   -0.65584397315979,\n",
              "   -0.19361497461795807,\n",
              "   0.19741998612880707,\n",
              "   0.39245161414146423,\n",
              "   0.00481635145843029,\n",
              "   -0.16515347361564636,\n",
              "   -0.7891227602958679,\n",
              "   -3.703023672103882,\n",
              "   -0.22065035998821259,\n",
              "   -0.5570077896118164,\n",
              "   0.4285966455936432,\n",
              "   -0.12305016070604324,\n",
              "   -0.46490952372550964,\n",
              "   0.4374384582042694,\n",
              "   -0.36393240094184875,\n",
              "   -0.6636695265769958,\n",
              "   -0.4193861484527588,\n",
              "   -0.41136977076530457,\n",
              "   0.08179308474063873,\n",
              "   0.5035277009010315,\n",
              "   0.6287649869918823,\n",
              "   0.7637037038803101,\n",
              "   0.40999341011047363,\n",
              "   0.0682782232761383,\n",
              "   -0.11346421390771866,\n",
              "   0.10303923487663269,\n",
              "   0.8638062477111816,\n",
              "   0.06092960759997368,\n",
              "   -0.13977181911468506,\n",
              "   0.1089649572968483,\n",
              "   -0.36886632442474365,\n",
              "   0.46384522318840027,\n",
              "   0.8958065509796143,\n",
              "   -0.6079283356666565,\n",
              "   0.25664374232292175,\n",
              "   -0.13939163088798523,\n",
              "   0.26839372515678406,\n",
              "   -0.19812028110027313,\n",
              "   0.2686157822608948,\n",
              "   0.19239799678325653,\n",
              "   -0.09772324562072754,\n",
              "   0.4855671525001526,\n",
              "   0.2379733771085739,\n",
              "   -0.26704856753349304,\n",
              "   0.15076416730880737,\n",
              "   0.6117838621139526,\n",
              "   0.718292772769928,\n",
              "   0.10295224189758301,\n",
              "   -0.5211331248283386,\n",
              "   -0.4039144515991211,\n",
              "   0.32009634375572205,\n",
              "   0.5659374594688416,\n",
              "   0.4315902292728424,\n",
              "   0.4531678259372711,\n",
              "   -0.5684138536453247,\n",
              "   0.17620743811130524,\n",
              "   0.23811469972133636,\n",
              "   -0.259724497795105,\n",
              "   -0.45359712839126587,\n",
              "   0.36701056361198425,\n",
              "   0.04899497702717781,\n",
              "   0.14949879050254822,\n",
              "   -0.6571471691131592,\n",
              "   0.37137436866760254,\n",
              "   0.5120850801467896,\n",
              "   0.024224651977419853,\n",
              "   -0.7820196747779846,\n",
              "   -0.028724102303385735,\n",
              "   -0.8362467288970947,\n",
              "   -0.7251570820808411,\n",
              "   -0.10580230504274368,\n",
              "   -0.07926823943853378,\n",
              "   -0.13732072710990906,\n",
              "   -0.2744695842266083,\n",
              "   -0.03505793958902359,\n",
              "   0.6040621399879456,\n",
              "   0.11284000426530838,\n",
              "   0.024973787367343903,\n",
              "   0.44586580991744995,\n",
              "   0.26215115189552307,\n",
              "   -1.316145896911621,\n",
              "   0.037305839359760284,\n",
              "   0.5146856904029846,\n",
              "   -0.574335515499115,\n",
              "   -0.1947791427373886,\n",
              "   0.04903912916779518,\n",
              "   0.8029783368110657,\n",
              "   -0.40960758924484253,\n",
              "   -0.6916363835334778,\n",
              "   -0.04019302502274513,\n",
              "   -0.021068714559078217,\n",
              "   0.35764119029045105,\n",
              "   -1.1951391696929932,\n",
              "   -0.06859134137630463,\n",
              "   0.07001911103725433,\n",
              "   0.0964314192533493,\n",
              "   -0.3765726685523987,\n",
              "   0.21993987262248993,\n",
              "   0.44110673666000366,\n",
              "   0.6054269671440125,\n",
              "   0.2066032588481903,\n",
              "   0.3187384307384491,\n",
              "   -0.04441596567630768,\n",
              "   0.23612749576568604,\n",
              "   -0.9643437266349792,\n",
              "   0.8393529653549194,\n",
              "   -0.22357618808746338,\n",
              "   0.0010598436929285526,\n",
              "   -0.19570006430149078,\n",
              "   0.6328477263450623,\n",
              "   -0.6216015219688416,\n",
              "   0.6351649761199951,\n",
              "   -0.36694851517677307,\n",
              "   -0.2735633850097656,\n",
              "   -0.08406668901443481,\n",
              "   -0.4369112551212311,\n",
              "   0.15587399899959564,\n",
              "   -0.1434926837682724,\n",
              "   -0.1615234613418579,\n",
              "   0.2646600008010864,\n",
              "   -0.11218839883804321,\n",
              "   0.2585718631744385,\n",
              "   -0.1601455807685852,\n",
              "   -0.16071078181266785,\n",
              "   0.5239596962928772,\n",
              "   -0.7551093101501465,\n",
              "   -0.03821958601474762,\n",
              "   0.2123740166425705,\n",
              "   0.8418472409248352,\n",
              "   -0.12624040246009827,\n",
              "   -0.9210243821144104,\n",
              "   -0.5652163028717041,\n",
              "   0.13266748189926147,\n",
              "   0.21005432307720184,\n",
              "   -1.0234626531600952,\n",
              "   -0.9835818409919739,\n",
              "   -0.027767647057771683,\n",
              "   -0.06177657097578049,\n",
              "   0.027700267732143402,\n",
              "   0.04302975907921791,\n",
              "   -0.2976332902908325,\n",
              "   0.1030820906162262,\n",
              "   -0.5022475719451904,\n",
              "   -0.4780237674713135,\n",
              "   -0.8494638800621033,\n",
              "   0.5103839039802551,\n",
              "   -0.01977483741939068,\n",
              "   0.46758297085762024,\n",
              "   0.12798096239566803,\n",
              "   0.13154977560043335,\n",
              "   0.3855018615722656,\n",
              "   -0.6004300117492676,\n",
              "   1.033729076385498,\n",
              "   -0.254205584526062,\n",
              "   -0.10616902261972427,\n",
              "   0.4552778899669647,\n",
              "   0.5696635842323303,\n",
              "   -0.5607631802558899,\n",
              "   -0.00951796118170023,\n",
              "   -0.15570494532585144,\n",
              "   -0.24294550716876984,\n",
              "   0.7047407627105713,\n",
              "   -0.20874251425266266,\n",
              "   0.5205751061439514,\n",
              "   -0.26910918951034546,\n",
              "   0.19585558772087097,\n",
              "   -0.44308069348335266,\n",
              "   -0.7055363059043884,\n",
              "   0.29885241389274597,\n",
              "   -0.3898199498653412,\n",
              "   0.1715373992919922,\n",
              "   0.18418553471565247,\n",
              "   0.5199676156044006,\n",
              "   -0.3032682538032532,\n",
              "   0.35007476806640625,\n",
              "   0.33846908807754517,\n",
              "   -0.014202806167304516,\n",
              "   -0.005810417700558901,\n",
              "   0.028009630739688873,\n",
              "   -0.08528641611337662,\n",
              "   -0.6551045179367065,\n",
              "   -0.6799247860908508,\n",
              "   -0.03616824001073837,\n",
              "   -0.11957021057605743,\n",
              "   0.42150211334228516,\n",
              "   -0.384546160697937,\n",
              "   0.3332533836364746,\n",
              "   -0.22560060024261475,\n",
              "   -0.10133344680070877,\n",
              "   0.22791939973831177,\n",
              "   -0.17950312793254852,\n",
              "   0.36442360281944275,\n",
              "   0.5808071494102478,\n",
              "   -0.15473566949367523,\n",
              "   0.10535994917154312,\n",
              "   -0.10373231768608093,\n",
              "   0.7940306067466736,\n",
              "   0.3799141049385071,\n",
              "   0.2622285783290863,\n",
              "   -0.3574523329734802,\n",
              "   -0.3309188783168793,\n",
              "   -0.5105805397033691,\n",
              "   -0.6084389686584473,\n",
              "   0.276275634765625,\n",
              "   0.05140917748212814,\n",
              "   -0.03444738686084747,\n",
              "   0.0009595106821507215,\n",
              "   -0.15245506167411804,\n",
              "   -0.3772926330566406,\n",
              "   -0.5318341851234436,\n",
              "   0.06065688282251358,\n",
              "   0.058548908680677414,\n",
              "   -0.35011863708496094,\n",
              "   0.9166205525398254,\n",
              "   0.7594859600067139,\n",
              "   -0.2453661561012268,\n",
              "   0.5257813930511475,\n",
              "   -1.404776930809021,\n",
              "   -0.6118152141571045,\n",
              "   -0.19855977594852448,\n",
              "   -0.2563708424568176,\n",
              "   -0.1334218680858612,\n",
              "   -0.19910424947738647,\n",
              "   0.37949827313423157,\n",
              "   0.008992635644972324,\n",
              "   -0.19470395147800446,\n",
              "   -0.19171147048473358,\n",
              "   -0.3835846483707428,\n",
              "   0.06422937661409378,\n",
              "   -0.6185911893844604,\n",
              "   -0.08484816551208496,\n",
              "   -0.9843347668647766,\n",
              "   0.290658175945282,\n",
              "   -0.38662630319595337,\n",
              "   -0.7816744446754456,\n",
              "   -0.6757748126983643,\n",
              "   -0.19171592593193054,\n",
              "   0.26397988200187683,\n",
              "   -0.6965316534042358,\n",
              "   0.5570572018623352,\n",
              "   0.15037716925144196,\n",
              "   0.08425431698560715,\n",
              "   0.31000661849975586,\n",
              "   -0.2707759737968445,\n",
              "   0.6164088249206543,\n",
              "   0.6288579106330872,\n",
              "   0.33216336369514465,\n",
              "   0.11992593854665756,\n",
              "   -0.1897851824760437,\n",
              "   0.024282734841108322,\n",
              "   -0.7807079553604126,\n",
              "   0.120533786714077,\n",
              "   -0.4725976586341858,\n",
              "   0.0005057319649495184,\n",
              "   0.5502936840057373,\n",
              "   -0.5609755516052246,\n",
              "   0.2695210576057434,\n",
              "   0.3862916827201843,\n",
              "   -0.34959492087364197,\n",
              "   0.053519997745752335,\n",
              "   -0.41602185368537903,\n",
              "   -0.19744732975959778,\n",
              "   -0.5295335054397583,\n",
              "   -0.7702569365501404,\n",
              "   -0.13656647503376007,\n",
              "   0.30236154794692993,\n",
              "   -0.15056212246418,\n",
              "   0.5168164372444153,\n",
              "   0.05797731503844261,\n",
              "   0.20439422130584717,\n",
              "   0.16425196826457977,\n",
              "   -0.034180860966444016,\n",
              "   0.7357367277145386,\n",
              "   0.7527303099632263,\n",
              "   0.17269115149974823,\n",
              "   0.14367139339447021,\n",
              "   -0.038813043385744095,\n",
              "   0.04333750158548355,\n",
              "   -0.10228672623634338,\n",
              "   0.03789224848151207,\n",
              "   0.17307832837104797,\n",
              "   0.3398052752017975,\n",
              "   0.012285611592233181,\n",
              "   0.08542683720588684,\n",
              "   0.034680623561143875,\n",
              "   -0.43509066104888916,\n",
              "   -0.1457768827676773,\n",
              "   0.6623178124427795,\n",
              "   1.0606015920639038,\n",
              "   -0.06706023216247559,\n",
              "   -0.42325738072395325,\n",
              "   0.269720196723938,\n",
              "   0.06623619794845581,\n",
              "   -0.03497131168842316,\n",
              "   0.992485523223877,\n",
              "   0.17531034350395203,\n",
              "   0.6078312993049622,\n",
              "   0.2237173169851303,\n",
              "   0.3356904685497284,\n",
              "   0.013435270637273788,\n",
              "   0.03804977983236313,\n",
              "   0.00990750826895237,\n",
              "   -0.4328222870826721,\n",
              "   0.31554487347602844,\n",
              "   0.12938666343688965,\n",
              "   -0.2145376205444336,\n",
              "   0.2911003828048706,\n",
              "   -0.7209134697914124,\n",
              "   -0.24281646311283112,\n",
              "   0.1028899997472763,\n",
              "   0.5284084677696228,\n",
              "   -0.3081730604171753,\n",
              "   -0.17661766707897186,\n",
              "   0.4831754267215729,\n",
              "   -0.11660567671060562,\n",
              "   0.514239490032196,\n",
              "   0.15762613713741302,\n",
              "   -0.06389176845550537,\n",
              "   -0.26068681478500366,\n",
              "   0.6166678071022034,\n",
              "   0.12242922186851501,\n",
              "   -0.9337992668151855,\n",
              "   -0.32962584495544434,\n",
              "   -0.08930576592683792,\n",
              "   0.10961954295635223,\n",
              "   -0.22611936926841736,\n",
              "   0.1607886254787445,\n",
              "   0.3130777180194855,\n",
              "   -0.6803010106086731,\n",
              "   0.21488800644874573,\n",
              "   -0.31597399711608887,\n",
              "   0.34314805269241333,\n",
              "   0.41240572929382324,\n",
              "   -0.3975827693939209,\n",
              "   -0.47754523158073425,\n",
              "   0.3012164831161499,\n",
              "   0.09670278429985046,\n",
              "   0.1104556992650032,\n",
              "   0.12489741295576096,\n",
              "   -0.2206026315689087,\n",
              "   0.3258058726787567,\n",
              "   -0.07067806273698807,\n",
              "   0.24413123726844788,\n",
              "   0.601901650428772,\n",
              "   -0.12822017073631287,\n",
              "   0.14376996457576752,\n",
              "   -0.7378180623054504,\n",
              "   0.05349326133728027,\n",
              "   0.9588487148284912,\n",
              "   0.9473274946212769,\n",
              "   1.0565731525421143,\n",
              "   0.5861404538154602,\n",
              "   0.10416373610496521,\n",
              "   0.1509355753660202,\n",
              "   0.015964481979608536,\n",
              "   0.5101616382598877,\n",
              "   0.08035625517368317,\n",
              "   0.4144386053085327,\n",
              "   0.5929529070854187,\n",
              "   0.3889264464378357,\n",
              "   -0.19013050198554993,\n",
              "   0.2334534376859665,\n",
              "   -0.7391021847724915,\n",
              "   0.2146168053150177,\n",
              "   0.6153815984725952,\n",
              "   0.1877461075782776,\n",
              "   -1.2895331382751465,\n",
              "   0.1413833498954773,\n",
              "   -0.4500497877597809,\n",
              "   -0.17100058495998383,\n",
              "   -0.4062066972255707,\n",
              "   0.404219388961792,\n",
              "   0.5433072447776794,\n",
              "   0.335077166557312,\n",
              "   0.3374550938606262,\n",
              "   -0.5856895446777344,\n",
              "   0.2590353190898895,\n",
              "   -0.13135205209255219,\n",
              "   -0.024154845625162125,\n",
              "   -0.1625235378742218,\n",
              "   -0.06100120395421982,\n",
              "   0.21227695047855377,\n",
              "   0.3899316191673279,\n",
              "   0.3338182866573334,\n",
              "   -0.471549928188324,\n",
              "   -0.18409667909145355,\n",
              "   0.44313615560531616,\n",
              "   0.510648250579834,\n",
              "   0.18120013177394867,\n",
              "   -0.4751346707344055,\n",
              "   -0.6137815713882446,\n",
              "   0.18104159832000732,\n",
              "   -0.7962199449539185,\n",
              "   -0.7734264135360718,\n",
              "   0.008084069006145,\n",
              "   0.26715603470802307,\n",
              "   -0.2955804169178009,\n",
              "   0.4046667516231537,\n",
              "   -0.36656349897384644,\n",
              "   -0.49022746086120605,\n",
              "   -1.1809272766113281,\n",
              "   0.08916286379098892,\n",
              "   -0.7436591982841492,\n",
              "   -0.1489938199520111,\n",
              "   0.6232394576072693,\n",
              "   0.3790706992149353,\n",
              "   -0.06093156710267067,\n",
              "   1.305397391319275,\n",
              "   0.2723957896232605,\n",
              "   0.00981175247579813,\n",
              "   -0.40752914547920227,\n",
              "   -0.5822188258171082,\n",
              "   0.3024944067001343,\n",
              "   -1.0892372131347656,\n",
              "   0.1552535742521286,\n",
              "   -0.3951551020145416,\n",
              "   0.0029896360356360674,\n",
              "   0.4478569030761719,\n",
              "   0.5958907604217529,\n",
              "   -0.5170362591743469,\n",
              "   -0.1376187950372696,\n",
              "   -0.5169315934181213,\n",
              "   0.22301198542118073,\n",
              "   0.4548148214817047,\n",
              "   -0.4167879521846771,\n",
              "   -0.09140714257955551,\n",
              "   0.3714851140975952,\n",
              "   0.07608212530612946,\n",
              "   0.06626201421022415,\n",
              "   0.042869310826063156,\n",
              "   0.05046243965625763,\n",
              "   -0.02147766202688217,\n",
              "   -0.6383672952651978,\n",
              "   0.10978879779577255,\n",
              "   0.8090054392814636,\n",
              "   -0.18249738216400146,\n",
              "   -0.43932926654815674,\n",
              "   -1.0398926734924316,\n",
              "   0.019071349874138832,\n",
              "   0.5684469938278198,\n",
              "   -0.1973850131034851,\n",
              "   -0.3919210731983185,\n",
              "   0.2834884524345398,\n",
              "   0.24380895495414734,\n",
              "   0.1692837029695511,\n",
              "   -0.13076402246952057,\n",
              "   -0.5157579183578491,\n",
              "   0.17053638398647308,\n",
              "   0.4380606710910797,\n",
              "   0.17145857214927673,\n",
              "   -0.5900399088859558,\n",
              "   -1.465410590171814,\n",
              "   -0.21814724802970886,\n",
              "   -0.9514141082763672,\n",
              "   -0.6698652505874634,\n",
              "   0.6599799394607544,\n",
              "   0.14625510573387146,\n",
              "   0.27548739314079285,\n",
              "   -0.32893112301826477,\n",
              "   0.2456037700176239,\n",
              "   -0.30548596382141113,\n",
              "   0.49368971586227417,\n",
              "   0.02387651614844799,\n",
              "   0.06893330067396164,\n",
              "   0.14207345247268677,\n",
              "   0.8880273103713989,\n",
              "   0.3884498178958893],\n",
              "  [0.5202011466026306,\n",
              "   0.24978236854076385,\n",
              "   0.38667404651641846,\n",
              "   0.3759371340274811,\n",
              "   -0.1648452877998352,\n",
              "   -0.3782495856285095,\n",
              "   -0.2996227443218231,\n",
              "   1.0650099515914917,\n",
              "   -0.8018838763237,\n",
              "   -0.05962398275732994,\n",
              "   0.03007596917450428,\n",
              "   -0.5185371041297913,\n",
              "   -0.11581338196992874,\n",
              "   0.6928452253341675,\n",
              "   0.19691269099712372,\n",
              "   0.35236889123916626,\n",
              "   0.3698018193244934,\n",
              "   -0.24978463351726532,\n",
              "   -0.5353102087974548,\n",
              "   0.6733987927436829,\n",
              "   -0.0008898904779925942,\n",
              "   -0.259705513715744,\n",
              "   -0.10588502138853073,\n",
              "   0.043873537331819534,\n",
              "   -0.07111303508281708,\n",
              "   -0.5803885459899902,\n",
              "   -0.21674111485481262,\n",
              "   -0.0910232812166214,\n",
              "   -0.20511655509471893,\n",
              "   0.1429450660943985,\n",
              "   0.15732455253601074,\n",
              "   0.06811530888080597,\n",
              "   -0.651878833770752,\n",
              "   -0.2917349636554718,\n",
              "   0.24675270915031433,\n",
              "   -0.22669589519500732,\n",
              "   0.14588548243045807,\n",
              "   0.3763461709022522,\n",
              "   -0.10562257468700409,\n",
              "   -0.08479683101177216,\n",
              "   -0.2913088798522949,\n",
              "   -0.7393676042556763,\n",
              "   -0.07084838300943375,\n",
              "   -0.017720302566885948,\n",
              "   -0.28429552912712097,\n",
              "   -0.44929543137550354,\n",
              "   0.8485278487205505,\n",
              "   0.3450393080711365,\n",
              "   0.4794177711009979,\n",
              "   -0.5130280256271362,\n",
              "   -0.38205835223197937,\n",
              "   0.4708564579486847,\n",
              "   0.1396157294511795,\n",
              "   0.0563894622027874,\n",
              "   -0.0922023206949234,\n",
              "   1.1690380573272705,\n",
              "   0.15751923620700836,\n",
              "   -0.309425413608551,\n",
              "   -0.4188787043094635,\n",
              "   0.006410960573703051,\n",
              "   0.13387639820575714,\n",
              "   -0.2954237461090088,\n",
              "   0.20807424187660217,\n",
              "   -0.5287591218948364,\n",
              "   -0.10552990436553955,\n",
              "   0.15978871285915375,\n",
              "   0.07826157659292221,\n",
              "   -0.32186049222946167,\n",
              "   0.0529041513800621,\n",
              "   0.3841346502304077,\n",
              "   -0.8722487688064575,\n",
              "   -0.08209434896707535,\n",
              "   -0.47600415349006653,\n",
              "   0.3519219756126404,\n",
              "   -0.6732465624809265,\n",
              "   -0.023840993642807007,\n",
              "   0.01644158363342285,\n",
              "   0.45483797788619995,\n",
              "   0.2422095239162445,\n",
              "   0.1877322643995285,\n",
              "   0.17415334284305573,\n",
              "   0.9227734208106995,\n",
              "   -0.8598780035972595,\n",
              "   0.5335029363632202,\n",
              "   0.22639767825603485,\n",
              "   0.0012987973168492317,\n",
              "   -0.7192950248718262,\n",
              "   -0.2464553862810135,\n",
              "   -0.01651190221309662,\n",
              "   0.685638427734375,\n",
              "   0.14522869884967804,\n",
              "   0.007707273121923208,\n",
              "   0.18428732454776764,\n",
              "   0.18738916516304016,\n",
              "   -0.24825476109981537,\n",
              "   -0.6098088026046753,\n",
              "   -0.2964943051338196,\n",
              "   0.2515287697315216,\n",
              "   -0.14359334111213684,\n",
              "   0.4988764226436615,\n",
              "   0.6169960498809814,\n",
              "   -1.001395583152771,\n",
              "   -0.1407187134027481,\n",
              "   -0.07576274871826172,\n",
              "   0.14805276691913605,\n",
              "   -0.06578388810157776,\n",
              "   0.2606934905052185,\n",
              "   0.16038081049919128,\n",
              "   -0.09233986586332321,\n",
              "   -0.15883959829807281,\n",
              "   -0.07847835123538971,\n",
              "   -0.6631355285644531,\n",
              "   0.357911080121994,\n",
              "   -0.28934910893440247,\n",
              "   -0.14855732023715973,\n",
              "   0.10952052474021912,\n",
              "   -0.1172243282198906,\n",
              "   -0.6309601068496704,\n",
              "   -0.21880555152893066,\n",
              "   0.09334416687488556,\n",
              "   -0.25762858986854553,\n",
              "   -0.05945321172475815,\n",
              "   -0.4138437509536743,\n",
              "   0.9994455575942993,\n",
              "   -0.15973800420761108,\n",
              "   0.0028904490172863007,\n",
              "   0.04523155465722084,\n",
              "   -0.25896546244621277,\n",
              "   0.12734955549240112,\n",
              "   -0.4664596617221832,\n",
              "   0.7977511882781982,\n",
              "   0.3343031108379364,\n",
              "   0.7027816772460938,\n",
              "   -0.8950889706611633,\n",
              "   -0.1761757880449295,\n",
              "   -0.042638685554265976,\n",
              "   0.11127842962741852,\n",
              "   -0.4579251706600189,\n",
              "   -0.5516202449798584,\n",
              "   0.19624337553977966,\n",
              "   -0.0006805907469242811,\n",
              "   0.07522103935480118,\n",
              "   1.0011324882507324,\n",
              "   0.30655062198638916,\n",
              "   0.6643912196159363,\n",
              "   0.12305615097284317,\n",
              "   -0.4310242235660553,\n",
              "   -0.34131160378456116,\n",
              "   -0.17427057027816772,\n",
              "   0.06117863953113556,\n",
              "   -0.4607850909233093,\n",
              "   -0.6716561913490295,\n",
              "   -0.20520493388175964,\n",
              "   -0.2806341052055359,\n",
              "   -0.11089418083429337,\n",
              "   0.003786781569942832,\n",
              "   -0.8088805079460144,\n",
              "   -0.18834735453128815,\n",
              "   0.6409646272659302,\n",
              "   0.19703909754753113,\n",
              "   0.5493030548095703,\n",
              "   -0.22219423949718475,\n",
              "   -0.41744229197502136,\n",
              "   0.02261953614652157,\n",
              "   0.13115407526493073,\n",
              "   -0.18589183688163757,\n",
              "   -0.004887949209660292,\n",
              "   0.3052407503128052,\n",
              "   0.22622127830982208,\n",
              "   0.221134752035141,\n",
              "   -0.4571154713630676,\n",
              "   -0.2603786885738373,\n",
              "   0.6950649619102478,\n",
              "   -0.4536299407482147,\n",
              "   0.002917440142482519,\n",
              "   -0.12579655647277832,\n",
              "   0.4417673349380493,\n",
              "   -0.18121427297592163,\n",
              "   0.30031657218933105,\n",
              "   0.42827779054641724,\n",
              "   -0.544304609298706,\n",
              "   0.21860331296920776,\n",
              "   -0.0340319499373436,\n",
              "   0.039839595556259155,\n",
              "   0.20318304002285004,\n",
              "   -0.3050118684768677,\n",
              "   0.9084163904190063,\n",
              "   -0.31444475054740906,\n",
              "   -0.4423021376132965,\n",
              "   0.28741899132728577,\n",
              "   0.056516580283641815,\n",
              "   -0.41995757818222046,\n",
              "   -0.2032729685306549,\n",
              "   0.07212027907371521,\n",
              "   0.6342688202857971,\n",
              "   -0.291850209236145,\n",
              "   -0.29413479566574097,\n",
              "   -1.0635437965393066,\n",
              "   0.14425042271614075,\n",
              "   0.02277572639286518,\n",
              "   0.044149547815322876,\n",
              "   -0.19924359023571014,\n",
              "   0.09116723388433456,\n",
              "   0.3088274300098419,\n",
              "   -0.5154769420623779,\n",
              "   0.5166388154029846,\n",
              "   -0.25844094157218933,\n",
              "   -0.5162892937660217,\n",
              "   -0.2509629726409912,\n",
              "   0.2091098576784134,\n",
              "   -0.3990528881549835,\n",
              "   0.7093623876571655,\n",
              "   0.04346124455332756,\n",
              "   0.47627511620521545,\n",
              "   -0.2817580997943878,\n",
              "   -0.146617129445076,\n",
              "   0.09933646768331528,\n",
              "   -0.06981829553842545,\n",
              "   -0.4998491108417511,\n",
              "   0.07874823361635208,\n",
              "   -0.03087536245584488,\n",
              "   0.24947310984134674,\n",
              "   -0.32149556279182434,\n",
              "   0.8830130696296692,\n",
              "   0.3802318572998047,\n",
              "   0.8233019113540649,\n",
              "   0.11808441579341888,\n",
              "   -0.34758704900741577,\n",
              "   -0.10504553467035294,\n",
              "   0.10057923942804337,\n",
              "   -0.03553431108593941,\n",
              "   -0.26688703894615173,\n",
              "   0.25634765625,\n",
              "   -0.43191102147102356,\n",
              "   -0.22301088273525238,\n",
              "   -0.3680444359779358,\n",
              "   -0.15739554166793823,\n",
              "   0.1683219075202942,\n",
              "   0.014968408271670341,\n",
              "   -0.27698805928230286,\n",
              "   -0.6017554998397827,\n",
              "   0.1284019947052002,\n",
              "   0.31826862692832947,\n",
              "   0.16143390536308289,\n",
              "   0.11810598522424698,\n",
              "   0.24400505423545837,\n",
              "   -0.12141910195350647,\n",
              "   0.1521177738904953,\n",
              "   -0.4715666174888611,\n",
              "   -0.23999576270580292,\n",
              "   -0.26751211285591125,\n",
              "   -0.43797898292541504,\n",
              "   -0.05653075873851776,\n",
              "   -1.0570347309112549,\n",
              "   -0.5074775218963623,\n",
              "   -0.4009847342967987,\n",
              "   -0.22656875848770142,\n",
              "   -0.2789624333381653,\n",
              "   -0.45500999689102173,\n",
              "   0.8586691617965698,\n",
              "   -0.3104006350040436,\n",
              "   -0.43454667925834656,\n",
              "   0.4336555004119873,\n",
              "   0.07050993293523788,\n",
              "   -0.6887553334236145,\n",
              "   -0.5871007442474365,\n",
              "   0.039278849959373474,\n",
              "   0.11106185615062714,\n",
              "   0.5842710137367249,\n",
              "   0.3350711166858673,\n",
              "   0.31937897205352783,\n",
              "   -0.37735432386398315,\n",
              "   0.08425699919462204,\n",
              "   0.881715714931488,\n",
              "   -0.02435729093849659,\n",
              "   -0.35412976145744324,\n",
              "   -0.1532151848077774,\n",
              "   -0.2309686243534088,\n",
              "   -0.192172110080719,\n",
              "   -0.5492733120918274,\n",
              "   0.27904215455055237,\n",
              "   0.495951384305954,\n",
              "   0.03995811566710472,\n",
              "   0.192648246884346,\n",
              "   0.7324711680412292,\n",
              "   -0.7850898504257202,\n",
              "   0.29922086000442505,\n",
              "   0.08097958564758301,\n",
              "   -0.5648394823074341,\n",
              "   -0.19529657065868378,\n",
              "   0.14444223046302795,\n",
              "   0.5468250513076782,\n",
              "   -0.5241602659225464,\n",
              "   -0.09714675694704056,\n",
              "   0.0823427364230156,\n",
              "   -0.0697997510433197,\n",
              "   0.3711663782596588,\n",
              "   -0.25834816694259644,\n",
              "   0.2586261034011841,\n",
              "   0.05979005992412567,\n",
              "   0.017477627843618393,\n",
              "   -0.46212834119796753,\n",
              "   0.05938136205077171,\n",
              "   0.31675297021865845,\n",
              "   0.24864725768566132,\n",
              "   0.20802204310894012,\n",
              "   -0.03632078319787979,\n",
              "   -1.1678274869918823,\n",
              "   -4.063625812530518,\n",
              "   -0.15257419645786285,\n",
              "   -0.7040587067604065,\n",
              "   0.3236781656742096,\n",
              "   -0.0366639643907547,\n",
              "   0.27324751019477844,\n",
              "   -0.0002083068247884512,\n",
              "   -0.22945788502693176,\n",
              "   -1.251502513885498,\n",
              "   -0.41162213683128357,\n",
              "   -0.7577388286590576,\n",
              "   0.0505463182926178,\n",
              "   0.9932500123977661,\n",
              "   0.7079375386238098,\n",
              "   0.6782163977622986,\n",
              "   0.6520742774009705,\n",
              "   0.1587386578321457,\n",
              "   -0.23837615549564362,\n",
              "   0.019557833671569824,\n",
              "   0.9026753306388855,\n",
              "   -0.09279651194810867,\n",
              "   -0.4296877086162567,\n",
              "   0.09829769283533096,\n",
              "   0.059482645243406296,\n",
              "   0.6838979125022888,\n",
              "   0.823232889175415,\n",
              "   -0.38507068157196045,\n",
              "   0.07661968469619751,\n",
              "   -0.03665405884385109,\n",
              "   -0.1402391642332077,\n",
              "   -0.338559627532959,\n",
              "   -0.22606182098388672,\n",
              "   -0.007509833667427301,\n",
              "   -0.3985048532485962,\n",
              "   0.34197214245796204,\n",
              "   0.22465229034423828,\n",
              "   -0.08712197095155716,\n",
              "   -0.09652850031852722,\n",
              "   0.09031421691179276,\n",
              "   0.49470266699790955,\n",
              "   0.044815704226493835,\n",
              "   -1.1069949865341187,\n",
              "   -0.29042157530784607,\n",
              "   0.6179378032684326,\n",
              "   0.9604945182800293,\n",
              "   0.14016689360141754,\n",
              "   0.17025494575500488,\n",
              "   -0.6204338073730469,\n",
              "   -0.031793076545000076,\n",
              "   0.1360105574131012,\n",
              "   0.42276060581207275,\n",
              "   0.00526803731918335,\n",
              "   0.10681260377168655,\n",
              "   -0.18112359941005707,\n",
              "   0.018167167901992798,\n",
              "   -0.6441609859466553,\n",
              "   0.5980715155601501,\n",
              "   1.200028419494629,\n",
              "   0.045207951217889786,\n",
              "   -0.5968292951583862,\n",
              "   -0.12980881333351135,\n",
              "   -0.41132909059524536,\n",
              "   -0.6329436898231506,\n",
              "   -0.04708196967840195,\n",
              "   -0.5168934464454651,\n",
              "   -0.3939782381057739,\n",
              "   -0.3386833667755127,\n",
              "   0.21461547911167145,\n",
              "   0.5986295342445374,\n",
              "   0.37763431668281555,\n",
              "   0.004472207743674517,\n",
              "   0.5689431428909302,\n",
              "   0.28319472074508667,\n",
              "   -0.9524162411689758,\n",
              "   0.15851594507694244,\n",
              "   0.2917938828468323,\n",
              "   -0.19376415014266968,\n",
              "   0.008956648409366608,\n",
              "   0.4555690884590149,\n",
              "   0.3394802510738373,\n",
              "   -0.5787521600723267,\n",
              "   -0.6484441161155701,\n",
              "   0.20033495128154755,\n",
              "   -0.29024580121040344,\n",
              "   0.48210662603378296,\n",
              "   -1.3537657260894775,\n",
              "   -0.1949649453163147,\n",
              "   0.23023152351379395,\n",
              "   -0.0858127698302269,\n",
              "   -0.4928012788295746,\n",
              "   0.421286016702652,\n",
              "   0.2696177065372467,\n",
              "   0.9019490480422974,\n",
              "   0.21069075167179108,\n",
              "   0.31548234820365906,\n",
              "   0.12855520844459534,\n",
              "   0.3158186376094818,\n",
              "   -0.40431898832321167,\n",
              "   0.6567494869232178,\n",
              "   -0.31313785910606384,\n",
              "   0.15016454458236694,\n",
              "   0.05966106057167053,\n",
              "   0.90872722864151,\n",
              "   -0.3401755094528198,\n",
              "   0.09361006319522858,\n",
              "   -0.4644891619682312,\n",
              "   -0.6389834880828857,\n",
              "   0.059839412569999695,\n",
              "   -0.2934081554412842,\n",
              "   0.40866976976394653,\n",
              "   -0.22117426991462708,\n",
              "   0.23209239542484283,\n",
              "   0.854611337184906,\n",
              "   -0.3987685441970825,\n",
              "   0.008439503610134125,\n",
              "   0.03339538723230362,\n",
              "   0.34390780329704285,\n",
              "   0.7284137010574341,\n",
              "   -0.6049224734306335,\n",
              "   -0.11570332944393158,\n",
              "   -0.04211531952023506,\n",
              "   0.7696918845176697,\n",
              "   -0.06268803030252457,\n",
              "   -0.7453224658966064,\n",
              "   -0.5205518007278442,\n",
              "   0.006800916511565447,\n",
              "   -0.26245108246803284,\n",
              "   -0.4174098074436188,\n",
              "   -0.7275772094726562,\n",
              "   -0.018907232210040092,\n",
              "   -0.2814788818359375,\n",
              "   -0.16917262971401215,\n",
              "   0.15839582681655884,\n",
              "   0.44452905654907227,\n",
              "   0.14212818443775177,\n",
              "   -0.13917763531208038,\n",
              "   -0.10778307169675827,\n",
              "   -0.6532713770866394,\n",
              "   0.36662957072257996,\n",
              "   0.4626486599445343,\n",
              "   0.5546636581420898,\n",
              "   -0.001815562485717237,\n",
              "   0.09289496392011642,\n",
              "   -0.011132110841572285,\n",
              "   -0.49017345905303955,\n",
              "   0.8632257580757141,\n",
              "   -0.08547773957252502,\n",
              "   -0.19290286302566528,\n",
              "   0.2922203540802002,\n",
              "   0.4712185859680176,\n",
              "   -0.8181086182594299,\n",
              "   -0.07473792880773544,\n",
              "   -0.5368252396583557,\n",
              "   -0.053147755563259125,\n",
              "   0.19846494495868683,\n",
              "   -0.3986794054508209,\n",
              "   0.23037081956863403,\n",
              "   -0.5128790140151978,\n",
              "   0.7169834971427917,\n",
              "   -0.08313407748937607,\n",
              "   -0.4002900719642639,\n",
              "   0.30270814895629883,\n",
              "   0.07814895361661911,\n",
              "   0.340907484292984,\n",
              "   0.28501516580581665,\n",
              "   0.38553139567375183,\n",
              "   -0.02460091933608055,\n",
              "   0.028679538518190384,\n",
              "   0.3525814712047577,\n",
              "   0.15018464624881744,\n",
              "   -0.42845094203948975,\n",
              "   -0.19682766497135162,\n",
              "   0.23999935388565063,\n",
              "   -0.1695740669965744,\n",
              "   -0.5303823351860046,\n",
              "   0.3002822697162628,\n",
              "   0.2252100110054016,\n",
              "   0.3407149016857147,\n",
              "   -0.1843876838684082,\n",
              "   0.4302738606929779,\n",
              "   -0.2911485433578491,\n",
              "   -0.09757383912801743,\n",
              "   -0.17854270339012146,\n",
              "   -0.15950465202331543,\n",
              "   0.6757081747055054,\n",
              "   0.024192294105887413,\n",
              "   -0.15316441655158997,\n",
              "   0.39157238602638245,\n",
              "   0.03862748667597771,\n",
              "   0.14359556138515472,\n",
              "   0.0950433686375618,\n",
              "   -0.10839657485485077,\n",
              "   -0.4167167842388153,\n",
              "   -0.6383026838302612,\n",
              "   -0.391653448343277,\n",
              "   -0.5089497566223145,\n",
              "   0.8012180328369141,\n",
              "   -0.03511448949575424,\n",
              "   0.02683105692267418,\n",
              "   -0.12137560546398163,\n",
              "   -0.09151141345500946,\n",
              "   -0.22161082923412323,\n",
              "   -0.6097090840339661,\n",
              "   0.18007515370845795,\n",
              "   0.28008008003234863,\n",
              "   -0.4538435935974121,\n",
              "   0.46607735753059387,\n",
              "   0.6294801235198975,\n",
              "   -0.4822443127632141,\n",
              "   0.40588998794555664,\n",
              "   -0.8106991648674011,\n",
              "   -0.5145951509475708,\n",
              "   0.3311493396759033,\n",
              "   0.01698511280119419,\n",
              "   0.1503768414258957,\n",
              "   0.09822062402963638,\n",
              "   0.4280948042869568,\n",
              "   0.17345030605793,\n",
              "   -0.46402475237846375,\n",
              "   0.015403572469949722,\n",
              "   -0.28633883595466614,\n",
              "   0.01612899638712406,\n",
              "   -0.43639934062957764,\n",
              "   -0.2020995169878006,\n",
              "   -0.7981898784637451,\n",
              "   0.002539985813200474,\n",
              "   0.2703523635864258,\n",
              "   -0.13112838566303253,\n",
              "   -0.47803962230682373,\n",
              "   -0.35912615060806274,\n",
              "   0.15411631762981415,\n",
              "   -0.6642230153083801,\n",
              "   0.1300189197063446,\n",
              "   -0.12335668504238129,\n",
              "   -0.6462595462799072,\n",
              "   -0.02099655568599701,\n",
              "   -0.11195827275514603,\n",
              "   0.08321269601583481,\n",
              "   0.23010534048080444,\n",
              "   0.16064950823783875,\n",
              "   0.049358490854501724,\n",
              "   -0.3390776813030243,\n",
              "   -0.11391086131334305,\n",
              "   -1.2447271347045898,\n",
              "   -0.2263258844614029,\n",
              "   -0.17602480947971344,\n",
              "   -0.052677422761917114,\n",
              "   0.3794766366481781,\n",
              "   -0.6882944703102112,\n",
              "   0.09268701076507568,\n",
              "   0.34943288564682007,\n",
              "   -0.09563044458627701,\n",
              "   0.08998327702283859,\n",
              "   -0.26897063851356506,\n",
              "   0.32970577478408813,\n",
              "   -0.23129737377166748,\n",
              "   -0.6343390345573425,\n",
              "   0.480663001537323,\n",
              "   0.0017111467896029353,\n",
              "   -0.843987762928009,\n",
              "   0.32233911752700806,\n",
              "   -0.28965795040130615,\n",
              "   0.09435476362705231,\n",
              "   -0.02604643628001213,\n",
              "   0.12989771366119385,\n",
              "   0.13167604804039001,\n",
              "   0.6147549748420715,\n",
              "   0.5363348722457886,\n",
              "   0.708130419254303,\n",
              "   -0.26091185212135315,\n",
              "   0.27279213070869446,\n",
              "   -0.04005243256688118,\n",
              "   0.4676908850669861,\n",
              "   -0.43704459071159363,\n",
              "   -0.20039905607700348,\n",
              "   -0.07097896188497543,\n",
              "   -0.12218448519706726,\n",
              "   -0.021411167457699776,\n",
              "   0.5280555486679077,\n",
              "   -0.020828325301408768,\n",
              "   -0.19681459665298462,\n",
              "   0.46194490790367126,\n",
              "   0.4224280118942261,\n",
              "   -0.2115539014339447,\n",
              "   0.4367983341217041,\n",
              "   -0.13441979885101318,\n",
              "   0.08380190283060074,\n",
              "   1.0468775033950806,\n",
              "   0.0572488009929657,\n",
              "   -0.18703432381153107,\n",
              "   -0.24260109663009644,\n",
              "   0.3099946081638336,\n",
              "   0.22821834683418274,\n",
              "   0.028681067749857903,\n",
              "   -0.1253141462802887,\n",
              "   -0.04780016839504242,\n",
              "   0.6976090669631958,\n",
              "   -0.21836337447166443,\n",
              "   0.4812687933444977,\n",
              "   0.20872360467910767,\n",
              "   -0.9582597613334656,\n",
              "   0.06255082041025162,\n",
              "   0.24915196001529694,\n",
              "   0.1548067033290863,\n",
              "   0.20366986095905304,\n",
              "   0.22404038906097412,\n",
              "   0.48556408286094666,\n",
              "   -0.496106892824173,\n",
              "   0.6746116876602173,\n",
              "   -0.16203400492668152,\n",
              "   0.12323574721813202,\n",
              "   -0.15109221637248993,\n",
              "   0.6215981245040894,\n",
              "   0.3636349141597748,\n",
              "   -0.8018441796302795,\n",
              "   -0.496987521648407,\n",
              "   0.08491788059473038,\n",
              "   -0.2096216231584549,\n",
              "   0.10172142833471298,\n",
              "   -0.08576805889606476,\n",
              "   0.06588448584079742,\n",
              "   -0.4859429895877838,\n",
              "   0.25186029076576233,\n",
              "   0.06886079907417297,\n",
              "   0.32808613777160645,\n",
              "   0.2658023536205292,\n",
              "   -0.0876072570681572,\n",
              "   -0.1932317465543747,\n",
              "   -0.11110765486955643,\n",
              "   0.4210483729839325,\n",
              "   0.06769111007452011,\n",
              "   0.17187732458114624,\n",
              "   -0.33590415120124817,\n",
              "   0.19517390429973602,\n",
              "   -0.15667274594306946,\n",
              "   -0.003921094816178083,\n",
              "   0.897478461265564,\n",
              "   0.2407921850681305,\n",
              "   -0.067506343126297,\n",
              "   -0.5387303233146667,\n",
              "   -0.3285081386566162,\n",
              "   0.5567528605461121,\n",
              "   0.296531081199646,\n",
              "   1.2082107067108154,\n",
              "   0.37576621770858765,\n",
              "   0.11214464157819748,\n",
              "   -0.10421113669872284,\n",
              "   0.010799122042953968,\n",
              "   0.18251167237758636,\n",
              "   0.3008037209510803,\n",
              "   0.5639985799789429,\n",
              "   0.9050697684288025,\n",
              "   0.06188220530748367,\n",
              "   -0.12907598912715912,\n",
              "   0.5857676267623901,\n",
              "   -0.05483115464448929,\n",
              "   0.3125166594982147,\n",
              "   0.5317208170890808,\n",
              "   -0.08590885996818542,\n",
              "   0.6737102270126343,\n",
              "   0.3195798099040985,\n",
              "   0.30091139674186707,\n",
              "   -0.3239096701145172,\n",
              "   -0.0761454626917839,\n",
              "   -0.12257643043994904,\n",
              "   0.3395945131778717,\n",
              "   0.3654938042163849,\n",
              "   -0.20403897762298584,\n",
              "   0.058733392506837845,\n",
              "   0.29946988821029663,\n",
              "   -0.04841267690062523,\n",
              "   0.059543635696172714,\n",
              "   0.3407398462295532,\n",
              "   -0.43157854676246643,\n",
              "   0.16696125268936157,\n",
              "   0.1399787962436676,\n",
              "   -0.37550023198127747,\n",
              "   -0.4275037944316864,\n",
              "   -0.09054309874773026,\n",
              "   0.2718639671802521,\n",
              "   0.2303496152162552,\n",
              "   0.3356155455112457,\n",
              "   -0.7032436728477478,\n",
              "   -0.10617458075284958,\n",
              "   -0.3641911745071411,\n",
              "   -0.7300217747688293,\n",
              "   -0.19388872385025024,\n",
              "   0.4871338903903961,\n",
              "   0.6056928634643555,\n",
              "   -0.2244294285774231,\n",
              "   0.029128262773156166,\n",
              "   -0.10595422238111496,\n",
              "   0.11585605889558792,\n",
              "   -0.9291768074035645,\n",
              "   0.3988727927207947,\n",
              "   -0.31884831190109253,\n",
              "   0.05605132877826691,\n",
              "   0.2520662844181061,\n",
              "   0.38276001811027527,\n",
              "   -0.15607942640781403,\n",
              "   0.4133145809173584,\n",
              "   0.23899824917316437,\n",
              "   -0.09465830028057098,\n",
              "   -0.23925620317459106,\n",
              "   -0.47648292779922485,\n",
              "   0.5462372899055481,\n",
              "   -0.6986439228057861,\n",
              "   -0.3398146331310272,\n",
              "   -0.4707222282886505,\n",
              "   -0.2837948799133301,\n",
              "   -0.03118829056620598,\n",
              "   0.7159548401832581,\n",
              "   -0.4768258035182953,\n",
              "   -0.39096635580062866,\n",
              "   -0.40585997700691223,\n",
              "   -0.10969778895378113,\n",
              "   0.48032501339912415,\n",
              "   -0.1635141670703888,\n",
              "   0.050369083881378174,\n",
              "   0.2197674661874771,\n",
              "   -0.07217887043952942,\n",
              "   -0.26108846068382263,\n",
              "   -0.05526435375213623,\n",
              "   -0.08167943358421326,\n",
              "   -0.03158089518547058,\n",
              "   -0.6774318218231201,\n",
              "   0.22284159064292908,\n",
              "   0.1852085441350937,\n",
              "   -0.4652094542980194,\n",
              "   -0.16866497695446014,\n",
              "   -0.6165667176246643,\n",
              "   -0.04643120989203453,\n",
              "   0.5021036863327026,\n",
              "   -0.25973841547966003,\n",
              "   -0.529176652431488,\n",
              "   -0.14793914556503296,\n",
              "   -0.28673285245895386,\n",
              "   0.18604503571987152,\n",
              "   -0.3779892325401306,\n",
              "   -0.584367036819458,\n",
              "   0.08874723315238953,\n",
              "   0.25602999329566956,\n",
              "   0.49717217683792114,\n",
              "   -0.508374035358429,\n",
              "   0.7352001070976257,\n",
              "   -0.008982348255813122,\n",
              "   -0.5575681328773499,\n",
              "   -0.6419658660888672,\n",
              "   0.6920607686042786,\n",
              "   0.04763436317443848,\n",
              "   -0.07604850083589554,\n",
              "   -0.11207102984189987,\n",
              "   0.24571900069713593,\n",
              "   -0.45990994572639465,\n",
              "   0.37221989035606384,\n",
              "   -0.18746018409729004,\n",
              "   0.17760513722896576,\n",
              "   -0.05167751386761665,\n",
              "   0.2916170060634613,\n",
              "   0.03155118599534035],\n",
              "  [0.3012485206127167,\n",
              "   0.6142287254333496,\n",
              "   0.2892265319824219,\n",
              "   0.4388444721698761,\n",
              "   0.24167978763580322,\n",
              "   -0.5282638669013977,\n",
              "   -0.6562857627868652,\n",
              "   0.9579881429672241,\n",
              "   -0.7039216756820679,\n",
              "   0.15096594393253326,\n",
              "   0.4500768184661865,\n",
              "   -0.2906167507171631,\n",
              "   -0.3415276110172272,\n",
              "   0.6203675270080566,\n",
              "   -0.05406266823410988,\n",
              "   0.5107600092887878,\n",
              "   0.401999831199646,\n",
              "   0.07847467064857483,\n",
              "   -0.0782473087310791,\n",
              "   0.40695324540138245,\n",
              "   0.07435030490159988,\n",
              "   -0.13979840278625488,\n",
              "   -0.2409902662038803,\n",
              "   0.10900235921144485,\n",
              "   0.033628057688474655,\n",
              "   -0.5254401564598083,\n",
              "   0.28747501969337463,\n",
              "   -0.21368317306041718,\n",
              "   0.044777631759643555,\n",
              "   0.1832152009010315,\n",
              "   0.05676136165857315,\n",
              "   0.09935586154460907,\n",
              "   -0.38957276940345764,\n",
              "   -0.6946349143981934,\n",
              "   0.516364336013794,\n",
              "   0.21162287890911102,\n",
              "   0.4136802554130554,\n",
              "   0.29577890038490295,\n",
              "   -0.043843481689691544,\n",
              "   -0.041372254490852356,\n",
              "   -0.5621918439865112,\n",
              "   -0.44817405939102173,\n",
              "   0.16768738627433777,\n",
              "   0.058400027453899384,\n",
              "   -0.31203681230545044,\n",
              "   -0.3601525127887726,\n",
              "   0.8657073974609375,\n",
              "   0.2719604969024658,\n",
              "   0.4263133704662323,\n",
              "   -0.11563502997159958,\n",
              "   -0.49958834052085876,\n",
              "   0.13035522401332855,\n",
              "   0.1722109168767929,\n",
              "   -0.26286736130714417,\n",
              "   -0.026773890480399132,\n",
              "   1.0931581258773804,\n",
              "   0.11277689039707184,\n",
              "   -0.6777496933937073,\n",
              "   -0.2944612205028534,\n",
              "   0.21384739875793457,\n",
              "   0.5599310398101807,\n",
              "   -0.18147830665111542,\n",
              "   -0.062151841819286346,\n",
              "   -0.5007854104042053,\n",
              "   -0.03523534908890724,\n",
              "   0.26408106088638306,\n",
              "   0.16553202271461487,\n",
              "   -0.14729225635528564,\n",
              "   -0.2100333273410797,\n",
              "   0.06223486736416817,\n",
              "   -0.5056454539299011,\n",
              "   -0.6398830413818359,\n",
              "   -0.36814215779304504,\n",
              "   0.28552159667015076,\n",
              "   -0.7464629411697388,\n",
              "   -0.022881103679537773,\n",
              "   -0.046179261058568954,\n",
              "   0.170920729637146,\n",
              "   0.2822287976741791,\n",
              "   0.5432115793228149,\n",
              "   0.03137907385826111,\n",
              "   0.7728011012077332,\n",
              "   -0.2520214915275574,\n",
              "   0.3300410509109497,\n",
              "   0.2590303122997284,\n",
              "   -0.17024552822113037,\n",
              "   -0.7840874195098877,\n",
              "   -0.14463065564632416,\n",
              "   0.36276137828826904,\n",
              "   1.1159381866455078,\n",
              "   -0.08127538859844208,\n",
              "   0.15864183008670807,\n",
              "   0.10117880254983902,\n",
              "   0.23860697448253632,\n",
              "   -0.19569247961044312,\n",
              "   -0.4230518043041229,\n",
              "   -0.2344050407409668,\n",
              "   0.6472070813179016,\n",
              "   -0.40854644775390625,\n",
              "   0.669223427772522,\n",
              "   0.2052803784608841,\n",
              "   -1.0157511234283447,\n",
              "   0.2654733657836914,\n",
              "   -0.010790432803332806,\n",
              "   0.6541845798492432,\n",
              "   -0.057072483003139496,\n",
              "   0.4365907609462738,\n",
              "   0.024974603205919266,\n",
              "   -0.12929627299308777,\n",
              "   -0.17506864666938782,\n",
              "   -0.3286406397819519,\n",
              "   -0.8537306189537048,\n",
              "   0.5373208522796631,\n",
              "   -0.4961366355419159,\n",
              "   0.10511599481105804,\n",
              "   0.37141674757003784,\n",
              "   0.012856141664087772,\n",
              "   -0.15655548870563507,\n",
              "   0.1394900232553482,\n",
              "   -0.21836762130260468,\n",
              "   -0.15840916335582733,\n",
              "   -0.20967549085617065,\n",
              "   -0.04807260259985924,\n",
              "   0.9022949934005737,\n",
              "   0.15349727869033813,\n",
              "   -0.2547389566898346,\n",
              "   -0.062375083565711975,\n",
              "   -0.31971225142478943,\n",
              "   0.16492000222206116,\n",
              "   -0.5585843324661255,\n",
              "   0.7684793472290039,\n",
              "   0.4320050776004791,\n",
              "   0.6199868321418762,\n",
              "   -1.0878169536590576,\n",
              "   0.03958379849791527,\n",
              "   0.34776070713996887,\n",
              "   0.413713276386261,\n",
              "   -0.442202091217041,\n",
              "   -0.6372241377830505,\n",
              "   -0.027329416945576668,\n",
              "   0.6140305399894714,\n",
              "   0.5766873955726624,\n",
              "   0.986700713634491,\n",
              "   0.253978967666626,\n",
              "   0.48914453387260437,\n",
              "   0.1450282782316208,\n",
              "   -0.29076695442199707,\n",
              "   -0.45417362451553345,\n",
              "   -0.0708608627319336,\n",
              "   -0.034775786101818085,\n",
              "   -0.2774057686328888,\n",
              "   -0.6679014563560486,\n",
              "   0.09834633022546768,\n",
              "   -0.24829182028770447,\n",
              "   0.2324644774198532,\n",
              "   -0.506776750087738,\n",
              "   -0.2557274401187897,\n",
              "   -0.16833730041980743,\n",
              "   0.9031237363815308,\n",
              "   0.2397618591785431,\n",
              "   0.39590752124786377,\n",
              "   -0.16996300220489502,\n",
              "   -0.009675665758550167,\n",
              "   0.47749975323677063,\n",
              "   -0.006816786713898182,\n",
              "   -0.014060081914067268,\n",
              "   -0.050625279545784,\n",
              "   0.5117493867874146,\n",
              "   0.2591875493526459,\n",
              "   0.10544105619192123,\n",
              "   -0.23449432849884033,\n",
              "   0.11372482776641846,\n",
              "   0.7511260509490967,\n",
              "   -0.36982372403144836,\n",
              "   -0.48619917035102844,\n",
              "   -0.034691907465457916,\n",
              "   0.32884520292282104,\n",
              "   0.31695500016212463,\n",
              "   0.3943384885787964,\n",
              "   0.34405699372291565,\n",
              "   -0.44039425253868103,\n",
              "   -0.15730999410152435,\n",
              "   -0.016573945060372353,\n",
              "   -0.10367479920387268,\n",
              "   0.2418430596590042,\n",
              "   -0.39972206950187683,\n",
              "   0.9058849215507507,\n",
              "   -0.019452853128314018,\n",
              "   -0.2507012188434601,\n",
              "   0.2553952932357788,\n",
              "   -0.27612337470054626,\n",
              "   0.3075874149799347,\n",
              "   -0.2204071283340454,\n",
              "   0.023391056805849075,\n",
              "   0.2688695788383484,\n",
              "   -0.519245445728302,\n",
              "   -0.6591774225234985,\n",
              "   -0.5908240079879761,\n",
              "   0.3171762228012085,\n",
              "   0.2529607117176056,\n",
              "   0.029707957059144974,\n",
              "   0.1532907485961914,\n",
              "   0.27616605162620544,\n",
              "   -0.07406263053417206,\n",
              "   -0.550203263759613,\n",
              "   0.10556773096323013,\n",
              "   -0.19026866555213928,\n",
              "   -0.2011878490447998,\n",
              "   -0.18242321908473969,\n",
              "   -0.32229888439178467,\n",
              "   -0.3873012959957123,\n",
              "   0.8520625233650208,\n",
              "   0.2781539261341095,\n",
              "   0.008955663070082664,\n",
              "   -0.2896876335144043,\n",
              "   -0.059800297021865845,\n",
              "   0.29860618710517883,\n",
              "   -0.27693191170692444,\n",
              "   -0.26493048667907715,\n",
              "   0.14971517026424408,\n",
              "   0.08108959347009659,\n",
              "   0.16333730518817902,\n",
              "   -0.37306898832321167,\n",
              "   0.68087238073349,\n",
              "   0.3114982843399048,\n",
              "   0.8589572906494141,\n",
              "   0.28272509574890137,\n",
              "   -0.3393602669239044,\n",
              "   -0.12840469181537628,\n",
              "   0.42369237542152405,\n",
              "   0.12759697437286377,\n",
              "   -0.39501819014549255,\n",
              "   0.38736438751220703,\n",
              "   -0.2864214777946472,\n",
              "   -0.4684337377548218,\n",
              "   -0.09367630630731583,\n",
              "   -0.10407909005880356,\n",
              "   0.22784489393234253,\n",
              "   0.2967902421951294,\n",
              "   -0.10003501176834106,\n",
              "   -0.6161572337150574,\n",
              "   0.15785764157772064,\n",
              "   0.47290554642677307,\n",
              "   -0.07156068831682205,\n",
              "   0.13251975178718567,\n",
              "   0.08123970776796341,\n",
              "   -0.005593771114945412,\n",
              "   0.12938500940799713,\n",
              "   -0.5370584726333618,\n",
              "   -0.7009092569351196,\n",
              "   -0.07183843851089478,\n",
              "   -1.0181715488433838,\n",
              "   -0.0893222838640213,\n",
              "   -0.7828116416931152,\n",
              "   -0.4760410189628601,\n",
              "   -0.5332272052764893,\n",
              "   -0.3320789039134979,\n",
              "   0.05459459125995636,\n",
              "   0.0026776641607284546,\n",
              "   0.409485399723053,\n",
              "   0.3163195252418518,\n",
              "   -0.46586689352989197,\n",
              "   -0.18924875557422638,\n",
              "   0.13452808558940887,\n",
              "   -0.6931136846542358,\n",
              "   -0.7795423865318298,\n",
              "   0.2599002420902252,\n",
              "   0.5242187976837158,\n",
              "   0.32018518447875977,\n",
              "   -0.06881162524223328,\n",
              "   0.12469486147165298,\n",
              "   -0.5700678825378418,\n",
              "   0.2329159677028656,\n",
              "   0.9291583299636841,\n",
              "   0.08639209717512131,\n",
              "   -0.2674010992050171,\n",
              "   -0.20876795053482056,\n",
              "   0.06435953080654144,\n",
              "   -0.18955357372760773,\n",
              "   -0.7018195986747742,\n",
              "   0.46003246307373047,\n",
              "   0.5303494334220886,\n",
              "   -0.23197631537914276,\n",
              "   0.09869514405727386,\n",
              "   0.2941667139530182,\n",
              "   -0.6526498198509216,\n",
              "   0.459062397480011,\n",
              "   0.08028025180101395,\n",
              "   -0.26421281695365906,\n",
              "   0.15708063542842865,\n",
              "   0.0662870705127716,\n",
              "   0.48593345284461975,\n",
              "   -0.5113756060600281,\n",
              "   -0.5092441439628601,\n",
              "   0.2574068307876587,\n",
              "   -0.23146310448646545,\n",
              "   0.4003031551837921,\n",
              "   -0.2516583800315857,\n",
              "   -0.3735548257827759,\n",
              "   -0.13655847311019897,\n",
              "   -0.21145983040332794,\n",
              "   -0.16349561512470245,\n",
              "   0.18562109768390656,\n",
              "   0.6997150778770447,\n",
              "   0.12651222944259644,\n",
              "   0.2527446746826172,\n",
              "   0.0009185325470753014,\n",
              "   -1.2978051900863647,\n",
              "   -4.082281589508057,\n",
              "   0.015660090371966362,\n",
              "   -0.5124518871307373,\n",
              "   0.04550981894135475,\n",
              "   0.1289781928062439,\n",
              "   0.006304455921053886,\n",
              "   -0.3128431439399719,\n",
              "   -0.520760715007782,\n",
              "   -1.1144399642944336,\n",
              "   -0.05373570695519447,\n",
              "   -0.3452032506465912,\n",
              "   -0.2591330111026764,\n",
              "   0.39610061049461365,\n",
              "   0.5299168825149536,\n",
              "   0.5313812494277954,\n",
              "   0.3557617664337158,\n",
              "   0.2021263837814331,\n",
              "   -0.2460125833749771,\n",
              "   -0.3318527340888977,\n",
              "   0.4717119038105011,\n",
              "   -0.3755311369895935,\n",
              "   -0.7472882866859436,\n",
              "   0.07608082890510559,\n",
              "   0.030696753412485123,\n",
              "   0.8227478861808777,\n",
              "   0.7344170212745667,\n",
              "   -0.23100657761096954,\n",
              "   0.37499818205833435,\n",
              "   -0.3539036810398102,\n",
              "   0.21876032650470734,\n",
              "   -0.09086527675390244,\n",
              "   -0.5151094198226929,\n",
              "   -0.07793934643268585,\n",
              "   -0.07609144598245621,\n",
              "   -0.054110314697027206,\n",
              "   0.12097685039043427,\n",
              "   0.08147973567247391,\n",
              "   -0.06718197464942932,\n",
              "   -0.16202615201473236,\n",
              "   0.2926848828792572,\n",
              "   0.04565787315368652,\n",
              "   -0.995793342590332,\n",
              "   -0.27474021911621094,\n",
              "   0.2850742042064667,\n",
              "   1.034299373626709,\n",
              "   -0.4136730432510376,\n",
              "   0.07893823087215424,\n",
              "   -0.537548303604126,\n",
              "   0.11969505995512009,\n",
              "   0.1846073716878891,\n",
              "   0.22748170793056488,\n",
              "   -0.003306388622149825,\n",
              "   -0.11201684176921844,\n",
              "   -0.5322847366333008,\n",
              "   -0.5698720812797546,\n",
              "   -0.6972585916519165,\n",
              "   0.7054131627082825,\n",
              "   0.5219330191612244,\n",
              "   -0.14945250749588013,\n",
              "   -0.6078149676322937,\n",
              "   0.032596588134765625,\n",
              "   -0.3998227119445801,\n",
              "   -1.1202491521835327,\n",
              "   -0.21389159560203552,\n",
              "   0.03469293564558029,\n",
              "   -0.7283520698547363,\n",
              "   -0.22069129347801208,\n",
              "   0.032212790101766586,\n",
              "   0.584142804145813,\n",
              "   0.2321011871099472,\n",
              "   -0.028833501040935516,\n",
              "   0.3793441355228424,\n",
              "   -0.0367782898247242,\n",
              "   -1.0514308214187622,\n",
              "   -0.03546498343348503,\n",
              "   0.5251981019973755,\n",
              "   0.18979059159755707,\n",
              "   -0.2159903645515442,\n",
              "   0.4499906301498413,\n",
              "   0.3862750828266144,\n",
              "   -0.7143847942352295,\n",
              "   -0.4228213131427765,\n",
              "   0.2901351749897003,\n",
              "   -0.16592198610305786,\n",
              "   0.33473068475723267,\n",
              "   -1.223557472229004,\n",
              "   0.12492048740386963,\n",
              "   -0.32356148958206177,\n",
              "   -0.26048389077186584,\n",
              "   -0.5140066146850586,\n",
              "   0.09573627263307571,\n",
              "   0.18196874856948853,\n",
              "   0.2577044665813446,\n",
              "   0.1878904104232788,\n",
              "   0.1577855795621872,\n",
              "   0.39479467272758484,\n",
              "   0.2539752721786499,\n",
              "   -0.19915127754211426,\n",
              "   0.14989925920963287,\n",
              "   -0.2621971368789673,\n",
              "   0.12662476301193237,\n",
              "   0.07078151404857635,\n",
              "   0.8862404227256775,\n",
              "   -0.15998123586177826,\n",
              "   0.30223315954208374,\n",
              "   -0.3065999448299408,\n",
              "   -0.8445015549659729,\n",
              "   0.28417715430259705,\n",
              "   -0.10745541751384735,\n",
              "   0.3592773675918579,\n",
              "   -0.15789853036403656,\n",
              "   -0.17461121082305908,\n",
              "   0.8698923587799072,\n",
              "   -0.5964503288269043,\n",
              "   0.14330856502056122,\n",
              "   0.28962069749832153,\n",
              "   0.7230535745620728,\n",
              "   0.7663582563400269,\n",
              "   -0.5577402710914612,\n",
              "   0.0533006489276886,\n",
              "   -0.03071219101548195,\n",
              "   0.6727609038352966,\n",
              "   -0.16660098731517792,\n",
              "   -0.2615871727466583,\n",
              "   -0.30702561140060425,\n",
              "   0.1295420229434967,\n",
              "   -0.2891426682472229,\n",
              "   -0.48097744584083557,\n",
              "   -0.5625824332237244,\n",
              "   0.03524082526564598,\n",
              "   -0.2546136975288391,\n",
              "   -0.05724214017391205,\n",
              "   0.19068418443202972,\n",
              "   0.45966553688049316,\n",
              "   0.46117308735847473,\n",
              "   0.08117975294589996,\n",
              "   0.1536363661289215,\n",
              "   -0.5056493878364563,\n",
              "   0.40700697898864746,\n",
              "   0.37050244212150574,\n",
              "   0.33113524317741394,\n",
              "   0.06915448606014252,\n",
              "   0.12355325371026993,\n",
              "   -0.01645885594189167,\n",
              "   -0.7584173083305359,\n",
              "   0.8521797060966492,\n",
              "   -0.23866581916809082,\n",
              "   -0.05341390147805214,\n",
              "   0.06851738691329956,\n",
              "   0.257705420255661,\n",
              "   -0.7373694777488708,\n",
              "   -0.43146997690200806,\n",
              "   -0.43334829807281494,\n",
              "   -0.06879016011953354,\n",
              "   0.27687788009643555,\n",
              "   -0.19682586193084717,\n",
              "   -0.23521316051483154,\n",
              "   -0.3301166594028473,\n",
              "   0.45611584186553955,\n",
              "   -0.20388855040073395,\n",
              "   -0.5061288475990295,\n",
              "   0.04002675414085388,\n",
              "   0.21375861763954163,\n",
              "   0.3135800361633301,\n",
              "   -0.1863429695367813,\n",
              "   -0.049788158386945724,\n",
              "   0.12759099900722504,\n",
              "   -0.17785784602165222,\n",
              "   0.425746351480484,\n",
              "   -0.06504775583744049,\n",
              "   -0.38092041015625,\n",
              "   -0.048675525933504105,\n",
              "   0.010608279146254063,\n",
              "   -0.3520110845565796,\n",
              "   -0.3494148850440979,\n",
              "   0.3188892900943756,\n",
              "   0.3235672414302826,\n",
              "   -0.12336881458759308,\n",
              "   -0.2254216969013214,\n",
              "   0.6800335645675659,\n",
              "   -0.13199764490127563,\n",
              "   0.006548796780407429,\n",
              "   0.3094644546508789,\n",
              "   0.1757192760705948,\n",
              "   0.6871987581253052,\n",
              "   -0.2827439308166504,\n",
              "   -0.001140596461482346,\n",
              "   -0.08693858981132507,\n",
              "   0.12849028408527374,\n",
              "   0.3839537799358368,\n",
              "   -0.06739384680986404,\n",
              "   0.11763115972280502,\n",
              "   -0.23676928877830505,\n",
              "   0.016441743820905685,\n",
              "   -0.34190014004707336,\n",
              "   -0.5088291168212891,\n",
              "   0.8216091394424438,\n",
              "   -0.2613168954849243,\n",
              "   0.023399079218506813,\n",
              "   -0.18171626329421997,\n",
              "   -0.3703576922416687,\n",
              "   -0.243678480386734,\n",
              "   -0.666722297668457,\n",
              "   0.34461212158203125,\n",
              "   0.14198961853981018,\n",
              "   -0.1599850356578827,\n",
              "   0.13256652653217316,\n",
              "   0.3131817877292633,\n",
              "   -0.18832555413246155,\n",
              "   0.2680775225162506,\n",
              "   -0.653903603553772,\n",
              "   -0.5699599385261536,\n",
              "   -0.3451809585094452,\n",
              "   -0.16998250782489777,\n",
              "   0.3323213756084442,\n",
              "   0.08067420870065689,\n",
              "   -0.03762486204504967,\n",
              "   0.13921956717967987,\n",
              "   -0.42407703399658203,\n",
              "   0.08202109485864639,\n",
              "   -0.6762622594833374,\n",
              "   0.01716584898531437,\n",
              "   -0.3324401378631592,\n",
              "   0.039946019649505615,\n",
              "   -0.4825296998023987,\n",
              "   -0.2916964292526245,\n",
              "   0.3044770359992981,\n",
              "   -0.057134222239255905,\n",
              "   -0.3549964427947998,\n",
              "   -0.27324405312538147,\n",
              "   0.15013937652111053,\n",
              "   -1.2042121887207031,\n",
              "   0.3366312086582184,\n",
              "   -0.137776181101799,\n",
              "   -0.4738958477973938,\n",
              "   0.02389778569340706,\n",
              "   -0.13221682608127594,\n",
              "   0.15704752504825592,\n",
              "   -0.02259833551943302,\n",
              "   0.07783238589763641,\n",
              "   0.003888181410729885,\n",
              "   -0.4412962794303894,\n",
              "   0.06490443646907806,\n",
              "   -0.9982973337173462,\n",
              "   -0.32122108340263367,\n",
              "   -0.43442341685295105,\n",
              "   -0.3469017744064331,\n",
              "   0.8297361731529236,\n",
              "   -0.6206562519073486,\n",
              "   0.22319144010543823,\n",
              "   0.3417498767375946,\n",
              "   -0.307794451713562,\n",
              "   -0.1522139310836792,\n",
              "   -0.4883560538291931,\n",
              "   0.013284653425216675,\n",
              "   -0.20269711315631866,\n",
              "   -0.6792333126068115,\n",
              "   0.10940216481685638,\n",
              "   0.032768964767456055,\n",
              "   -0.8078987002372742,\n",
              "   0.26788172125816345,\n",
              "   -0.12501122057437897,\n",
              "   0.15195097029209137,\n",
              "   0.3083590865135193,\n",
              "   0.206448495388031,\n",
              "   -0.0724414810538292,\n",
              "   0.05245589837431908,\n",
              "   0.06319203227758408,\n",
              "   0.685346245765686,\n",
              "   -0.4086237847805023,\n",
              "   -0.0414896085858345,\n",
              "   0.0004624676366802305,\n",
              "   0.5262553095817566,\n",
              "   0.043226342648267746,\n",
              "   -0.29827073216438293,\n",
              "   0.07289601862430573,\n",
              "   0.23215828835964203,\n",
              "   -0.20137536525726318,\n",
              "   0.2519815266132355,\n",
              "   -0.27766650915145874,\n",
              "   -0.447738915681839,\n",
              "   0.000893806223757565,\n",
              "   0.17381909489631653,\n",
              "   0.032356228679418564,\n",
              "   -0.31729623675346375,\n",
              "   -0.09255499392747879,\n",
              "   0.10904216021299362,\n",
              "   0.8460723757743835,\n",
              "   0.1142544075846672,\n",
              "   -0.4089013934135437,\n",
              "   -0.3172799348831177,\n",
              "   0.8171082735061646,\n",
              "   0.6751693487167358,\n",
              "   0.14045916497707367,\n",
              "   0.37662824988365173,\n",
              "   0.10667625069618225,\n",
              "   1.031031847000122,\n",
              "   0.08253541588783264,\n",
              "   0.5713661313056946,\n",
              "   -0.02784610353410244,\n",
              "   -0.8677537441253662,\n",
              "   -0.0559871606528759,\n",
              "   0.13619954884052277,\n",
              "   0.20724551379680634,\n",
              "   0.44119027256965637,\n",
              "   0.5845298171043396,\n",
              "   0.545648992061615,\n",
              "   -0.2762911319732666,\n",
              "   0.3117654025554657,\n",
              "   0.38684946298599243,\n",
              "   0.43486180901527405,\n",
              "   -0.165695458650589,\n",
              "   0.7046720385551453,\n",
              "   0.2505834698677063,\n",
              "   -0.7687788605690002,\n",
              "   -0.42789697647094727,\n",
              "   0.0978606715798378,\n",
              "   -0.6591994762420654,\n",
              "   0.02406727895140648,\n",
              "   0.11699771136045456,\n",
              "   0.24942578375339508,\n",
              "   -0.3761017918586731,\n",
              "   0.5603893995285034,\n",
              "   -0.007349110208451748,\n",
              "   0.43261709809303284,\n",
              "   0.6189433336257935,\n",
              "   -0.32912641763687134,\n",
              "   -0.12343902885913849,\n",
              "   -0.07815225422382355,\n",
              "   0.25468215346336365,\n",
              "   -0.26703307032585144,\n",
              "   0.26211708784103394,\n",
              "   -0.45307743549346924,\n",
              "   0.21490153670310974,\n",
              "   -0.3566559851169586,\n",
              "   -0.04154321923851967,\n",
              "   0.2884823977947235,\n",
              "   0.2322012484073639,\n",
              "   0.0741882175207138,\n",
              "   -0.3538883328437805,\n",
              "   -0.3687455356121063,\n",
              "   0.054073601961135864,\n",
              "   0.5483565926551819,\n",
              "   1.0942946672439575,\n",
              "   0.4326918423175812,\n",
              "   0.2655867636203766,\n",
              "   0.27211904525756836,\n",
              "   -0.18303436040878296,\n",
              "   0.23790432512760162,\n",
              "   0.5128013491630554,\n",
              "   0.481754869222641,\n",
              "   0.6619018316268921,\n",
              "   -0.059005964547395706,\n",
              "   0.10104843974113464,\n",
              "   0.7453975081443787,\n",
              "   0.42825812101364136,\n",
              "   0.5799128413200378,\n",
              "   0.4905083477497101,\n",
              "   -0.2537628710269928,\n",
              "   0.4637155830860138,\n",
              "   0.43652400374412537,\n",
              "   0.4617311358451843,\n",
              "   -0.20876578986644745,\n",
              "   -0.04917939379811287,\n",
              "   -0.4318627417087555,\n",
              "   0.43269479274749756,\n",
              "   0.19255925714969635,\n",
              "   -0.5356354117393494,\n",
              "   0.07060454040765762,\n",
              "   -0.053738269954919815,\n",
              "   0.23732897639274597,\n",
              "   -0.08601853996515274,\n",
              "   0.13260024785995483,\n",
              "   -0.4730086922645569,\n",
              "   -0.07302097976207733,\n",
              "   -0.020794250071048737,\n",
              "   -0.1164608970284462,\n",
              "   -0.6114741563796997,\n",
              "   -0.5014919638633728,\n",
              "   0.24609331786632538,\n",
              "   0.32740944623947144,\n",
              "   0.23994788527488708,\n",
              "   -0.49016064405441284,\n",
              "   -0.2785053849220276,\n",
              "   -0.12667152285575867,\n",
              "   -0.6419781446456909,\n",
              "   0.35073357820510864,\n",
              "   0.5376038551330566,\n",
              "   0.5396363735198975,\n",
              "   -0.0903753861784935,\n",
              "   0.26154571771621704,\n",
              "   -0.32959792017936707,\n",
              "   -0.22012002766132355,\n",
              "   -0.8408416509628296,\n",
              "   0.1884097456932068,\n",
              "   -0.4913860857486725,\n",
              "   0.0677686259150505,\n",
              "   0.11366578936576843,\n",
              "   0.5488317608833313,\n",
              "   -0.1085762232542038,\n",
              "   0.20779849588871002,\n",
              "   -0.013254360295832157,\n",
              "   -0.28287386894226074,\n",
              "   0.3015662431716919,\n",
              "   -0.640900194644928,\n",
              "   0.4627644717693329,\n",
              "   -0.6305187344551086,\n",
              "   -0.42233529686927795,\n",
              "   -0.2336810827255249,\n",
              "   -0.46547770500183105,\n",
              "   0.43517929315567017,\n",
              "   0.7193994522094727,\n",
              "   -0.4571141004562378,\n",
              "   -0.08518494665622711,\n",
              "   -0.5770202279090881,\n",
              "   -0.42419058084487915,\n",
              "   0.5654143691062927,\n",
              "   -0.08948753774166107,\n",
              "   -0.2692820429801941,\n",
              "   0.7086215019226074,\n",
              "   0.15785251557826996,\n",
              "   -0.5864728093147278,\n",
              "   -0.2513265609741211,\n",
              "   0.3558506667613983,\n",
              "   0.33387628197669983,\n",
              "   -0.718783438205719,\n",
              "   0.037801843136548996,\n",
              "   0.33937880396842957,\n",
              "   -0.12367585301399231,\n",
              "   -0.4121396839618683,\n",
              "   -0.05676915496587753,\n",
              "   0.012971203774213791,\n",
              "   0.20123738050460815,\n",
              "   -0.24504338204860687,\n",
              "   -0.45535796880722046,\n",
              "   -0.39374446868896484,\n",
              "   -0.3019559979438782,\n",
              "   -0.07434416562318802,\n",
              "   -0.40142059326171875,\n",
              "   -0.7350885272026062,\n",
              "   0.06346290558576584,\n",
              "   0.5233606100082397,\n",
              "   0.05179455503821373,\n",
              "   0.1069444864988327,\n",
              "   0.723095178604126,\n",
              "   -0.23364757001399994,\n",
              "   -0.46102213859558105,\n",
              "   -0.6731036901473999,\n",
              "   0.4719148278236389,\n",
              "   -0.1012474074959755,\n",
              "   -0.18559470772743225,\n",
              "   0.35638612508773804,\n",
              "   0.13323372602462769,\n",
              "   -0.6187854409217834,\n",
              "   -0.04267597198486328,\n",
              "   0.15937665104866028,\n",
              "   -0.318549245595932,\n",
              "   -0.03752008080482483,\n",
              "   0.4483323097229004,\n",
              "   -0.12102682143449783],\n",
              "  [0.10822286456823349,\n",
              "   0.36769500374794006,\n",
              "   0.3816297948360443,\n",
              "   0.3400751054286957,\n",
              "   0.5704489946365356,\n",
              "   -0.4571007788181305,\n",
              "   0.034171272069215775,\n",
              "   0.5260571241378784,\n",
              "   -0.7456549406051636,\n",
              "   -0.01796342059969902,\n",
              "   0.42720842361450195,\n",
              "   -0.4056304097175598,\n",
              "   -0.10309998691082001,\n",
              "   0.634259045124054,\n",
              "   0.07721194624900818,\n",
              "   0.5243242979049683,\n",
              "   0.2838427722454071,\n",
              "   0.04031146690249443,\n",
              "   -0.05752607062458992,\n",
              "   0.293854296207428,\n",
              "   -0.12390702217817307,\n",
              "   0.1855906993150711,\n",
              "   -0.14192542433738708,\n",
              "   0.36783289909362793,\n",
              "   0.24335353076457977,\n",
              "   -0.45933422446250916,\n",
              "   -0.19825339317321777,\n",
              "   -0.07123149931430817,\n",
              "   -0.23402011394500732,\n",
              "   -0.15225982666015625,\n",
              "   -0.07864599674940109,\n",
              "   0.13463295996189117,\n",
              "   0.04327065497636795,\n",
              "   0.22614240646362305,\n",
              "   -0.4439510703086853,\n",
              "   0.05533093214035034,\n",
              "   -0.14400556683540344,\n",
              "   0.2103707492351532,\n",
              "   -0.36339977383613586,\n",
              "   0.06441407650709152,\n",
              "   -0.0993705689907074,\n",
              "   -0.004648149479180574,\n",
              "   0.11376004666090012,\n",
              "   0.2436290979385376,\n",
              "   -0.28362876176834106,\n",
              "   -0.12921860814094543,\n",
              "   0.09834448248147964,\n",
              "   0.030726222321391106,\n",
              "   0.24796098470687866,\n",
              "   -0.5636889934539795,\n",
              "   -0.3482988178730011,\n",
              "   0.46660640835762024,\n",
              "   0.21878138184547424,\n",
              "   -0.016652079299092293,\n",
              "   -0.2693571150302887,\n",
              "   0.6799299716949463,\n",
              "   0.17207324504852295,\n",
              "   -0.6308461427688599,\n",
              "   -0.11675594002008438,\n",
              "   -0.20827804505825043,\n",
              "   0.4080210328102112,\n",
              "   -0.03630419448018074,\n",
              "   0.024676691740751266,\n",
              "   -0.33025234937667847,\n",
              "   0.10576120018959045,\n",
              "   0.0905090793967247,\n",
              "   -0.1953425109386444,\n",
              "   0.02985536865890026,\n",
              "   0.5229144096374512,\n",
              "   -0.1184217557311058,\n",
              "   0.1729913055896759,\n",
              "   0.1834104210138321,\n",
              "   -0.3856215178966522,\n",
              "   0.05491863191127777,\n",
              "   -0.2873423397541046,\n",
              "   0.09522862732410431,\n",
              "   0.27208149433135986,\n",
              "   0.31878095865249634,\n",
              "   -0.06423923373222351,\n",
              "   0.28571510314941406,\n",
              "   0.06043847277760506,\n",
              "   0.5607046484947205,\n",
              "   -0.5499853491783142,\n",
              "   0.29155611991882324,\n",
              "   0.08296235650777817,\n",
              "   0.24283717572689056,\n",
              "   -0.38363033533096313,\n",
              "   0.01534274686127901,\n",
              "   0.14195245504379272,\n",
              "   0.08860805630683899,\n",
              "   0.2162308394908905,\n",
              "   0.0008185530896298587,\n",
              "   0.11519570648670197,\n",
              "   0.28369802236557007,\n",
              "   -0.03425023704767227,\n",
              "   -0.136008158326149,\n",
              "   -0.45497792959213257,\n",
              "   0.21700641512870789,\n",
              "   -0.1601269245147705,\n",
              "   0.27836236357688904,\n",
              "   -0.024189157411456108,\n",
              "   -0.4138883650302887,\n",
              "   0.06192825734615326,\n",
              "   0.2899216115474701,\n",
              "   0.26459962129592896,\n",
              "   0.1652902215719223,\n",
              "   0.03853625804185867,\n",
              "   0.02338007278740406,\n",
              "   -0.2140747308731079,\n",
              "   0.007341176737099886,\n",
              "   0.004575025290250778,\n",
              "   -0.15461193025112152,\n",
              "   0.11036547273397446,\n",
              "   -0.20156395435333252,\n",
              "   0.041076693683862686,\n",
              "   -0.009712274186313152,\n",
              "   0.4735860824584961,\n",
              "   -0.4586767852306366,\n",
              "   0.15246336162090302,\n",
              "   0.012728122062981129,\n",
              "   -0.0963728204369545,\n",
              "   -0.07474929839372635,\n",
              "   -0.1776501089334488,\n",
              "   0.5206039547920227,\n",
              "   -0.0665619969367981,\n",
              "   -0.46326130628585815,\n",
              "   0.028031466528773308,\n",
              "   -0.13801389932632446,\n",
              "   0.30173903703689575,\n",
              "   -0.10053914040327072,\n",
              "   0.5050788521766663,\n",
              "   0.22435668110847473,\n",
              "   0.5186116695404053,\n",
              "   -0.3097282648086548,\n",
              "   0.018017170950770378,\n",
              "   -0.1537657231092453,\n",
              "   0.09670355170965195,\n",
              "   -0.3108423054218292,\n",
              "   -0.4574424922466278,\n",
              "   0.09056401252746582,\n",
              "   0.1626197099685669,\n",
              "   -0.03434876352548599,\n",
              "   0.6132704615592957,\n",
              "   0.09096335619688034,\n",
              "   -0.03572513535618782,\n",
              "   0.023070640861988068,\n",
              "   -0.4768517017364502,\n",
              "   -0.09716276824474335,\n",
              "   -0.10400840640068054,\n",
              "   -0.18318381905555725,\n",
              "   -0.025563089177012444,\n",
              "   0.16384439170360565,\n",
              "   -0.2377798706293106,\n",
              "   -0.3063429892063141,\n",
              "   -0.20350992679595947,\n",
              "   -0.3245326578617096,\n",
              "   -0.46378055214881897,\n",
              "   -0.30681294202804565,\n",
              "   0.15687645971775055,\n",
              "   -0.10071113705635071,\n",
              "   0.5144073963165283,\n",
              "   -0.22166359424591064,\n",
              "   -0.09136040508747101,\n",
              "   -0.06125809997320175,\n",
              "   0.22916589677333832,\n",
              "   -0.1800866723060608,\n",
              "   0.1383838653564453,\n",
              "   0.6051841974258423,\n",
              "   -0.06746615469455719,\n",
              "   0.03220374137163162,\n",
              "   -0.20059965550899506,\n",
              "   0.43798333406448364,\n",
              "   0.7043048739433289,\n",
              "   -0.31125038862228394,\n",
              "   -0.6768109798431396,\n",
              "   0.018541177734732628,\n",
              "   0.27697116136550903,\n",
              "   -0.07346481829881668,\n",
              "   0.21456734836101532,\n",
              "   0.29193249344825745,\n",
              "   -0.10822155326604843,\n",
              "   -0.1227700337767601,\n",
              "   0.04966602101922035,\n",
              "   0.026075471192598343,\n",
              "   -0.03644812852144241,\n",
              "   -0.01691659726202488,\n",
              "   -0.03139752149581909,\n",
              "   0.3125365674495697,\n",
              "   -0.1572588086128235,\n",
              "   -0.16635467112064362,\n",
              "   -0.0799499899148941,\n",
              "   0.2724606394767761,\n",
              "   -0.18533986806869507,\n",
              "   -0.29594483971595764,\n",
              "   0.17980508506298065,\n",
              "   0.10880210995674133,\n",
              "   3.5492867027642205e-05,\n",
              "   -0.3492405414581299,\n",
              "   -0.06875912845134735,\n",
              "   -0.20644307136535645,\n",
              "   0.03253033757209778,\n",
              "   0.18436960875988007,\n",
              "   -0.026761038228869438,\n",
              "   0.20965200662612915,\n",
              "   -0.07805903255939484,\n",
              "   -0.08826058357954025,\n",
              "   0.0323677621781826,\n",
              "   -0.2154938280582428,\n",
              "   -0.17408014833927155,\n",
              "   0.2342972457408905,\n",
              "   -0.6592991948127747,\n",
              "   0.31307438015937805,\n",
              "   -0.05990690737962723,\n",
              "   0.27860358357429504,\n",
              "   -0.3355431854724884,\n",
              "   0.21246927976608276,\n",
              "   -0.13003872334957123,\n",
              "   -0.010731564834713936,\n",
              "   -0.5098033547401428,\n",
              "   0.14042283594608307,\n",
              "   0.03282397612929344,\n",
              "   -0.2645481824874878,\n",
              "   -0.26036348938941956,\n",
              "   0.48338496685028076,\n",
              "   -0.10146966576576233,\n",
              "   0.3358156979084015,\n",
              "   0.3641204237937927,\n",
              "   -0.22543245553970337,\n",
              "   0.1282152384519577,\n",
              "   0.027399949729442596,\n",
              "   -0.13136319816112518,\n",
              "   -0.23817430436611176,\n",
              "   0.4487130045890808,\n",
              "   -0.6035414934158325,\n",
              "   0.09367361664772034,\n",
              "   0.25748226046562195,\n",
              "   -0.3029656410217285,\n",
              "   -0.07069965451955795,\n",
              "   0.14971287548542023,\n",
              "   0.1954089254140854,\n",
              "   -0.40341413021087646,\n",
              "   -0.16619710624217987,\n",
              "   0.036441631615161896,\n",
              "   0.3229284882545471,\n",
              "   0.25012415647506714,\n",
              "   0.08713706582784653,\n",
              "   -0.22076432406902313,\n",
              "   0.1900135725736618,\n",
              "   -0.11417223513126373,\n",
              "   -0.1186511367559433,\n",
              "   -0.5672253966331482,\n",
              "   -0.27607524394989014,\n",
              "   -0.19313429296016693,\n",
              "   -0.2775719463825226,\n",
              "   0.045282989740371704,\n",
              "   -0.007949826307594776,\n",
              "   0.032784365117549896,\n",
              "   0.11659249663352966,\n",
              "   0.023929288610816002,\n",
              "   0.399125337600708,\n",
              "   -0.003788790898397565,\n",
              "   -0.2813645899295807,\n",
              "   0.10742338746786118,\n",
              "   -0.16449569165706635,\n",
              "   -0.14209561049938202,\n",
              "   -0.5805293917655945,\n",
              "   0.2109035700559616,\n",
              "   -0.1460449993610382,\n",
              "   0.38812780380249023,\n",
              "   0.016631390899419785,\n",
              "   0.249980166554451,\n",
              "   0.1668880581855774,\n",
              "   0.15976010262966156,\n",
              "   0.921704888343811,\n",
              "   -0.7140797972679138,\n",
              "   0.05931921675801277,\n",
              "   -0.03067910484969616,\n",
              "   -0.4125526547431946,\n",
              "   0.24929526448249817,\n",
              "   -0.5024688839912415,\n",
              "   0.2017124444246292,\n",
              "   0.34791362285614014,\n",
              "   -0.3447629511356354,\n",
              "   0.23144026100635529,\n",
              "   0.1547463983297348,\n",
              "   -0.4927261769771576,\n",
              "   0.4981343150138855,\n",
              "   -0.26612669229507446,\n",
              "   -0.11123058944940567,\n",
              "   -0.14432808756828308,\n",
              "   0.12024453282356262,\n",
              "   0.30743691325187683,\n",
              "   -0.283526211977005,\n",
              "   -0.33169057965278625,\n",
              "   0.4481009542942047,\n",
              "   0.10339625924825668,\n",
              "   0.2826911211013794,\n",
              "   -0.13596104085445404,\n",
              "   -0.05534408241510391,\n",
              "   -0.28455179929733276,\n",
              "   -0.39166688919067383,\n",
              "   -0.17148809134960175,\n",
              "   0.268081396818161,\n",
              "   0.3433295488357544,\n",
              "   -0.019154017791152,\n",
              "   0.31605926156044006,\n",
              "   -0.3137551546096802,\n",
              "   -0.5745530724525452,\n",
              "   -4.9022746086120605,\n",
              "   0.08248362690210342,\n",
              "   -0.4283351004123688,\n",
              "   0.1569845825433731,\n",
              "   -0.07820681482553482,\n",
              "   0.27396124601364136,\n",
              "   -0.13939614593982697,\n",
              "   -0.05882108584046364,\n",
              "   -0.5840919017791748,\n",
              "   -0.23349513113498688,\n",
              "   -0.2887350618839264,\n",
              "   0.17343482375144958,\n",
              "   0.29345443844795227,\n",
              "   0.4451409876346588,\n",
              "   0.17129336297512054,\n",
              "   -0.0379788838326931,\n",
              "   0.22915394604206085,\n",
              "   0.2499828189611435,\n",
              "   -0.12381994724273682,\n",
              "   0.19570674002170563,\n",
              "   -0.1660240739583969,\n",
              "   -0.36482325196266174,\n",
              "   0.0663246214389801,\n",
              "   0.07169058173894882,\n",
              "   0.29731011390686035,\n",
              "   0.310407429933548,\n",
              "   -0.30053821206092834,\n",
              "   0.2688881456851959,\n",
              "   0.06664203107357025,\n",
              "   0.046297863125801086,\n",
              "   0.3050583302974701,\n",
              "   -0.45899754762649536,\n",
              "   -0.24757607281208038,\n",
              "   0.11013667285442352,\n",
              "   -0.05418363958597183,\n",
              "   0.3246719241142273,\n",
              "   -0.12413287907838821,\n",
              "   0.06770936399698257,\n",
              "   0.041350461542606354,\n",
              "   -0.1883438676595688,\n",
              "   0.006434846203774214,\n",
              "   -0.6455604434013367,\n",
              "   -0.2337464988231659,\n",
              "   0.08414976298809052,\n",
              "   0.43780383467674255,\n",
              "   -0.4861953854560852,\n",
              "   0.13185079395771027,\n",
              "   -0.33901628851890564,\n",
              "   -0.00601854408159852,\n",
              "   -0.27276599407196045,\n",
              "   0.09820841252803802,\n",
              "   0.035345450043678284,\n",
              "   0.11200666427612305,\n",
              "   -0.1897616982460022,\n",
              "   -0.4876624345779419,\n",
              "   -0.36843183636665344,\n",
              "   0.636369526386261,\n",
              "   0.449008584022522,\n",
              "   -0.1637294590473175,\n",
              "   -0.36741867661476135,\n",
              "   0.035944774746894836,\n",
              "   -0.25056374073028564,\n",
              "   -0.6977382302284241,\n",
              "   -0.026854822412133217,\n",
              "   -0.21175581216812134,\n",
              "   -0.14390848577022552,\n",
              "   -0.1263323724269867,\n",
              "   -0.4329012334346771,\n",
              "   0.23887278139591217,\n",
              "   0.3898281157016754,\n",
              "   -0.07404646277427673,\n",
              "   0.2477637231349945,\n",
              "   0.004297883249819279,\n",
              "   -0.8729057908058167,\n",
              "   -0.12931188941001892,\n",
              "   0.1968085914850235,\n",
              "   0.2263384610414505,\n",
              "   -0.07481560856103897,\n",
              "   0.31687477231025696,\n",
              "   -0.2987269163131714,\n",
              "   -0.6090587377548218,\n",
              "   -0.6295995712280273,\n",
              "   -0.10357088595628738,\n",
              "   -0.08715697377920151,\n",
              "   -0.3219973146915436,\n",
              "   -0.6552784442901611,\n",
              "   -0.13226786255836487,\n",
              "   0.16882596909999847,\n",
              "   -0.549224853515625,\n",
              "   -0.21060596406459808,\n",
              "   0.14713574945926666,\n",
              "   0.16110524535179138,\n",
              "   0.054864734411239624,\n",
              "   0.07604703307151794,\n",
              "   0.13690143823623657,\n",
              "   0.4479857087135315,\n",
              "   -0.17797048389911652,\n",
              "   0.09284745156764984,\n",
              "   0.34229353070259094,\n",
              "   -0.24246007204055786,\n",
              "   0.49817129969596863,\n",
              "   -0.04480075463652611,\n",
              "   0.6448514461517334,\n",
              "   0.18091098964214325,\n",
              "   -0.4165595769882202,\n",
              "   -0.07283030450344086,\n",
              "   -0.29436707496643066,\n",
              "   -0.11754698306322098,\n",
              "   0.37621599435806274,\n",
              "   0.19994735717773438,\n",
              "   -0.3025985360145569,\n",
              "   0.3530760705471039,\n",
              "   0.4917571246623993,\n",
              "   -0.23551638424396515,\n",
              "   0.06649570167064667,\n",
              "   -0.10444650799036026,\n",
              "   -0.039016351103782654,\n",
              "   0.6015673279762268,\n",
              "   -0.024339864030480385,\n",
              "   0.09592317789793015,\n",
              "   -0.03175677731633186,\n",
              "   0.3743686079978943,\n",
              "   -0.3684034049510956,\n",
              "   -0.27402564883232117,\n",
              "   0.38718876242637634,\n",
              "   -0.36526668071746826,\n",
              "   -0.0885099545121193,\n",
              "   -0.3241744041442871,\n",
              "   0.24163708090782166,\n",
              "   0.09551018476486206,\n",
              "   0.07505722343921661,\n",
              "   -0.18954332172870636,\n",
              "   0.2626308500766754,\n",
              "   0.3301596939563751,\n",
              "   0.21755200624465942,\n",
              "   -0.10570672154426575,\n",
              "   -0.1429552435874939,\n",
              "   -0.05399138480424881,\n",
              "   0.2410176396369934,\n",
              "   0.1539371907711029,\n",
              "   0.2957460880279541,\n",
              "   0.23645609617233276,\n",
              "   0.4715304970741272,\n",
              "   0.21032491326332092,\n",
              "   -0.23450984060764313,\n",
              "   0.260765939950943,\n",
              "   0.19964425265789032,\n",
              "   0.07253982871770859,\n",
              "   -0.49846142530441284,\n",
              "   0.5257843136787415,\n",
              "   -0.4841078817844391,\n",
              "   -0.38571518659591675,\n",
              "   -0.06763700395822525,\n",
              "   -0.06706073135137558,\n",
              "   0.1367979198694229,\n",
              "   0.11691364645957947,\n",
              "   0.42982742190361023,\n",
              "   -0.5058115124702454,\n",
              "   0.2494817078113556,\n",
              "   0.0036126773338764906,\n",
              "   -0.1327015906572342,\n",
              "   -0.09381842613220215,\n",
              "   0.7849198579788208,\n",
              "   0.23443248867988586,\n",
              "   -0.21544702351093292,\n",
              "   0.46543556451797485,\n",
              "   -0.07669084519147873,\n",
              "   0.061063092201948166,\n",
              "   -0.06096721813082695,\n",
              "   -0.15361228585243225,\n",
              "   -0.3170417547225952,\n",
              "   -0.07273338735103607,\n",
              "   0.24033184349536896,\n",
              "   -0.15698957443237305,\n",
              "   -0.36619147658348083,\n",
              "   0.3284936845302582,\n",
              "   0.06749992817640305,\n",
              "   -0.042151108384132385,\n",
              "   -0.2614534795284271,\n",
              "   0.5205950140953064,\n",
              "   -0.3196127116680145,\n",
              "   -0.20043572783470154,\n",
              "   -0.36327674984931946,\n",
              "   -0.09020239114761353,\n",
              "   0.31405508518218994,\n",
              "   0.05062934011220932,\n",
              "   -0.013222244568169117,\n",
              "   -0.018834663555026054,\n",
              "   0.26736971735954285,\n",
              "   0.16435450315475464,\n",
              "   -0.32287633419036865,\n",
              "   0.30990269780158997,\n",
              "   -0.12043879181146622,\n",
              "   -0.19789165258407593,\n",
              "   -0.18577753007411957,\n",
              "   -0.1957995742559433,\n",
              "   0.7786413431167603,\n",
              "   0.12586362659931183,\n",
              "   -0.20645852386951447,\n",
              "   -0.04393335431814194,\n",
              "   -0.17150193452835083,\n",
              "   -0.31621241569519043,\n",
              "   -0.007794834673404694,\n",
              "   0.151641845703125,\n",
              "   -0.21186716854572296,\n",
              "   -0.17596369981765747,\n",
              "   -0.02635650895535946,\n",
              "   -0.04642365500330925,\n",
              "   -0.036936115473508835,\n",
              "   -0.12951204180717468,\n",
              "   0.06166548281908035,\n",
              "   -0.3296603262424469,\n",
              "   -0.24359369277954102,\n",
              "   -0.09149906039237976,\n",
              "   0.2935093641281128,\n",
              "   0.43653082847595215,\n",
              "   -0.2405506670475006,\n",
              "   0.00039875172660686076,\n",
              "   -0.19648106396198273,\n",
              "   0.24873170256614685,\n",
              "   -0.00857794564217329,\n",
              "   0.27354860305786133,\n",
              "   0.12407250702381134,\n",
              "   0.13644550740718842,\n",
              "   -0.09462275356054306,\n",
              "   -0.07354062050580978,\n",
              "   0.1880079060792923,\n",
              "   0.33038318157196045,\n",
              "   -0.23664607107639313,\n",
              "   0.4484027326107025,\n",
              "   -0.01289387047290802,\n",
              "   -0.5966556072235107,\n",
              "   0.017267698422074318,\n",
              "   -0.2456100434064865,\n",
              "   -0.0726897120475769,\n",
              "   0.22175219655036926,\n",
              "   -0.39948442578315735,\n",
              "   -0.029518891125917435,\n",
              "   0.11739706993103027,\n",
              "   -0.051432859152555466,\n",
              "   -0.39621812105178833,\n",
              "   -0.5279017090797424,\n",
              "   -0.2052726000547409,\n",
              "   -0.4077134430408478,\n",
              "   0.04850504919886589,\n",
              "   -0.1542234718799591,\n",
              "   -0.07916053384542465,\n",
              "   0.4232412576675415,\n",
              "   -0.6880723834037781,\n",
              "   -0.1703001707792282,\n",
              "   0.49837416410446167,\n",
              "   -0.21737807989120483,\n",
              "   0.07314267009496689,\n",
              "   -0.26641562581062317,\n",
              "   0.13145188987255096,\n",
              "   -0.019859708845615387,\n",
              "   -0.5973702669143677,\n",
              "   0.23475788533687592,\n",
              "   -0.20553745329380035,\n",
              "   -0.31357795000076294,\n",
              "   0.08702339977025986,\n",
              "   -0.12737895548343658,\n",
              "   -0.29105907678604126,\n",
              "   0.1809411346912384,\n",
              "   0.152517631649971,\n",
              "   -0.3394642770290375,\n",
              "   -0.0032838284969329834,\n",
              "   0.515760064125061,\n",
              "   0.19125017523765564,\n",
              "   0.06692120432853699,\n",
              "   0.11008436977863312,\n",
              "   0.17479339241981506,\n",
              "   0.3349160850048065,\n",
              "   0.1535639613866806,\n",
              "   -0.36623814702033997,\n",
              "   0.21169666945934296,\n",
              "   -0.14894506335258484,\n",
              "   -0.23648998141288757,\n",
              "   -0.046702608466148376,\n",
              "   -0.3425671458244324,\n",
              "   -0.2896735668182373,\n",
              "   0.3510359525680542,\n",
              "   0.04815292730927467,\n",
              "   0.1380823850631714,\n",
              "   -0.1802283376455307,\n",
              "   -0.012823791243135929,\n",
              "   -0.0873439759016037,\n",
              "   0.31999439001083374,\n",
              "   0.10950636118650436,\n",
              "   0.09656518697738647,\n",
              "   -0.1785956472158432,\n",
              "   0.30281171202659607,\n",
              "   -0.0658937618136406,\n",
              "   0.328728049993515,\n",
              "   0.02976357378065586,\n",
              "   0.3823121190071106,\n",
              "   0.4896705448627472,\n",
              "   -0.006696464493870735,\n",
              "   0.5508281588554382,\n",
              "   -0.01986684836447239,\n",
              "   -0.554408609867096,\n",
              "   0.22245725989341736,\n",
              "   0.3184790313243866,\n",
              "   0.027952583506703377,\n",
              "   0.2107805758714676,\n",
              "   0.4167429804801941,\n",
              "   0.2316623330116272,\n",
              "   0.1302437037229538,\n",
              "   0.02755577489733696,\n",
              "   -0.5487793684005737,\n",
              "   -0.06179196015000343,\n",
              "   -0.0849948301911354,\n",
              "   0.18165823817253113,\n",
              "   0.3299444317817688,\n",
              "   -0.47719666361808777,\n",
              "   -0.47941479086875916,\n",
              "   0.2849123179912567,\n",
              "   -0.4407859742641449,\n",
              "   -0.03949356824159622,\n",
              "   0.4485768973827362,\n",
              "   0.10741384327411652,\n",
              "   -0.06004001572728157,\n",
              "   0.14748099446296692,\n",
              "   -0.052592746913433075,\n",
              "   -0.26567208766937256,\n",
              "   0.06411072611808777,\n",
              "   0.01636512577533722,\n",
              "   0.01922762766480446,\n",
              "   0.29812565445899963,\n",
              "   -0.055492911487817764,\n",
              "   -0.09370961040258408,\n",
              "   0.1851840764284134,\n",
              "   -0.3041015565395355,\n",
              "   0.13422884047031403,\n",
              "   -0.5603769421577454,\n",
              "   -0.2926831841468811,\n",
              "   0.6108521223068237,\n",
              "   -0.12991593778133392,\n",
              "   0.057109296321868896,\n",
              "   -0.09735137224197388,\n",
              "   0.023582808673381805,\n",
              "   0.15639707446098328,\n",
              "   -0.5189739465713501,\n",
              "   0.37099137902259827,\n",
              "   -0.2635825574398041,\n",
              "   0.14733761548995972,\n",
              "   0.03694526478648186,\n",
              "   0.06600435078144073,\n",
              "   -0.09220325201749802,\n",
              "   -0.0305353794246912,\n",
              "   0.054208751767873764,\n",
              "   0.2890370190143585,\n",
              "   -0.2456214725971222,\n",
              "   0.011694622226059437,\n",
              "   -0.12265537679195404,\n",
              "   -0.1380891650915146,\n",
              "   0.39960038661956787,\n",
              "   0.4274963140487671,\n",
              "   0.05927520617842674,\n",
              "   0.36259815096855164,\n",
              "   0.25183871388435364,\n",
              "   0.40342146158218384,\n",
              "   -0.020981663838028908,\n",
              "   -0.19527436792850494,\n",
              "   0.22695370018482208,\n",
              "   0.13719497621059418,\n",
              "   0.21626855432987213,\n",
              "   -0.2537025511264801,\n",
              "   0.42121630907058716,\n",
              "   -0.09850167483091354,\n",
              "   -0.036934271454811096,\n",
              "   -0.2626534104347229,\n",
              "   -0.16191796958446503,\n",
              "   -0.522680938243866,\n",
              "   0.08216393738985062,\n",
              "   0.026925211772322655,\n",
              "   -0.26999861001968384,\n",
              "   -0.0449785478413105,\n",
              "   -0.2875882685184479,\n",
              "   -0.02241818606853485,\n",
              "   0.5280253291130066,\n",
              "   0.10628768056631088,\n",
              "   -0.5160397887229919,\n",
              "   -0.4497523903846741,\n",
              "   -0.02154362015426159,\n",
              "   -0.15461626648902893,\n",
              "   0.31386420130729675,\n",
              "   0.5342572331428528,\n",
              "   0.3229568600654602,\n",
              "   -0.15244176983833313,\n",
              "   0.07609197497367859,\n",
              "   -0.04176478832960129,\n",
              "   -0.06146181747317314,\n",
              "   -0.4112296998500824,\n",
              "   0.07880639284849167,\n",
              "   0.00721572432667017,\n",
              "   0.14492447674274445,\n",
              "   -0.1940121203660965,\n",
              "   0.696388304233551,\n",
              "   0.18923714756965637,\n",
              "   -0.31507933139801025,\n",
              "   -0.18650296330451965,\n",
              "   -0.13061529397964478,\n",
              "   0.27322688698768616,\n",
              "   -0.0209936685860157,\n",
              "   0.17670461535453796,\n",
              "   -0.22267897427082062,\n",
              "   0.06062868610024452,\n",
              "   -0.4766661822795868,\n",
              "   -0.28199848532676697,\n",
              "   -0.2412663698196411,\n",
              "   0.455309122800827,\n",
              "   -0.2301039695739746,\n",
              "   0.005359175149351358,\n",
              "   0.05707307904958725,\n",
              "   -0.407341331243515,\n",
              "   0.006304892245680094,\n",
              "   0.04140608385205269,\n",
              "   0.3625911474227905,\n",
              "   0.07070407271385193,\n",
              "   -0.11390992254018784,\n",
              "   -0.43367600440979004,\n",
              "   -0.11289753019809723,\n",
              "   0.3668202757835388,\n",
              "   0.006505704019218683,\n",
              "   0.12353526800870895,\n",
              "   0.029635172337293625,\n",
              "   0.2175641506910324,\n",
              "   -0.03619758412241936,\n",
              "   -0.0838811919093132,\n",
              "   -0.3525964617729187,\n",
              "   0.24439863860607147,\n",
              "   -0.22360734641551971,\n",
              "   -0.03555433452129364,\n",
              "   0.144150510430336,\n",
              "   -0.3436916768550873,\n",
              "   -0.6241346001625061,\n",
              "   0.23759493231773376,\n",
              "   0.012330153957009315,\n",
              "   -0.3893108367919922,\n",
              "   0.054344162344932556,\n",
              "   0.04684731364250183,\n",
              "   0.7573750019073486,\n",
              "   -0.13869068026542664,\n",
              "   0.36423569917678833,\n",
              "   0.13724413514137268,\n",
              "   0.11880891025066376,\n",
              "   0.01665463112294674,\n",
              "   -0.2656038999557495,\n",
              "   -0.2050953060388565,\n",
              "   -0.16625070571899414,\n",
              "   0.34604841470718384,\n",
              "   -0.045359861105680466,\n",
              "   -0.45402735471725464,\n",
              "   0.07509414106607437,\n",
              "   -0.3408227562904358,\n",
              "   0.34416186809539795,\n",
              "   -0.12939094007015228,\n",
              "   0.27373456954956055,\n",
              "   0.13262221217155457],\n",
              "  [0.18547149002552032,\n",
              "   -0.03021375834941864,\n",
              "   0.01718927174806595,\n",
              "   0.2109605073928833,\n",
              "   1.0941616296768188,\n",
              "   -0.16310401260852814,\n",
              "   0.18625180423259735,\n",
              "   0.4245399236679077,\n",
              "   -0.4336850643157959,\n",
              "   0.01190200075507164,\n",
              "   0.22860360145568848,\n",
              "   -0.29938551783561707,\n",
              "   -0.18280410766601562,\n",
              "   0.4411054253578186,\n",
              "   -0.4369679391384125,\n",
              "   0.3783794641494751,\n",
              "   0.18918363749980927,\n",
              "   0.04656759649515152,\n",
              "   0.2760733366012573,\n",
              "   0.41116055846214294,\n",
              "   -0.14233790338039398,\n",
              "   0.10094340145587921,\n",
              "   -0.14185982942581177,\n",
              "   0.212063267827034,\n",
              "   0.10619348287582397,\n",
              "   -0.1699664443731308,\n",
              "   -0.3303045928478241,\n",
              "   -0.045181021094322205,\n",
              "   -0.5196561217308044,\n",
              "   -0.3722134828567505,\n",
              "   0.07374938577413559,\n",
              "   0.36496806144714355,\n",
              "   -0.28690648078918457,\n",
              "   0.09835268557071686,\n",
              "   0.005727726500481367,\n",
              "   -0.026314465329051018,\n",
              "   -0.38289108872413635,\n",
              "   0.24947980046272278,\n",
              "   -0.25933679938316345,\n",
              "   0.047114402055740356,\n",
              "   -0.33084434270858765,\n",
              "   -0.446380078792572,\n",
              "   -0.18818847835063934,\n",
              "   0.25865593552589417,\n",
              "   -0.21931308507919312,\n",
              "   0.10735580325126648,\n",
              "   -0.2127622365951538,\n",
              "   0.34159305691719055,\n",
              "   0.013765440322458744,\n",
              "   0.0839368924498558,\n",
              "   -0.6415162682533264,\n",
              "   0.10235730558633804,\n",
              "   -0.46268588304519653,\n",
              "   -0.1417657881975174,\n",
              "   -0.26843827962875366,\n",
              "   0.7165857553482056,\n",
              "   0.24942579865455627,\n",
              "   -0.40824416279792786,\n",
              "   -0.051987823098897934,\n",
              "   -0.14986702799797058,\n",
              "   0.6489890217781067,\n",
              "   -0.09583554416894913,\n",
              "   -0.09068527817726135,\n",
              "   -0.10358258336782455,\n",
              "   0.3207571804523468,\n",
              "   0.20563574135303497,\n",
              "   -0.035918060690164566,\n",
              "   0.17378097772598267,\n",
              "   -0.11627937108278275,\n",
              "   0.2311842292547226,\n",
              "   -0.20588408410549164,\n",
              "   0.20668934285640717,\n",
              "   -0.4072365164756775,\n",
              "   -0.03257063031196594,\n",
              "   -0.2657327950000763,\n",
              "   0.17631958425045013,\n",
              "   0.24571529030799866,\n",
              "   0.44558316469192505,\n",
              "   0.16252204775810242,\n",
              "   0.2090500444173813,\n",
              "   -0.3237837553024292,\n",
              "   0.3568245768547058,\n",
              "   -0.5358208417892456,\n",
              "   0.41396477818489075,\n",
              "   0.14899778366088867,\n",
              "   -0.06034182757139206,\n",
              "   -0.45493200421333313,\n",
              "   -0.2957167327404022,\n",
              "   -0.3510955274105072,\n",
              "   0.6003934144973755,\n",
              "   -0.06494490057229996,\n",
              "   -0.023342132568359375,\n",
              "   0.4349992275238037,\n",
              "   0.2656652331352234,\n",
              "   0.14022178947925568,\n",
              "   -0.2899741232395172,\n",
              "   -0.1465628743171692,\n",
              "   0.1446714699268341,\n",
              "   -0.10724025964736938,\n",
              "   0.29538723826408386,\n",
              "   0.06652116030454636,\n",
              "   -0.9483307003974915,\n",
              "   -0.10358945280313492,\n",
              "   0.48816847801208496,\n",
              "   0.07271169126033783,\n",
              "   -0.0202536191791296,\n",
              "   -0.3613431751728058,\n",
              "   -0.10106991976499557,\n",
              "   -0.10498777031898499,\n",
              "   0.26450255513191223,\n",
              "   -0.39083558320999146,\n",
              "   0.08309829235076904,\n",
              "   -0.02473192662000656,\n",
              "   -0.4008965492248535,\n",
              "   -0.37827104330062866,\n",
              "   -0.05198060721158981,\n",
              "   0.19962331652641296,\n",
              "   0.1383923888206482,\n",
              "   -0.13639295101165771,\n",
              "   -0.012006309814751148,\n",
              "   0.014335811138153076,\n",
              "   0.17896124720573425,\n",
              "   0.0690934807062149,\n",
              "   0.6790555119514465,\n",
              "   0.18292707204818726,\n",
              "   0.124142587184906,\n",
              "   0.3233751654624939,\n",
              "   -0.017171910032629967,\n",
              "   0.33339521288871765,\n",
              "   -0.23884303867816925,\n",
              "   0.06040804833173752,\n",
              "   0.3373168110847473,\n",
              "   0.452746719121933,\n",
              "   -0.33362677693367004,\n",
              "   0.17323647439479828,\n",
              "   -0.11165060102939606,\n",
              "   -0.0441276840865612,\n",
              "   0.15581382811069489,\n",
              "   -0.555607259273529,\n",
              "   0.20271478593349457,\n",
              "   0.4984634518623352,\n",
              "   0.1596400886774063,\n",
              "   0.6741588115692139,\n",
              "   0.3587871789932251,\n",
              "   -0.07625733315944672,\n",
              "   0.09391038119792938,\n",
              "   -0.35353490710258484,\n",
              "   -0.292412132024765,\n",
              "   -0.3297601640224457,\n",
              "   0.16715678572654724,\n",
              "   0.02314736135303974,\n",
              "   0.26432180404663086,\n",
              "   0.07832253724336624,\n",
              "   -0.27007734775543213,\n",
              "   0.024799343198537827,\n",
              "   0.08476071059703827,\n",
              "   -0.43341559171676636,\n",
              "   0.16603690385818481,\n",
              "   0.09247049689292908,\n",
              "   -0.18488849699497223,\n",
              "   0.727654218673706,\n",
              "   0.1916486918926239,\n",
              "   0.10591141134500504,\n",
              "   0.13360470533370972,\n",
              "   0.15319855511188507,\n",
              "   -0.27546337246894836,\n",
              "   0.6463807821273804,\n",
              "   0.48087888956069946,\n",
              "   -0.05707714334130287,\n",
              "   0.28888964653015137,\n",
              "   0.03382300212979317,\n",
              "   0.4322516918182373,\n",
              "   0.5777465105056763,\n",
              "   0.12473668158054352,\n",
              "   0.33701276779174805,\n",
              "   0.015143043361604214,\n",
              "   0.22203585505485535,\n",
              "   0.13261070847511292,\n",
              "   0.16781815886497498,\n",
              "   0.219635471701622,\n",
              "   0.07142210751771927,\n",
              "   -0.16588237881660461,\n",
              "   0.41106805205345154,\n",
              "   -0.22488825023174286,\n",
              "   0.01816091686487198,\n",
              "   -0.20399144291877747,\n",
              "   0.9719829559326172,\n",
              "   -0.16879622638225555,\n",
              "   -0.05324903875589371,\n",
              "   -0.03354138880968094,\n",
              "   -0.3300025761127472,\n",
              "   -0.26457762718200684,\n",
              "   -0.11747612804174423,\n",
              "   -0.21505090594291687,\n",
              "   0.014281301759183407,\n",
              "   -0.5892943143844604,\n",
              "   0.2980574667453766,\n",
              "   -0.15986758470535278,\n",
              "   -0.30771297216415405,\n",
              "   -0.21442002058029175,\n",
              "   0.2165658324956894,\n",
              "   0.2956972122192383,\n",
              "   0.14750966429710388,\n",
              "   -0.02567218989133835,\n",
              "   -0.4720780551433563,\n",
              "   -0.06798616051673889,\n",
              "   0.17693862318992615,\n",
              "   -0.3005266487598419,\n",
              "   -0.33272603154182434,\n",
              "   -0.31133899092674255,\n",
              "   -0.2408190816640854,\n",
              "   0.3073902130126953,\n",
              "   -0.03622371703386307,\n",
              "   0.06533607095479965,\n",
              "   -0.49112555384635925,\n",
              "   0.03296186402440071,\n",
              "   -0.11239353567361832,\n",
              "   -0.4215507209300995,\n",
              "   -0.013496972620487213,\n",
              "   0.36759012937545776,\n",
              "   0.1142161563038826,\n",
              "   0.09667452424764633,\n",
              "   -0.057021498680114746,\n",
              "   0.45117026567459106,\n",
              "   0.1622294783592224,\n",
              "   0.8312504887580872,\n",
              "   0.5669649243354797,\n",
              "   -0.2497786581516266,\n",
              "   -0.039369843900203705,\n",
              "   0.34464606642723083,\n",
              "   -0.1486099809408188,\n",
              "   -0.33623310923576355,\n",
              "   0.46256402134895325,\n",
              "   -0.3205408453941345,\n",
              "   0.0843784362077713,\n",
              "   -0.09831108152866364,\n",
              "   0.0036185819189995527,\n",
              "   -0.17516037821769714,\n",
              "   0.26909583806991577,\n",
              "   0.01910882443189621,\n",
              "   -0.09980183094739914,\n",
              "   -0.22338111698627472,\n",
              "   0.6372348666191101,\n",
              "   0.18823839724063873,\n",
              "   0.17156954109668732,\n",
              "   -0.08880452811717987,\n",
              "   0.1117539256811142,\n",
              "   -0.21695375442504883,\n",
              "   -0.17666268348693848,\n",
              "   -0.27755463123321533,\n",
              "   -0.08924213796854019,\n",
              "   -0.09742697328329086,\n",
              "   -0.032298218458890915,\n",
              "   -0.31883782148361206,\n",
              "   -0.07174742221832275,\n",
              "   -0.32853126525878906,\n",
              "   -0.02926243282854557,\n",
              "   0.10979063808917999,\n",
              "   -0.1704162210226059,\n",
              "   0.42147037386894226,\n",
              "   0.6986711025238037,\n",
              "   -0.07298784703016281,\n",
              "   -0.15100784599781036,\n",
              "   -0.3492056131362915,\n",
              "   -0.19090181589126587,\n",
              "   -1.0275192260742188,\n",
              "   -0.030109666287899017,\n",
              "   -0.20096278190612793,\n",
              "   0.4407920837402344,\n",
              "   0.21507015824317932,\n",
              "   0.07724833488464355,\n",
              "   -0.38511747121810913,\n",
              "   0.4258379340171814,\n",
              "   0.5425784587860107,\n",
              "   -0.7403643727302551,\n",
              "   -0.8130045533180237,\n",
              "   0.13959220051765442,\n",
              "   -0.08971824496984482,\n",
              "   0.17074859142303467,\n",
              "   0.03623184189200401,\n",
              "   0.38908034563064575,\n",
              "   0.45379838347435,\n",
              "   -0.48315972089767456,\n",
              "   -0.0030077421106398106,\n",
              "   -0.21761971712112427,\n",
              "   -0.4953555762767792,\n",
              "   0.32102227210998535,\n",
              "   1.0939463376998901,\n",
              "   -0.2084403932094574,\n",
              "   -0.24925632774829865,\n",
              "   0.05083756893873215,\n",
              "   0.41842153668403625,\n",
              "   -0.552479088306427,\n",
              "   -0.5285788178443909,\n",
              "   0.3948495388031006,\n",
              "   0.1484956443309784,\n",
              "   0.1381988227367401,\n",
              "   -0.07856661826372147,\n",
              "   -0.4671750068664551,\n",
              "   0.3385102450847626,\n",
              "   -0.1845603883266449,\n",
              "   0.2938433289527893,\n",
              "   0.15427681803703308,\n",
              "   0.14414432644844055,\n",
              "   -0.4374236762523651,\n",
              "   0.054122403264045715,\n",
              "   -0.29406648874282837,\n",
              "   -0.4926421344280243,\n",
              "   -4.8050713539123535,\n",
              "   0.1562684178352356,\n",
              "   -0.296716570854187,\n",
              "   -0.2669164836406708,\n",
              "   -0.16678984463214874,\n",
              "   -0.18743735551834106,\n",
              "   0.1816101223230362,\n",
              "   0.06457573920488358,\n",
              "   -0.30120405554771423,\n",
              "   -0.614414393901825,\n",
              "   -0.33084022998809814,\n",
              "   -0.03728879988193512,\n",
              "   -0.12027409672737122,\n",
              "   0.3128034472465515,\n",
              "   0.4104815721511841,\n",
              "   0.03946523740887642,\n",
              "   0.36131376028060913,\n",
              "   0.05047871917486191,\n",
              "   -0.0976465567946434,\n",
              "   0.6126905679702759,\n",
              "   -0.2701006233692169,\n",
              "   -0.4474245607852936,\n",
              "   0.08659518510103226,\n",
              "   0.08826792240142822,\n",
              "   0.33280149102211,\n",
              "   -0.1168873980641365,\n",
              "   -0.2960355877876282,\n",
              "   0.19296695291996002,\n",
              "   -0.35784822702407837,\n",
              "   0.045069918036460876,\n",
              "   -0.4705442488193512,\n",
              "   -0.4649277627468109,\n",
              "   -0.262986421585083,\n",
              "   0.4121794104576111,\n",
              "   -0.3271077573299408,\n",
              "   -0.1549319475889206,\n",
              "   -0.22950299084186554,\n",
              "   -0.08181241899728775,\n",
              "   0.027940724045038223,\n",
              "   -0.017025655135512352,\n",
              "   -0.34221866726875305,\n",
              "   -0.2574794292449951,\n",
              "   -0.09086047857999802,\n",
              "   0.013703246600925922,\n",
              "   0.6796702742576599,\n",
              "   -0.059421271085739136,\n",
              "   -0.028990404680371284,\n",
              "   0.08491628617048264,\n",
              "   -0.04106681048870087,\n",
              "   -0.03878375515341759,\n",
              "   0.14018625020980835,\n",
              "   0.04657517373561859,\n",
              "   -0.16137605905532837,\n",
              "   -0.2856214642524719,\n",
              "   -0.4373488426208496,\n",
              "   -0.19672049582004547,\n",
              "   0.7078648805618286,\n",
              "   0.2937881350517273,\n",
              "   -0.3070329427719116,\n",
              "   -0.11596150696277618,\n",
              "   0.3489672839641571,\n",
              "   -0.37415996193885803,\n",
              "   -0.023477964103221893,\n",
              "   -0.32001206278800964,\n",
              "   0.10872423648834229,\n",
              "   -0.17910362780094147,\n",
              "   -0.5708670616149902,\n",
              "   -0.3177880346775055,\n",
              "   0.2473583072423935,\n",
              "   0.2079247534275055,\n",
              "   0.05545290932059288,\n",
              "   -0.07479927688837051,\n",
              "   -0.4758109450340271,\n",
              "   -1.2884291410446167,\n",
              "   -0.1659662127494812,\n",
              "   -0.09078629314899445,\n",
              "   0.43699532747268677,\n",
              "   0.16866198182106018,\n",
              "   0.13512654602527618,\n",
              "   0.10679323971271515,\n",
              "   -0.13644878566265106,\n",
              "   -0.5551813244819641,\n",
              "   0.2614067494869232,\n",
              "   0.10361034423112869,\n",
              "   -0.2657979726791382,\n",
              "   -0.5063920021057129,\n",
              "   -0.4308292865753174,\n",
              "   0.338189959526062,\n",
              "   -0.5126028656959534,\n",
              "   -0.5406522750854492,\n",
              "   -0.0010408253874629736,\n",
              "   -0.27917802333831787,\n",
              "   0.4739120304584503,\n",
              "   0.10185911506414413,\n",
              "   0.6920153498649597,\n",
              "   0.18629488348960876,\n",
              "   0.3418516218662262,\n",
              "   -0.29268747568130493,\n",
              "   0.16034235060214996,\n",
              "   -0.36140894889831543,\n",
              "   -0.033970367163419724,\n",
              "   -0.498654842376709,\n",
              "   0.43013840913772583,\n",
              "   -0.179936945438385,\n",
              "   -0.5052857995033264,\n",
              "   0.19759468734264374,\n",
              "   -0.22535240650177002,\n",
              "   0.23531466722488403,\n",
              "   0.12845373153686523,\n",
              "   -0.18992170691490173,\n",
              "   0.32123997807502747,\n",
              "   -0.0944775715470314,\n",
              "   0.42828458547592163,\n",
              "   -0.48023954033851624,\n",
              "   -0.10867314040660858,\n",
              "   -0.08794794231653214,\n",
              "   0.06256299465894699,\n",
              "   0.5808248519897461,\n",
              "   -0.26090964674949646,\n",
              "   0.03430449962615967,\n",
              "   -0.06471198052167892,\n",
              "   0.4359659254550934,\n",
              "   -0.07080279290676117,\n",
              "   -0.14500552415847778,\n",
              "   -0.20450006425380707,\n",
              "   -0.09449094533920288,\n",
              "   -0.09746285527944565,\n",
              "   0.1089964434504509,\n",
              "   0.07580191642045975,\n",
              "   -0.23074841499328613,\n",
              "   0.08053997904062271,\n",
              "   -0.07231523096561432,\n",
              "   0.26432231068611145,\n",
              "   0.09419301152229309,\n",
              "   0.4262608587741852,\n",
              "   -0.028295185416936874,\n",
              "   0.32913097739219666,\n",
              "   -0.16988907754421234,\n",
              "   0.04597361385822296,\n",
              "   -0.18330489099025726,\n",
              "   0.0004236808163113892,\n",
              "   0.3058646023273468,\n",
              "   0.22492411732673645,\n",
              "   0.32418757677078247,\n",
              "   -0.4985305666923523,\n",
              "   0.3463149964809418,\n",
              "   -0.3364507257938385,\n",
              "   0.34314706921577454,\n",
              "   -0.1589612513780594,\n",
              "   0.22308498620986938,\n",
              "   -0.10527671873569489,\n",
              "   -0.4761413037776947,\n",
              "   0.07903821766376495,\n",
              "   0.21998976171016693,\n",
              "   0.3181178867816925,\n",
              "   0.13901647925376892,\n",
              "   0.43776243925094604,\n",
              "   -0.18638966977596283,\n",
              "   0.07113058865070343,\n",
              "   -0.7481594085693359,\n",
              "   -0.2932983338832855,\n",
              "   -0.0884726494550705,\n",
              "   0.28310906887054443,\n",
              "   0.24056178331375122,\n",
              "   -0.4117237627506256,\n",
              "   0.619560956954956,\n",
              "   0.05744977295398712,\n",
              "   -0.055103376507759094,\n",
              "   0.2005307823419571,\n",
              "   -0.015834273770451546,\n",
              "   -0.12366031855344772,\n",
              "   0.008163143880665302,\n",
              "   0.11773768812417984,\n",
              "   -0.11434922367334366,\n",
              "   -0.4259810447692871,\n",
              "   0.3908323347568512,\n",
              "   0.05301370099186897,\n",
              "   -0.1901465803384781,\n",
              "   -0.3934590518474579,\n",
              "   0.7932888865470886,\n",
              "   -0.08406831324100494,\n",
              "   0.13874466717243195,\n",
              "   -0.10215277224779129,\n",
              "   -0.027579672634601593,\n",
              "   0.007701368536800146,\n",
              "   0.5442817211151123,\n",
              "   0.21347860991954803,\n",
              "   -0.15043583512306213,\n",
              "   0.06315447390079498,\n",
              "   0.35987135767936707,\n",
              "   -0.42096588015556335,\n",
              "   0.5354316234588623,\n",
              "   -0.31112074851989746,\n",
              "   -0.17428478598594666,\n",
              "   0.004626537207514048,\n",
              "   -0.2297646403312683,\n",
              "   0.6290695667266846,\n",
              "   0.20010516047477722,\n",
              "   0.17052341997623444,\n",
              "   -0.3569227457046509,\n",
              "   -0.05176147073507309,\n",
              "   -0.5006741285324097,\n",
              "   -0.2530079185962677,\n",
              "   0.16677549481391907,\n",
              "   -0.028579076752066612,\n",
              "   -0.24608170986175537,\n",
              "   0.3378898501396179,\n",
              "   0.33145833015441895,\n",
              "   -0.127513587474823,\n",
              "   -0.10142995417118073,\n",
              "   -0.2610984742641449,\n",
              "   -0.4810299575328827,\n",
              "   -0.5316787362098694,\n",
              "   -0.2895643711090088,\n",
              "   0.2115170657634735,\n",
              "   0.13278059661388397,\n",
              "   -0.10004466772079468,\n",
              "   -0.4168368875980377,\n",
              "   -0.34856492280960083,\n",
              "   0.29955869913101196,\n",
              "   -0.08854866772890091,\n",
              "   0.3748038709163666,\n",
              "   0.13843928277492523,\n",
              "   -0.0074905529618263245,\n",
              "   -0.1772392988204956,\n",
              "   -0.31355980038642883,\n",
              "   0.27708807587623596,\n",
              "   0.1756262183189392,\n",
              "   -0.536457359790802,\n",
              "   0.06519650667905807,\n",
              "   0.023270675912499428,\n",
              "   -0.4195534884929657,\n",
              "   -0.09632281213998795,\n",
              "   -0.15570589900016785,\n",
              "   -0.032491642981767654,\n",
              "   0.17188134789466858,\n",
              "   -0.13609005510807037,\n",
              "   -0.08326470851898193,\n",
              "   -0.032203834503889084,\n",
              "   0.07511593401432037,\n",
              "   -0.29112708568573,\n",
              "   -0.24471743404865265,\n",
              "   -0.1479720026254654,\n",
              "   -0.22550980746746063,\n",
              "   0.4264831840991974,\n",
              "   -0.07731381058692932,\n",
              "   -0.14000223577022552,\n",
              "   0.7847713828086853,\n",
              "   -0.2699252963066101,\n",
              "   -0.23924225568771362,\n",
              "   -0.19611994922161102,\n",
              "   -0.09858045727014542,\n",
              "   -0.28075873851776123,\n",
              "   0.06480076164007187,\n",
              "   0.07502664625644684,\n",
              "   -0.5937476754188538,\n",
              "   0.059198129922151566,\n",
              "   0.05264275521039963,\n",
              "   -0.2172223925590515,\n",
              "   0.07547132670879364,\n",
              "   0.39399969577789307,\n",
              "   -0.3788853883743286,\n",
              "   -0.040619637817144394,\n",
              "   0.41872578859329224,\n",
              "   0.2642628252506256,\n",
              "   0.012268640100955963,\n",
              "   -0.27201414108276367,\n",
              "   0.2501142621040344,\n",
              "   0.3598416745662689,\n",
              "   -0.18174679577350616,\n",
              "   0.3998880684375763,\n",
              "   -0.30241531133651733,\n",
              "   0.11734279245138168,\n",
              "   0.40928152203559875,\n",
              "   -0.531089723110199,\n",
              "   -0.11611120402812958,\n",
              "   0.11132248491048813,\n",
              "   -0.1212596669793129,\n",
              "   0.3337532579898834,\n",
              "   -0.24808335304260254,\n",
              "   -0.4342553913593292,\n",
              "   0.3848366439342499,\n",
              "   0.058985453099012375,\n",
              "   -0.22783415019512177,\n",
              "   -0.4159051775932312,\n",
              "   0.22211003303527832,\n",
              "   -0.11247292906045914,\n",
              "   0.2708278000354767,\n",
              "   -0.02844841219484806,\n",
              "   -0.07203943282365799,\n",
              "   0.22572892904281616,\n",
              "   0.23643144965171814,\n",
              "   -0.1865745335817337,\n",
              "   -0.24611425399780273,\n",
              "   0.29751864075660706,\n",
              "   0.3429080545902252,\n",
              "   0.6108715534210205,\n",
              "   0.04013228788971901,\n",
              "   0.6172116994857788,\n",
              "   0.1489270180463791,\n",
              "   0.05201442539691925,\n",
              "   0.24411414563655853,\n",
              "   0.27355054020881653,\n",
              "   -0.1487240344285965,\n",
              "   0.10561352223157883,\n",
              "   0.1379469633102417,\n",
              "   0.13618090748786926,\n",
              "   0.04676056653261185,\n",
              "   -0.1115991547703743,\n",
              "   0.15887923538684845,\n",
              "   -0.2436603307723999,\n",
              "   -0.07002255320549011,\n",
              "   0.46469730138778687,\n",
              "   0.27700069546699524,\n",
              "   -0.7438845038414001,\n",
              "   -0.047906544059515,\n",
              "   0.3688737452030182,\n",
              "   -0.32863208651542664,\n",
              "   0.01796889305114746,\n",
              "   0.20033930242061615,\n",
              "   -0.22432555258274078,\n",
              "   0.03605683520436287,\n",
              "   0.12432508915662766,\n",
              "   -0.1351078748703003,\n",
              "   -0.223047137260437,\n",
              "   0.14674143493175507,\n",
              "   0.23904012143611908,\n",
              "   0.0046152276918292046,\n",
              "   -0.2484968900680542,\n",
              "   0.16278718411922455,\n",
              "   -0.10943174362182617,\n",
              "   0.5124334692955017,\n",
              "   -0.09719836711883545,\n",
              "   0.18837517499923706,\n",
              "   -0.3992778956890106,\n",
              "   -0.22068680822849274,\n",
              "   -0.07246267795562744,\n",
              "   0.32530897855758667,\n",
              "   0.30918318033218384,\n",
              "   0.28846311569213867,\n",
              "   0.22651781141757965,\n",
              "   0.11025857925415039,\n",
              "   0.012954394333064556,\n",
              "   0.7847238779067993,\n",
              "   0.1421404629945755,\n",
              "   0.1958617866039276,\n",
              "   0.03427353873848915,\n",
              "   0.0032544513233006,\n",
              "   0.10842146724462509,\n",
              "   -0.06080424040555954,\n",
              "   0.08664482831954956,\n",
              "   0.3494357168674469,\n",
              "   0.18827401101589203,\n",
              "   -0.048821911215782166,\n",
              "   -0.11239731311798096,\n",
              "   -0.22449536621570587,\n",
              "   0.3455262780189514,\n",
              "   0.3113093078136444,\n",
              "   0.250644326210022,\n",
              "   0.32395973801612854,\n",
              "   0.0919150859117508,\n",
              "   0.07029002159833908,\n",
              "   0.19682954251766205,\n",
              "   -0.1450086086988449,\n",
              "   -0.10139309614896774,\n",
              "   0.6645925045013428,\n",
              "   0.5479555726051331,\n",
              "   -0.3350616693496704,\n",
              "   0.04914255067706108,\n",
              "   -0.10748083144426346,\n",
              "   -0.370574951171875,\n",
              "   -0.022220168262720108,\n",
              "   -0.5590570569038391,\n",
              "   -0.4951832890510559,\n",
              "   0.14051498472690582,\n",
              "   0.03245839849114418,\n",
              "   -0.3970302641391754,\n",
              "   -0.18022845685482025,\n",
              "   -0.08900414407253265,\n",
              "   0.19614873826503754,\n",
              "   0.2566956579685211,\n",
              "   0.44209903478622437,\n",
              "   -0.16697178781032562,\n",
              "   0.048855457454919815,\n",
              "   0.18539874255657196,\n",
              "   -0.5015202164649963,\n",
              "   0.09679651260375977,\n",
              "   0.26567113399505615,\n",
              "   0.5416440367698669,\n",
              "   -0.4509832262992859,\n",
              "   0.15976040065288544,\n",
              "   -0.024427752941846848,\n",
              "   -0.24196617305278778,\n",
              "   -0.17520549893379211,\n",
              "   -0.2387579381465912,\n",
              "   -0.01974637806415558,\n",
              "   0.2624092400074005,\n",
              "   -0.03418330103158951,\n",
              "   0.5429505109786987,\n",
              "   0.04438333213329315,\n",
              "   -0.21477246284484863,\n",
              "   -0.3751578629016876,\n",
              "   -0.09240718185901642,\n",
              "   -0.2242361307144165,\n",
              "   -0.4970753490924835,\n",
              "   0.04070253670215607,\n",
              "   -0.2901616394519806,\n",
              "   0.040086980909109116,\n",
              "   -0.19139476120471954,\n",
              "   -0.2731938660144806,\n",
              "   -0.03696684166789055,\n",
              "   0.24253833293914795,\n",
              "   -0.28769025206565857,\n",
              "   0.2578470706939697,\n",
              "   -0.10416695475578308,\n",
              "   -0.1687629073858261,\n",
              "   0.26232898235321045,\n",
              "   -0.2779541313648224,\n",
              "   -0.09026778489351273,\n",
              "   0.4144216775894165,\n",
              "   -0.40956154465675354,\n",
              "   -0.496246874332428,\n",
              "   0.1895695924758911,\n",
              "   0.14348387718200684,\n",
              "   -0.0758952647447586,\n",
              "   -0.34873896837234497,\n",
              "   -0.14962369203567505,\n",
              "   0.0985691025853157,\n",
              "   -0.0026200488209724426,\n",
              "   -0.22208912670612335,\n",
              "   0.046555206179618835,\n",
              "   0.24429163336753845,\n",
              "   -0.19759982824325562,\n",
              "   -0.5045653581619263,\n",
              "   0.2256609946489334,\n",
              "   -0.12097006291151047,\n",
              "   -0.3737717568874359,\n",
              "   0.23047392070293427,\n",
              "   -0.04328759014606476,\n",
              "   -0.15392416715621948,\n",
              "   0.056880369782447815,\n",
              "   0.25038886070251465,\n",
              "   -0.29396459460258484,\n",
              "   -0.20093503594398499,\n",
              "   0.22953678667545319,\n",
              "   -0.1826322078704834,\n",
              "   0.004084852058440447,\n",
              "   -0.4833644926548004,\n",
              "   -0.46137991547584534,\n",
              "   -0.16380757093429565,\n",
              "   -0.2596700191497803,\n",
              "   0.4528706669807434,\n",
              "   -0.05841774865984917,\n",
              "   -0.21182113885879517,\n",
              "   -0.13981235027313232,\n",
              "   0.01464552991092205,\n",
              "   0.17107661068439484,\n",
              "   -0.2810065746307373,\n",
              "   0.16000686585903168,\n",
              "   0.001858095987699926],\n",
              "  [-0.31077632308006287,\n",
              "   -0.11755551397800446,\n",
              "   0.7746285200119019,\n",
              "   0.46346524357795715,\n",
              "   0.24455486238002777,\n",
              "   -0.6843851804733276,\n",
              "   -0.13353322446346283,\n",
              "   1.6385968923568726,\n",
              "   -0.8579133152961731,\n",
              "   0.09603902697563171,\n",
              "   0.23095354437828064,\n",
              "   -0.6414260864257812,\n",
              "   -0.5440449118614197,\n",
              "   0.5782445073127747,\n",
              "   0.08573389798402786,\n",
              "   -0.017449211329221725,\n",
              "   0.5882045030593872,\n",
              "   0.10966002941131592,\n",
              "   -0.011724676005542278,\n",
              "   -0.136139914393425,\n",
              "   0.4504198431968689,\n",
              "   -0.14587560296058655,\n",
              "   -0.20729964971542358,\n",
              "   0.5126195549964905,\n",
              "   0.3376108705997467,\n",
              "   -0.4722737669944763,\n",
              "   -0.34850072860717773,\n",
              "   0.10237831622362137,\n",
              "   -0.23033012449741364,\n",
              "   -0.18393109738826752,\n",
              "   -0.0888744369149208,\n",
              "   0.08234464377164841,\n",
              "   -1.2216342687606812,\n",
              "   -0.7771689891815186,\n",
              "   -0.5161805748939514,\n",
              "   -0.4821060001850128,\n",
              "   0.060536690056324005,\n",
              "   -0.11879365891218185,\n",
              "   -0.30827492475509644,\n",
              "   0.3037353456020355,\n",
              "   -0.4982245862483978,\n",
              "   -0.48575976490974426,\n",
              "   -0.24356883764266968,\n",
              "   -0.4059850871562958,\n",
              "   -0.038321565836668015,\n",
              "   -0.2937255799770355,\n",
              "   0.9573398232460022,\n",
              "   0.06456265598535538,\n",
              "   0.7592743039131165,\n",
              "   -0.47759881615638733,\n",
              "   -0.6886274218559265,\n",
              "   -0.12284105271100998,\n",
              "   0.5536788105964661,\n",
              "   0.022538375109434128,\n",
              "   -0.037493880838155746,\n",
              "   1.043062686920166,\n",
              "   -0.08525034040212631,\n",
              "   -0.5471941232681274,\n",
              "   -0.5215846300125122,\n",
              "   0.330885112285614,\n",
              "   0.5562868714332581,\n",
              "   -0.3902709186077118,\n",
              "   -0.10472173243761063,\n",
              "   -0.516977846622467,\n",
              "   0.4210990071296692,\n",
              "   0.3484540283679962,\n",
              "   0.0873764306306839,\n",
              "   0.289612740278244,\n",
              "   0.20991772413253784,\n",
              "   -0.03342445194721222,\n",
              "   -0.713840126991272,\n",
              "   -0.24306418001651764,\n",
              "   -0.3060707747936249,\n",
              "   0.02214542031288147,\n",
              "   -0.392218679189682,\n",
              "   -0.30341464281082153,\n",
              "   0.15252935886383057,\n",
              "   -0.03713994845747948,\n",
              "   0.07754432410001755,\n",
              "   0.43753111362457275,\n",
              "   0.03732232004404068,\n",
              "   1.209622859954834,\n",
              "   -0.925327718257904,\n",
              "   0.8307057619094849,\n",
              "   -0.13601326942443848,\n",
              "   0.2750299870967865,\n",
              "   -0.7193790078163147,\n",
              "   -0.5592150092124939,\n",
              "   0.2898899018764496,\n",
              "   0.45080140233039856,\n",
              "   0.3131895065307617,\n",
              "   -0.15859568119049072,\n",
              "   0.498060405254364,\n",
              "   0.36041882634162903,\n",
              "   0.10654217004776001,\n",
              "   -0.36414259672164917,\n",
              "   -0.3263864517211914,\n",
              "   0.5691198110580444,\n",
              "   0.2689850628376007,\n",
              "   0.789539098739624,\n",
              "   -0.03368242830038071,\n",
              "   -0.5743579864501953,\n",
              "   0.5175065398216248,\n",
              "   0.4895143508911133,\n",
              "   0.4635128378868103,\n",
              "   -0.028031442314386368,\n",
              "   0.44871145486831665,\n",
              "   0.07352828234434128,\n",
              "   0.2689046561717987,\n",
              "   0.4596381187438965,\n",
              "   -0.00811675563454628,\n",
              "   -0.7935664653778076,\n",
              "   -0.4243908226490021,\n",
              "   -0.08103419840335846,\n",
              "   0.40150922536849976,\n",
              "   0.28016945719718933,\n",
              "   0.052630357444286346,\n",
              "   -0.5914801359176636,\n",
              "   -0.17952856421470642,\n",
              "   0.1497192531824112,\n",
              "   -0.4555823504924774,\n",
              "   -0.21303796768188477,\n",
              "   0.181482195854187,\n",
              "   0.7198630571365356,\n",
              "   0.33322709798812866,\n",
              "   -0.08315529674291611,\n",
              "   -0.8167647123336792,\n",
              "   -0.07442343235015869,\n",
              "   -0.03256341814994812,\n",
              "   -0.6498286128044128,\n",
              "   0.7302214503288269,\n",
              "   0.727443277835846,\n",
              "   0.6790797114372253,\n",
              "   -0.7479855418205261,\n",
              "   0.12000536173582077,\n",
              "   0.23448461294174194,\n",
              "   0.3558422327041626,\n",
              "   -0.25512540340423584,\n",
              "   -0.4956207871437073,\n",
              "   0.4149499535560608,\n",
              "   -0.05856335535645485,\n",
              "   -0.10193818062543869,\n",
              "   0.49674978852272034,\n",
              "   0.26345792412757874,\n",
              "   0.017347997054457664,\n",
              "   -0.2606693208217621,\n",
              "   0.33783984184265137,\n",
              "   -0.5611252784729004,\n",
              "   0.03896009922027588,\n",
              "   0.04666810855269432,\n",
              "   0.056694529950618744,\n",
              "   -0.7578395009040833,\n",
              "   -0.11841955780982971,\n",
              "   0.0337056964635849,\n",
              "   -0.08928391337394714,\n",
              "   0.026955505833029747,\n",
              "   -0.6977609992027283,\n",
              "   -0.3579983413219452,\n",
              "   0.29250773787498474,\n",
              "   -0.3238348066806793,\n",
              "   0.22464975714683533,\n",
              "   -0.33713921904563904,\n",
              "   0.26462483406066895,\n",
              "   0.570389986038208,\n",
              "   -0.0636424571275711,\n",
              "   0.2925410866737366,\n",
              "   0.35265791416168213,\n",
              "   0.4517686665058136,\n",
              "   0.15898045897483826,\n",
              "   0.30207788944244385,\n",
              "   -0.11667902767658234,\n",
              "   0.4643690288066864,\n",
              "   0.993884801864624,\n",
              "   -0.6138915419578552,\n",
              "   -0.1219320073723793,\n",
              "   0.03578817471861839,\n",
              "   0.4176478683948517,\n",
              "   0.23022235929965973,\n",
              "   0.2602531313896179,\n",
              "   0.1569361686706543,\n",
              "   -0.5608527660369873,\n",
              "   0.33790579438209534,\n",
              "   -0.26311543583869934,\n",
              "   -0.11847900599241257,\n",
              "   0.19657334685325623,\n",
              "   -0.07657256722450256,\n",
              "   0.0033573985565453768,\n",
              "   0.07537928223609924,\n",
              "   -0.26559799909591675,\n",
              "   0.294137179851532,\n",
              "   0.2900336980819702,\n",
              "   0.39497533440589905,\n",
              "   -0.5646121501922607,\n",
              "   -0.26739487051963806,\n",
              "   0.3938266336917877,\n",
              "   -0.7087833881378174,\n",
              "   -0.5645331144332886,\n",
              "   -0.7762150168418884,\n",
              "   -0.2658824622631073,\n",
              "   -4.9963739002123475e-05,\n",
              "   0.4382827579975128,\n",
              "   -0.0872708112001419,\n",
              "   0.4360778033733368,\n",
              "   -0.27963799238204956,\n",
              "   -0.42903971672058105,\n",
              "   -0.22526651620864868,\n",
              "   -0.08461540937423706,\n",
              "   -0.06457038223743439,\n",
              "   -0.37608104944229126,\n",
              "   0.47583702206611633,\n",
              "   -0.32478004693984985,\n",
              "   1.012580156326294,\n",
              "   0.12829309701919556,\n",
              "   0.03179449960589409,\n",
              "   -0.2806284427642822,\n",
              "   -0.21715468168258667,\n",
              "   0.7863131761550903,\n",
              "   0.3278786540031433,\n",
              "   -0.4151381850242615,\n",
              "   -0.11509108543395996,\n",
              "   -0.07066600769758224,\n",
              "   0.24354663491249084,\n",
              "   -0.6319512128829956,\n",
              "   0.9848800301551819,\n",
              "   -0.031665340065956116,\n",
              "   0.7068045139312744,\n",
              "   0.0766129121184349,\n",
              "   -0.30406463146209717,\n",
              "   -0.43821802735328674,\n",
              "   0.6329255104064941,\n",
              "   -0.1807287484407425,\n",
              "   -0.3516833484172821,\n",
              "   0.30513080954551697,\n",
              "   0.08403816819190979,\n",
              "   0.03720114007592201,\n",
              "   -0.46769970655441284,\n",
              "   -0.1951628178358078,\n",
              "   0.2600460946559906,\n",
              "   0.16124700009822845,\n",
              "   -0.34074610471725464,\n",
              "   -0.12259957194328308,\n",
              "   0.39829567074775696,\n",
              "   0.9473172426223755,\n",
              "   -0.3063822090625763,\n",
              "   0.527519702911377,\n",
              "   0.4628005623817444,\n",
              "   -0.050227221101522446,\n",
              "   0.13650058209896088,\n",
              "   -0.5692894458770752,\n",
              "   -0.34801432490348816,\n",
              "   -0.18034060299396515,\n",
              "   -1.5324721336364746,\n",
              "   0.3378937244415283,\n",
              "   -1.1350980997085571,\n",
              "   -0.4755999743938446,\n",
              "   -0.9975692629814148,\n",
              "   -0.2652323842048645,\n",
              "   -0.28408515453338623,\n",
              "   -0.2579311728477478,\n",
              "   0.5557875037193298,\n",
              "   0.15257370471954346,\n",
              "   -0.47974252700805664,\n",
              "   -0.178766131401062,\n",
              "   -0.2386014610528946,\n",
              "   -0.38743457198143005,\n",
              "   -0.7759408354759216,\n",
              "   0.33356383442878723,\n",
              "   0.5309455394744873,\n",
              "   0.18199868500232697,\n",
              "   0.08616703748703003,\n",
              "   0.26444974541664124,\n",
              "   -0.2031380981206894,\n",
              "   0.17964288592338562,\n",
              "   1.0915273427963257,\n",
              "   -0.30780962109565735,\n",
              "   -0.4960046112537384,\n",
              "   -0.10517297685146332,\n",
              "   0.10157261788845062,\n",
              "   -0.5777711272239685,\n",
              "   -0.34493759274482727,\n",
              "   0.41298243403434753,\n",
              "   1.0531786680221558,\n",
              "   -0.2886509299278259,\n",
              "   0.31730660796165466,\n",
              "   -0.2188831865787506,\n",
              "   -0.45665276050567627,\n",
              "   0.6147239208221436,\n",
              "   0.5529576539993286,\n",
              "   -0.4579336941242218,\n",
              "   0.038910962641239166,\n",
              "   -0.2479797750711441,\n",
              "   0.3962097764015198,\n",
              "   -0.542760968208313,\n",
              "   -0.5053920745849609,\n",
              "   0.29010292887687683,\n",
              "   -0.20891357958316803,\n",
              "   0.12904438376426697,\n",
              "   -0.5132877230644226,\n",
              "   -0.4419538676738739,\n",
              "   -0.40096521377563477,\n",
              "   -0.3887430429458618,\n",
              "   -0.49346300959587097,\n",
              "   -0.27730435132980347,\n",
              "   0.5735948085784912,\n",
              "   -0.17311391234397888,\n",
              "   0.024707898497581482,\n",
              "   -0.24506241083145142,\n",
              "   -1.0660316944122314,\n",
              "   -3.6381850242614746,\n",
              "   0.09362979978322983,\n",
              "   -0.18005023896694183,\n",
              "   -0.15145201981067657,\n",
              "   0.11044783145189285,\n",
              "   0.01634720154106617,\n",
              "   0.1952340453863144,\n",
              "   -0.28182536363601685,\n",
              "   -1.2925443649291992,\n",
              "   -0.3719714283943176,\n",
              "   -0.6161844730377197,\n",
              "   -0.10947016626596451,\n",
              "   0.8260509967803955,\n",
              "   0.617983341217041,\n",
              "   0.8863746523857117,\n",
              "   -0.19358190894126892,\n",
              "   0.08549801260232925,\n",
              "   0.401627779006958,\n",
              "   -0.12919579446315765,\n",
              "   0.9982874989509583,\n",
              "   0.1419478952884674,\n",
              "   -0.3712864816188812,\n",
              "   -0.1999276578426361,\n",
              "   -0.15219813585281372,\n",
              "   0.7037853002548218,\n",
              "   0.3083331882953644,\n",
              "   0.2646414339542389,\n",
              "   0.033571507781744,\n",
              "   -0.4082121253013611,\n",
              "   0.3042808175086975,\n",
              "   -0.049423884600400925,\n",
              "   -0.35200440883636475,\n",
              "   -0.17357125878334045,\n",
              "   -0.026921292766928673,\n",
              "   -0.010305851697921753,\n",
              "   -0.00629445118829608,\n",
              "   0.15449316799640656,\n",
              "   -0.3687776029109955,\n",
              "   -0.12442972511053085,\n",
              "   0.5233833193778992,\n",
              "   -0.38663092255592346,\n",
              "   -0.8194969892501831,\n",
              "   -0.3007385730743408,\n",
              "   0.18407054245471954,\n",
              "   0.5843334197998047,\n",
              "   -0.11327168345451355,\n",
              "   0.2036534547805786,\n",
              "   -0.6661555171012878,\n",
              "   0.2746952176094055,\n",
              "   -0.038375936448574066,\n",
              "   0.2761036157608032,\n",
              "   -0.2828260362148285,\n",
              "   -0.18996413052082062,\n",
              "   -0.39489251375198364,\n",
              "   -0.5662197470664978,\n",
              "   -0.26965489983558655,\n",
              "   1.0462888479232788,\n",
              "   0.33707818388938904,\n",
              "   -0.4617050290107727,\n",
              "   -0.8034272789955139,\n",
              "   0.3134894371032715,\n",
              "   -0.3039684593677521,\n",
              "   -0.8649023771286011,\n",
              "   -0.09038109332323074,\n",
              "   0.20130988955497742,\n",
              "   -0.09932972490787506,\n",
              "   -0.02878444641828537,\n",
              "   -0.10120724141597748,\n",
              "   0.15417607128620148,\n",
              "   0.5228806734085083,\n",
              "   0.08506288379430771,\n",
              "   0.05471542850136757,\n",
              "   -0.6826478242874146,\n",
              "   -0.9763070344924927,\n",
              "   -0.16604584455490112,\n",
              "   0.28695398569107056,\n",
              "   -0.18143509328365326,\n",
              "   -0.20334555208683014,\n",
              "   0.10296190530061722,\n",
              "   0.3012513220310211,\n",
              "   -1.0841312408447266,\n",
              "   -0.7240371704101562,\n",
              "   -0.40359896421432495,\n",
              "   -0.5291231870651245,\n",
              "   0.4841257631778717,\n",
              "   -1.4230071306228638,\n",
              "   -0.4367891550064087,\n",
              "   -0.587872326374054,\n",
              "   -0.09149325639009476,\n",
              "   -0.6201649308204651,\n",
              "   0.26154640316963196,\n",
              "   0.16637323796749115,\n",
              "   0.16522769629955292,\n",
              "   -0.16727831959724426,\n",
              "   0.21114608645439148,\n",
              "   0.3049509823322296,\n",
              "   0.225411057472229,\n",
              "   -0.4959942698478699,\n",
              "   0.46287035942077637,\n",
              "   -0.5982124209403992,\n",
              "   0.5420795679092407,\n",
              "   -0.08422878384590149,\n",
              "   1.243853211402893,\n",
              "   -0.002828043419867754,\n",
              "   0.011113191023468971,\n",
              "   -0.44073209166526794,\n",
              "   0.07177652418613434,\n",
              "   -0.3486272692680359,\n",
              "   -0.30544114112854004,\n",
              "   0.4331880807876587,\n",
              "   0.42399439215660095,\n",
              "   -0.1686864048242569,\n",
              "   1.1811639070510864,\n",
              "   -0.4413892328739166,\n",
              "   -0.03150096535682678,\n",
              "   -0.30446866154670715,\n",
              "   0.3982897102832794,\n",
              "   1.1205432415008545,\n",
              "   -0.29777592420578003,\n",
              "   0.0622699111700058,\n",
              "   -0.40628138184547424,\n",
              "   0.9769352674484253,\n",
              "   -0.2993408739566803,\n",
              "   -0.6917768716812134,\n",
              "   -0.39398685097694397,\n",
              "   0.0007585505372844636,\n",
              "   -0.45529282093048096,\n",
              "   -1.3690325021743774,\n",
              "   -0.022750193253159523,\n",
              "   0.40846312046051025,\n",
              "   -0.8466689586639404,\n",
              "   -0.29027536511421204,\n",
              "   0.2668050527572632,\n",
              "   0.021867655217647552,\n",
              "   0.49252089858055115,\n",
              "   0.06861542910337448,\n",
              "   0.17396603524684906,\n",
              "   -0.6677910685539246,\n",
              "   0.07130417972803116,\n",
              "   0.33017799258232117,\n",
              "   0.13006436824798584,\n",
              "   0.5054259896278381,\n",
              "   0.016143448650836945,\n",
              "   -0.2896873354911804,\n",
              "   -0.35586872696876526,\n",
              "   0.9265490770339966,\n",
              "   0.5316137671470642,\n",
              "   -0.19075874984264374,\n",
              "   -0.24468904733657837,\n",
              "   0.646063506603241,\n",
              "   -0.2789539396762848,\n",
              "   -0.7601656913757324,\n",
              "   -0.30016395449638367,\n",
              "   -0.28611376881599426,\n",
              "   0.46922603249549866,\n",
              "   0.08412998169660568,\n",
              "   0.6921231746673584,\n",
              "   -0.29991012811660767,\n",
              "   -0.059467848390340805,\n",
              "   -0.39518457651138306,\n",
              "   -0.41896262764930725,\n",
              "   -0.011804522015154362,\n",
              "   0.8456019163131714,\n",
              "   0.5138698816299438,\n",
              "   0.2569256126880646,\n",
              "   0.09417197853326797,\n",
              "   -0.36942753195762634,\n",
              "   -0.04365484043955803,\n",
              "   0.3584216833114624,\n",
              "   0.14706188440322876,\n",
              "   -0.16267834603786469,\n",
              "   -0.33498314023017883,\n",
              "   0.2751023471355438,\n",
              "   -0.12158093601465225,\n",
              "   -0.9458434581756592,\n",
              "   0.9070746898651123,\n",
              "   0.5046346187591553,\n",
              "   -0.11773593723773956,\n",
              "   -0.705805242061615,\n",
              "   0.5951581597328186,\n",
              "   -0.30804353952407837,\n",
              "   0.11021576076745987,\n",
              "   0.3007504642009735,\n",
              "   -0.20869235694408417,\n",
              "   0.8148142695426941,\n",
              "   0.4301045536994934,\n",
              "   0.5349463224411011,\n",
              "   -0.002759687602519989,\n",
              "   0.01953473500907421,\n",
              "   0.20686979591846466,\n",
              "   -0.11767172813415527,\n",
              "   0.08532451838254929,\n",
              "   -0.1628689020872116,\n",
              "   -0.5385984182357788,\n",
              "   -0.241206556558609,\n",
              "   -0.3358083963394165,\n",
              "   1.0925935506820679,\n",
              "   0.2878771424293518,\n",
              "   0.002674046205356717,\n",
              "   0.030368613079190254,\n",
              "   -0.11303982883691788,\n",
              "   -0.9106400012969971,\n",
              "   -0.3233283460140228,\n",
              "   -0.1458657830953598,\n",
              "   0.29652753472328186,\n",
              "   -0.7916387319564819,\n",
              "   0.4477829933166504,\n",
              "   0.27207911014556885,\n",
              "   0.39131224155426025,\n",
              "   0.37521791458129883,\n",
              "   -0.27166998386383057,\n",
              "   -0.4818141460418701,\n",
              "   -0.20491208136081696,\n",
              "   -0.362000972032547,\n",
              "   0.47332364320755005,\n",
              "   -0.27998459339141846,\n",
              "   -0.04147461801767349,\n",
              "   -0.22112539410591125,\n",
              "   -0.4470163881778717,\n",
              "   0.028875255957245827,\n",
              "   -0.4917827248573303,\n",
              "   0.3602440655231476,\n",
              "   -0.3683837652206421,\n",
              "   0.15595068037509918,\n",
              "   -0.8340955972671509,\n",
              "   -0.47073638439178467,\n",
              "   0.25828248262405396,\n",
              "   -0.08859407901763916,\n",
              "   -0.7348707914352417,\n",
              "   -0.1401338279247284,\n",
              "   0.26809802651405334,\n",
              "   -1.0111433267593384,\n",
              "   0.026199443265795708,\n",
              "   0.29499638080596924,\n",
              "   -0.39615681767463684,\n",
              "   0.06105048581957817,\n",
              "   -0.18947432935237885,\n",
              "   0.00017280764586757869,\n",
              "   -0.35329827666282654,\n",
              "   0.08081944286823273,\n",
              "   -0.423665314912796,\n",
              "   -0.5340661406517029,\n",
              "   -0.10573092103004456,\n",
              "   -0.46515291929244995,\n",
              "   0.18469151854515076,\n",
              "   -0.17482230067253113,\n",
              "   -0.1462695151567459,\n",
              "   0.8746044039726257,\n",
              "   -0.7665292024612427,\n",
              "   0.11333323270082474,\n",
              "   0.41738829016685486,\n",
              "   -0.11526482552289963,\n",
              "   0.1561352163553238,\n",
              "   -0.5426103472709656,\n",
              "   0.07435446232557297,\n",
              "   -0.03983476758003235,\n",
              "   -0.5112213492393494,\n",
              "   0.10256404429674149,\n",
              "   0.16894274950027466,\n",
              "   -0.02698839269578457,\n",
              "   -0.32013633847236633,\n",
              "   -0.47975409030914307,\n",
              "   -0.5336804389953613,\n",
              "   0.19016529619693756,\n",
              "   0.3046889305114746,\n",
              "   0.28791728615760803,\n",
              "   -0.05418476462364197,\n",
              "   0.8313710689544678,\n",
              "   -0.019006824120879173,\n",
              "   -0.21910254657268524,\n",
              "   0.5852163434028625,\n",
              "   -0.05858810245990753,\n",
              "   0.3700666129589081,\n",
              "   -0.2739481031894684,\n",
              "   0.2071223258972168,\n",
              "   0.0920608639717102,\n",
              "   -0.09284941852092743,\n",
              "   -0.3493739068508148,\n",
              "   0.45943355560302734,\n",
              "   -0.1156519278883934,\n",
              "   -0.18413662910461426,\n",
              "   0.05764120817184448,\n",
              "   0.48631954193115234,\n",
              "   -0.2721439599990845,\n",
              "   0.0269059706479311,\n",
              "   -0.010096200741827488,\n",
              "   0.3000987470149994,\n",
              "   1.0998592376708984,\n",
              "   0.4306967556476593,\n",
              "   -0.15614314377307892,\n",
              "   0.26109305024147034,\n",
              "   0.6036436557769775,\n",
              "   0.4114758372306824,\n",
              "   0.20660214126110077,\n",
              "   -0.12174328416585922,\n",
              "   0.1458410620689392,\n",
              "   0.6681695580482483,\n",
              "   0.3070647120475769,\n",
              "   -0.0381302535533905,\n",
              "   -0.19729438424110413,\n",
              "   -0.5301450490951538,\n",
              "   -0.12286590039730072,\n",
              "   0.42739710211753845,\n",
              "   0.6804860830307007,\n",
              "   -0.44710347056388855,\n",
              "   0.45082128047943115,\n",
              "   0.43758177757263184,\n",
              "   -0.2875754237174988,\n",
              "   0.31431663036346436,\n",
              "   0.5186235904693604,\n",
              "   0.08052880316972733,\n",
              "   -0.6205043196678162,\n",
              "   1.0836944580078125,\n",
              "   0.9041215777397156,\n",
              "   -0.900031566619873,\n",
              "   -0.6415663957595825,\n",
              "   0.005184014793485403,\n",
              "   0.20150241255760193,\n",
              "   0.43834394216537476,\n",
              "   0.34048640727996826,\n",
              "   0.42836859822273254,\n",
              "   -0.2837957441806793,\n",
              "   0.31044790148735046,\n",
              "   0.045588307082653046,\n",
              "   0.017819060012698174,\n",
              "   0.6164402961730957,\n",
              "   0.02529587596654892,\n",
              "   -0.42681360244750977,\n",
              "   0.28821510076522827,\n",
              "   0.333848774433136,\n",
              "   -0.49379026889801025,\n",
              "   -0.12007344514131546,\n",
              "   -0.7027686238288879,\n",
              "   0.4156906008720398,\n",
              "   -0.26739129424095154,\n",
              "   -0.2615123689174652,\n",
              "   -0.10951446741819382,\n",
              "   0.33302736282348633,\n",
              "   0.36041176319122314,\n",
              "   -0.3297092020511627,\n",
              "   0.020542100071907043,\n",
              "   0.5654990673065186,\n",
              "   0.7332457900047302,\n",
              "   1.1385047435760498,\n",
              "   0.4213058054447174,\n",
              "   0.4384717643260956,\n",
              "   0.38835859298706055,\n",
              "   -0.6528413891792297,\n",
              "   0.8523520231246948,\n",
              "   0.40520527958869934,\n",
              "   -0.047251731157302856,\n",
              "   0.6539430022239685,\n",
              "   -0.005770055111497641,\n",
              "   -0.1891719102859497,\n",
              "   0.4858177900314331,\n",
              "   -0.47887253761291504,\n",
              "   0.2908335030078888,\n",
              "   0.9159520268440247,\n",
              "   -0.019607611000537872,\n",
              "   0.5811401605606079,\n",
              "   0.597138524055481,\n",
              "   0.4946798086166382,\n",
              "   0.025674981996417046,\n",
              "   -0.705281138420105,\n",
              "   0.014390313066542149,\n",
              "   0.43620046973228455,\n",
              "   0.8540464043617249,\n",
              "   -0.35932499170303345,\n",
              "   -0.16270564496517181,\n",
              "   0.23955775797367096,\n",
              "   -0.29485735297203064,\n",
              "   -0.1356000453233719,\n",
              "   -0.11510898172855377,\n",
              "   -0.6994284987449646,\n",
              "   -0.21331173181533813,\n",
              "   0.12084761261940002,\n",
              "   -0.17342032492160797,\n",
              "   -0.4557623863220215,\n",
              "   -0.20075775682926178,\n",
              "   0.7148731350898743,\n",
              "   0.8104689121246338,\n",
              "   0.18085657060146332,\n",
              "   -0.3709663152694702,\n",
              "   -0.35976922512054443,\n",
              "   -0.5941434502601624,\n",
              "   -0.7878499627113342,\n",
              "   -0.017360588535666466,\n",
              "   -0.30509519577026367,\n",
              "   0.17579014599323273,\n",
              "   -0.06462924927473068,\n",
              "   0.24181288480758667,\n",
              "   -0.2453770488500595,\n",
              "   -0.39453619718551636,\n",
              "   -0.796262264251709,\n",
              "   0.14242272078990936,\n",
              "   -0.6708372831344604,\n",
              "   0.22046321630477905,\n",
              "   0.34808531403541565,\n",
              "   0.6628511548042297,\n",
              "   -0.24069331586360931,\n",
              "   0.09466211497783661,\n",
              "   0.21085868775844574,\n",
              "   -0.18491627275943756,\n",
              "   -0.07784810662269592,\n",
              "   -0.3016917407512665,\n",
              "   0.06629814952611923,\n",
              "   -0.5595464706420898,\n",
              "   0.5813827514648438,\n",
              "   -0.4590427577495575,\n",
              "   0.037987641990184784,\n",
              "   0.017124371603131294,\n",
              "   0.7891268134117126,\n",
              "   -0.42963656783103943,\n",
              "   0.2051423192024231,\n",
              "   -0.452004998922348,\n",
              "   -0.29891151189804077,\n",
              "   0.546800971031189,\n",
              "   -0.12245971709489822,\n",
              "   -0.2282593697309494,\n",
              "   0.7038829326629639,\n",
              "   0.3192180395126343,\n",
              "   -0.2779872417449951,\n",
              "   -0.14287415146827698,\n",
              "   0.7525047063827515,\n",
              "   0.15934672951698303,\n",
              "   -0.12886649370193481,\n",
              "   0.14189274609088898,\n",
              "   0.5808618664741516,\n",
              "   0.23147930204868317,\n",
              "   -0.6166960000991821,\n",
              "   -0.26317667961120605,\n",
              "   0.27594754099845886,\n",
              "   0.26030847430229187,\n",
              "   -0.3238131105899811,\n",
              "   -0.38199806213378906,\n",
              "   -0.305545836687088,\n",
              "   -0.6412554979324341,\n",
              "   -0.019520418718457222,\n",
              "   -0.5700313448905945,\n",
              "   -0.5657910704612732,\n",
              "   0.26883071660995483,\n",
              "   0.06522741168737411,\n",
              "   0.2753335237503052,\n",
              "   0.16287696361541748,\n",
              "   1.0249483585357666,\n",
              "   -0.6806690096855164,\n",
              "   -0.4375724196434021,\n",
              "   -0.7855058312416077,\n",
              "   0.4798479378223419,\n",
              "   -0.3260570466518402,\n",
              "   -0.05309435725212097,\n",
              "   -0.060998376458883286,\n",
              "   0.25911813974380493,\n",
              "   -0.722008228302002,\n",
              "   -0.13070346415042877,\n",
              "   0.39880114793777466,\n",
              "   -0.08384477347135544,\n",
              "   -0.31313762068748474,\n",
              "   0.3187757134437561,\n",
              "   0.11651685833930969],\n",
              "  [0.0426807776093483,\n",
              "   -0.013645250350236893,\n",
              "   0.4714745879173279,\n",
              "   0.28927212953567505,\n",
              "   0.4132942855358124,\n",
              "   -1.0063236951828003,\n",
              "   0.28293317556381226,\n",
              "   0.7238183617591858,\n",
              "   -0.6014288067817688,\n",
              "   -0.5218938589096069,\n",
              "   0.12050914019346237,\n",
              "   -0.2776881754398346,\n",
              "   -0.41629037261009216,\n",
              "   0.5429713129997253,\n",
              "   -0.05003074184060097,\n",
              "   0.6390940546989441,\n",
              "   0.367798388004303,\n",
              "   -0.04751218855381012,\n",
              "   0.27212879061698914,\n",
              "   -0.3743765950202942,\n",
              "   -0.11923138052225113,\n",
              "   -0.5220564007759094,\n",
              "   -0.07724092900753021,\n",
              "   0.7303982377052307,\n",
              "   0.26144149899482727,\n",
              "   0.6467394828796387,\n",
              "   -0.13687364757061005,\n",
              "   0.29683566093444824,\n",
              "   -0.11716067045927048,\n",
              "   -0.08884608745574951,\n",
              "   0.021839026361703873,\n",
              "   -0.06425324827432632,\n",
              "   -0.619495153427124,\n",
              "   -0.25372734665870667,\n",
              "   -0.21031422913074493,\n",
              "   -0.2672904431819916,\n",
              "   -0.12450622022151947,\n",
              "   -0.07453373074531555,\n",
              "   -0.03511418029665947,\n",
              "   0.27889642119407654,\n",
              "   -0.7224372625350952,\n",
              "   -0.7747911810874939,\n",
              "   -0.23124577105045319,\n",
              "   -0.07808841019868851,\n",
              "   0.15744952857494354,\n",
              "   0.2772749066352844,\n",
              "   1.0702903270721436,\n",
              "   0.38223689794540405,\n",
              "   0.12951132655143738,\n",
              "   -0.07881113886833191,\n",
              "   0.002753628185018897,\n",
              "   0.3468491733074188,\n",
              "   -0.33170151710510254,\n",
              "   -0.38500547409057617,\n",
              "   0.011045940220355988,\n",
              "   0.9336801171302795,\n",
              "   -0.20719105005264282,\n",
              "   -0.8470925688743591,\n",
              "   -0.5201619863510132,\n",
              "   0.3522655665874481,\n",
              "   0.29186582565307617,\n",
              "   -0.535313069820404,\n",
              "   -0.273246169090271,\n",
              "   -0.18358421325683594,\n",
              "   0.41311508417129517,\n",
              "   0.22866293787956238,\n",
              "   0.04974256083369255,\n",
              "   -0.46626466512680054,\n",
              "   -0.4019835591316223,\n",
              "   0.38491106033325195,\n",
              "   -0.8695037364959717,\n",
              "   -0.4330139756202698,\n",
              "   0.14158499240875244,\n",
              "   0.357801616191864,\n",
              "   -0.7638835906982422,\n",
              "   0.050845663994550705,\n",
              "   0.3519675135612488,\n",
              "   0.12534292042255402,\n",
              "   0.1737346202135086,\n",
              "   0.20277424156665802,\n",
              "   -0.4277600049972534,\n",
              "   0.4347236752510071,\n",
              "   0.3825902044773102,\n",
              "   0.2859538197517395,\n",
              "   0.00827248115092516,\n",
              "   -0.001869436353445053,\n",
              "   -0.7880272269248962,\n",
              "   -0.4960066080093384,\n",
              "   -0.03541981801390648,\n",
              "   0.8137214183807373,\n",
              "   -0.15058067440986633,\n",
              "   -0.005445245653390884,\n",
              "   0.14806686341762543,\n",
              "   -0.05002938583493233,\n",
              "   0.1478659063577652,\n",
              "   -0.39412492513656616,\n",
              "   -0.17866499722003937,\n",
              "   0.2617088258266449,\n",
              "   0.03992516174912453,\n",
              "   0.517059862613678,\n",
              "   0.07219205051660538,\n",
              "   -0.23700803518295288,\n",
              "   0.05377013608813286,\n",
              "   -0.18542060256004333,\n",
              "   0.5646752119064331,\n",
              "   -0.29892823100090027,\n",
              "   0.866199791431427,\n",
              "   -0.27751901745796204,\n",
              "   0.04280227795243263,\n",
              "   0.2943674921989441,\n",
              "   -0.4499267339706421,\n",
              "   -0.3616696298122406,\n",
              "   -0.27835744619369507,\n",
              "   0.08270195871591568,\n",
              "   0.17494449019432068,\n",
              "   0.3784426748752594,\n",
              "   0.274335116147995,\n",
              "   -0.9039535522460938,\n",
              "   -0.21704860031604767,\n",
              "   -0.44477489590644836,\n",
              "   -0.2477320432662964,\n",
              "   0.24979136884212494,\n",
              "   0.5662675499916077,\n",
              "   0.8046910166740417,\n",
              "   -0.055311840027570724,\n",
              "   0.2064073085784912,\n",
              "   -0.26988232135772705,\n",
              "   -0.208043172955513,\n",
              "   -0.06709348410367966,\n",
              "   -0.42027193307876587,\n",
              "   0.7019696235656738,\n",
              "   0.6547712087631226,\n",
              "   0.7274819612503052,\n",
              "   -0.7051500082015991,\n",
              "   -0.26185280084609985,\n",
              "   -0.05975889787077904,\n",
              "   0.3835817873477936,\n",
              "   -0.04153857380151749,\n",
              "   -0.38804182410240173,\n",
              "   0.27546098828315735,\n",
              "   -0.06372003257274628,\n",
              "   -0.09130803495645523,\n",
              "   0.7834653258323669,\n",
              "   0.4741727411746979,\n",
              "   0.06294360011816025,\n",
              "   0.005068665370345116,\n",
              "   0.6525068283081055,\n",
              "   -0.30200374126434326,\n",
              "   -0.06341880559921265,\n",
              "   -0.13899552822113037,\n",
              "   0.02433316968381405,\n",
              "   -0.4888139069080353,\n",
              "   -0.15182629227638245,\n",
              "   -0.26371458172798157,\n",
              "   -0.15684017539024353,\n",
              "   -0.1928061544895172,\n",
              "   -0.4483664631843567,\n",
              "   -0.009371043182909489,\n",
              "   -0.20768292248249054,\n",
              "   0.02986053004860878,\n",
              "   0.46811768412590027,\n",
              "   -0.14947101473808289,\n",
              "   -0.015751151368021965,\n",
              "   0.3925423324108124,\n",
              "   0.24137382209300995,\n",
              "   -0.0243300162255764,\n",
              "   0.04601588472723961,\n",
              "   -0.005787511356174946,\n",
              "   0.035897355526685715,\n",
              "   -0.10269250720739365,\n",
              "   -0.5922184586524963,\n",
              "   -0.6101381182670593,\n",
              "   0.8936710357666016,\n",
              "   -0.14561444520950317,\n",
              "   0.1583903729915619,\n",
              "   0.2982153296470642,\n",
              "   -0.08133184909820557,\n",
              "   0.27101999521255493,\n",
              "   0.3501744866371155,\n",
              "   0.2607216536998749,\n",
              "   -0.0821852907538414,\n",
              "   0.12669114768505096,\n",
              "   0.48418375849723816,\n",
              "   0.2603791058063507,\n",
              "   0.33849263191223145,\n",
              "   0.034291863441467285,\n",
              "   0.9826279282569885,\n",
              "   -0.4113103747367859,\n",
              "   -0.27786359190940857,\n",
              "   0.20202210545539856,\n",
              "   -0.654101550579071,\n",
              "   -0.9051730036735535,\n",
              "   -0.2284158617258072,\n",
              "   0.1372218281030655,\n",
              "   -0.06815627962350845,\n",
              "   -1.0261937379837036,\n",
              "   -0.44314634799957275,\n",
              "   -0.37753671407699585,\n",
              "   -0.16120992600917816,\n",
              "   -0.30096298456192017,\n",
              "   0.13655434548854828,\n",
              "   -0.12814413011074066,\n",
              "   0.37182408571243286,\n",
              "   -0.166144460439682,\n",
              "   -0.19237801432609558,\n",
              "   0.06754407286643982,\n",
              "   -0.09346313029527664,\n",
              "   -0.13353382050991058,\n",
              "   -0.20138958096504211,\n",
              "   -0.15625569224357605,\n",
              "   -0.5073018074035645,\n",
              "   0.14156703650951385,\n",
              "   0.18759353458881378,\n",
              "   0.33794593811035156,\n",
              "   -0.0690394788980484,\n",
              "   0.1340976059436798,\n",
              "   0.4670961797237396,\n",
              "   -0.02325439453125,\n",
              "   0.0033572509419173002,\n",
              "   0.03435802459716797,\n",
              "   -0.44698673486709595,\n",
              "   0.3145247995853424,\n",
              "   -0.6670451164245605,\n",
              "   0.6937428712844849,\n",
              "   0.18104144930839539,\n",
              "   1.0062477588653564,\n",
              "   0.44348573684692383,\n",
              "   -0.3663845658302307,\n",
              "   -0.1387476772069931,\n",
              "   0.022963479161262512,\n",
              "   -0.008120383135974407,\n",
              "   -0.3339175581932068,\n",
              "   0.27816420793533325,\n",
              "   0.5361887216567993,\n",
              "   0.40048810839653015,\n",
              "   -0.45724034309387207,\n",
              "   -0.19159218668937683,\n",
              "   0.07558924704790115,\n",
              "   0.7138167023658752,\n",
              "   -0.2180437445640564,\n",
              "   -0.2292046695947647,\n",
              "   0.32927727699279785,\n",
              "   1.0918645858764648,\n",
              "   0.01760121062397957,\n",
              "   0.2162584364414215,\n",
              "   0.1941610723733902,\n",
              "   -0.17650651931762695,\n",
              "   0.1618250012397766,\n",
              "   0.027960464358329773,\n",
              "   -0.345009982585907,\n",
              "   -0.20748259127140045,\n",
              "   -0.27292975783348083,\n",
              "   0.017393410205841064,\n",
              "   -0.7404062747955322,\n",
              "   -0.7612658739089966,\n",
              "   -0.9104903936386108,\n",
              "   -0.02032190002501011,\n",
              "   0.04728899151086807,\n",
              "   -0.0527043491601944,\n",
              "   0.6564406752586365,\n",
              "   -0.645474910736084,\n",
              "   0.09368805587291718,\n",
              "   -0.6073344945907593,\n",
              "   -0.24503743648529053,\n",
              "   -0.3850593566894531,\n",
              "   -0.4699459671974182,\n",
              "   -0.36183279752731323,\n",
              "   0.7217720746994019,\n",
              "   0.23909245431423187,\n",
              "   0.17936575412750244,\n",
              "   0.3301379978656769,\n",
              "   -0.12016790360212326,\n",
              "   0.44407400488853455,\n",
              "   0.8953613638877869,\n",
              "   -0.17585088312625885,\n",
              "   -1.0876468420028687,\n",
              "   -0.007391452789306641,\n",
              "   -0.011787044815719128,\n",
              "   -0.2204083651304245,\n",
              "   -0.14638651907444,\n",
              "   0.3423728048801422,\n",
              "   0.5798707008361816,\n",
              "   0.045090410858392715,\n",
              "   0.015342572703957558,\n",
              "   -0.20166350901126862,\n",
              "   -0.6156066656112671,\n",
              "   0.3746309280395508,\n",
              "   0.32800689339637756,\n",
              "   -0.3686144948005676,\n",
              "   0.13886375725269318,\n",
              "   -0.12961570918560028,\n",
              "   -0.1605978161096573,\n",
              "   -0.7432242631912231,\n",
              "   0.050412166863679886,\n",
              "   0.4061774015426636,\n",
              "   -0.05705473944544792,\n",
              "   0.3264000117778778,\n",
              "   -0.519729495048523,\n",
              "   -0.12808729708194733,\n",
              "   -0.39887017011642456,\n",
              "   0.7308669686317444,\n",
              "   -0.11059276759624481,\n",
              "   0.6358407735824585,\n",
              "   0.5221713185310364,\n",
              "   -0.28612086176872253,\n",
              "   -0.13522280752658844,\n",
              "   -0.018045563250780106,\n",
              "   -0.6399438977241516,\n",
              "   -4.014870643615723,\n",
              "   0.06657131016254425,\n",
              "   -0.3616737425327301,\n",
              "   0.12614266574382782,\n",
              "   0.4244595766067505,\n",
              "   -0.027203848585486412,\n",
              "   0.17741423845291138,\n",
              "   -0.1617777794599533,\n",
              "   -0.8322892785072327,\n",
              "   -0.3852573037147522,\n",
              "   -0.31242233514785767,\n",
              "   0.04274914786219597,\n",
              "   0.2081282138824463,\n",
              "   0.5032172799110413,\n",
              "   0.3406563997268677,\n",
              "   -0.5776545405387878,\n",
              "   -0.40021130442619324,\n",
              "   0.33775901794433594,\n",
              "   -0.01113925315439701,\n",
              "   1.234699010848999,\n",
              "   0.15131476521492004,\n",
              "   -0.15637703239917755,\n",
              "   0.26867201924324036,\n",
              "   0.02975301258265972,\n",
              "   0.5828565359115601,\n",
              "   -0.3055432438850403,\n",
              "   0.41096818447113037,\n",
              "   -0.08905313909053802,\n",
              "   -0.3947497308254242,\n",
              "   0.3487555682659149,\n",
              "   -0.4058757424354553,\n",
              "   -0.7032781839370728,\n",
              "   -0.2642785906791687,\n",
              "   -0.3719061613082886,\n",
              "   0.4440068304538727,\n",
              "   -0.27059826254844666,\n",
              "   0.2683551609516144,\n",
              "   -0.07468509674072266,\n",
              "   0.17923825979232788,\n",
              "   0.371222585439682,\n",
              "   -0.20097331702709198,\n",
              "   -1.2206405401229858,\n",
              "   -0.11687512695789337,\n",
              "   0.5375786423683167,\n",
              "   0.5229477882385254,\n",
              "   0.01461341418325901,\n",
              "   -0.577526867389679,\n",
              "   -0.6615521311759949,\n",
              "   0.11186514049768448,\n",
              "   0.12684296071529388,\n",
              "   0.4824320077896118,\n",
              "   0.17504805326461792,\n",
              "   0.23099125921726227,\n",
              "   -0.7006745934486389,\n",
              "   -0.5408235788345337,\n",
              "   -0.3670210838317871,\n",
              "   0.9011607766151428,\n",
              "   0.34519362449645996,\n",
              "   -0.48083731532096863,\n",
              "   -0.39108186960220337,\n",
              "   0.04164283722639084,\n",
              "   -0.32752665877342224,\n",
              "   -0.1765967607498169,\n",
              "   -0.7412896156311035,\n",
              "   0.583905816078186,\n",
              "   0.14215560257434845,\n",
              "   -0.47357961535453796,\n",
              "   -0.03558362275362015,\n",
              "   -0.24604417383670807,\n",
              "   0.5648850202560425,\n",
              "   0.20101960003376007,\n",
              "   0.17217794060707092,\n",
              "   -0.3811708092689514,\n",
              "   -0.9508840441703796,\n",
              "   0.1556893140077591,\n",
              "   0.20445005595684052,\n",
              "   0.15661101043224335,\n",
              "   0.1666906625032425,\n",
              "   0.46083149313926697,\n",
              "   0.555548369884491,\n",
              "   -0.5963500738143921,\n",
              "   -0.7167907357215881,\n",
              "   -0.23923450708389282,\n",
              "   -0.5302295088768005,\n",
              "   0.13002531230449677,\n",
              "   -0.7941460609436035,\n",
              "   -0.24543902277946472,\n",
              "   -0.2856557369232178,\n",
              "   0.07781438529491425,\n",
              "   -0.2679542601108551,\n",
              "   0.32905420660972595,\n",
              "   0.38247981667518616,\n",
              "   0.6493341326713562,\n",
              "   0.16983000934123993,\n",
              "   0.6619423627853394,\n",
              "   0.25216203927993774,\n",
              "   0.18146462738513947,\n",
              "   -0.17702478170394897,\n",
              "   -0.04373469576239586,\n",
              "   -0.3634871244430542,\n",
              "   -0.1463755965232849,\n",
              "   -0.27038541436195374,\n",
              "   1.019855260848999,\n",
              "   -0.4387221336364746,\n",
              "   0.023977402597665787,\n",
              "   -0.03101980686187744,\n",
              "   -0.35270875692367554,\n",
              "   0.18323096632957458,\n",
              "   -0.17070575058460236,\n",
              "   0.3108067214488983,\n",
              "   0.2645488977432251,\n",
              "   -0.03171606361865997,\n",
              "   0.8554071187973022,\n",
              "   -0.31720274686813354,\n",
              "   -0.13636581599712372,\n",
              "   -0.20045462250709534,\n",
              "   0.6677413582801819,\n",
              "   0.6693435311317444,\n",
              "   -0.12712830305099487,\n",
              "   -0.24051795899868011,\n",
              "   -0.3911168873310089,\n",
              "   0.3392980992794037,\n",
              "   -0.0633905827999115,\n",
              "   -0.6325379014015198,\n",
              "   -0.9056247472763062,\n",
              "   0.046415578573942184,\n",
              "   -0.45720770955085754,\n",
              "   -0.8054443001747131,\n",
              "   0.17933087050914764,\n",
              "   0.1557396650314331,\n",
              "   -0.6900442838668823,\n",
              "   -0.8646301627159119,\n",
              "   0.017727067694067955,\n",
              "   0.4495227634906769,\n",
              "   0.1290210634469986,\n",
              "   0.029414741322398186,\n",
              "   -0.4178885817527771,\n",
              "   -0.9289345741271973,\n",
              "   -0.1019577831029892,\n",
              "   0.6084113717079163,\n",
              "   0.4321249723434448,\n",
              "   0.9291136860847473,\n",
              "   0.20652003586292267,\n",
              "   -0.38327911496162415,\n",
              "   -0.42561566829681396,\n",
              "   0.36687400937080383,\n",
              "   0.21170340478420258,\n",
              "   -0.32189440727233887,\n",
              "   -0.06801088899374008,\n",
              "   0.8167492151260376,\n",
              "   -0.8004114031791687,\n",
              "   -0.44170209765434265,\n",
              "   0.10943327844142914,\n",
              "   0.44143375754356384,\n",
              "   0.027043960988521576,\n",
              "   0.2870955467224121,\n",
              "   0.6720623970031738,\n",
              "   -0.3309500515460968,\n",
              "   -0.13923081755638123,\n",
              "   -0.17923912405967712,\n",
              "   -0.20718422532081604,\n",
              "   -0.5934246182441711,\n",
              "   0.3982623219490051,\n",
              "   0.3403465747833252,\n",
              "   0.3710985481739044,\n",
              "   0.5869454741477966,\n",
              "   -0.250337690114975,\n",
              "   -0.045657653361558914,\n",
              "   0.2943037152290344,\n",
              "   -0.04282599687576294,\n",
              "   0.0848735123872757,\n",
              "   0.003375678090378642,\n",
              "   -0.5816295146942139,\n",
              "   -0.42558473348617554,\n",
              "   -0.8120574951171875,\n",
              "   0.3681441843509674,\n",
              "   0.19108721613883972,\n",
              "   -0.11617906391620636,\n",
              "   0.07442279160022736,\n",
              "   0.7113635540008545,\n",
              "   0.02057797648012638,\n",
              "   0.10173375904560089,\n",
              "   -0.25851717591285706,\n",
              "   -0.2752452790737152,\n",
              "   0.7057341933250427,\n",
              "   -0.16671176254749298,\n",
              "   0.7026982307434082,\n",
              "   0.24383975565433502,\n",
              "   0.22864970564842224,\n",
              "   0.49562978744506836,\n",
              "   0.03412800282239914,\n",
              "   0.2661134600639343,\n",
              "   -0.8581229448318481,\n",
              "   -0.5981288552284241,\n",
              "   -0.1639682650566101,\n",
              "   -0.7556725144386292,\n",
              "   1.143642544746399,\n",
              "   0.6421597003936768,\n",
              "   -0.1417664885520935,\n",
              "   0.3296537399291992,\n",
              "   -0.13548150658607483,\n",
              "   0.19957520067691803,\n",
              "   -0.5034219622612,\n",
              "   0.05352092534303665,\n",
              "   0.01763046346604824,\n",
              "   -0.3596132695674896,\n",
              "   0.6645937561988831,\n",
              "   0.2530268728733063,\n",
              "   0.06915063410997391,\n",
              "   0.011064608581364155,\n",
              "   -0.5378767251968384,\n",
              "   -0.18932314217090607,\n",
              "   -0.6175630688667297,\n",
              "   -0.46692565083503723,\n",
              "   -0.1937747746706009,\n",
              "   -0.14956820011138916,\n",
              "   0.5020917057991028,\n",
              "   -0.05263787880539894,\n",
              "   -0.28018712997436523,\n",
              "   0.012995942495763302,\n",
              "   -0.32456666231155396,\n",
              "   0.4003694951534271,\n",
              "   -0.40532898902893066,\n",
              "   0.2950291931629181,\n",
              "   -0.49430739879608154,\n",
              "   -0.5423142910003662,\n",
              "   0.22408755123615265,\n",
              "   0.11603926867246628,\n",
              "   -0.568913459777832,\n",
              "   -0.018668074160814285,\n",
              "   0.38807424902915955,\n",
              "   -0.936181366443634,\n",
              "   -0.10599885880947113,\n",
              "   0.05938093364238739,\n",
              "   -0.29674050211906433,\n",
              "   0.004020668566226959,\n",
              "   -0.20537954568862915,\n",
              "   0.01671239361166954,\n",
              "   -0.15541023015975952,\n",
              "   0.39719465374946594,\n",
              "   0.185127854347229,\n",
              "   0.14933185279369354,\n",
              "   0.05757788196206093,\n",
              "   -0.2876572906970978,\n",
              "   0.008051411248743534,\n",
              "   0.027041839435696602,\n",
              "   -0.005633254069834948,\n",
              "   0.18006901443004608,\n",
              "   -0.2556428909301758,\n",
              "   0.30624914169311523,\n",
              "   0.3629645109176636,\n",
              "   -0.16387374699115753,\n",
              "   -0.41335275769233704,\n",
              "   -0.47407084703445435,\n",
              "   0.10429807752370834,\n",
              "   -1.0614463090896606,\n",
              "   -0.17266230285167694,\n",
              "   0.5136646628379822,\n",
              "   -0.2619131803512573,\n",
              "   -0.1954173743724823,\n",
              "   -0.4108009338378906,\n",
              "   -0.4458812475204468,\n",
              "   -0.5067088007926941,\n",
              "   -0.11363225430250168,\n",
              "   0.20825596153736115,\n",
              "   0.45370644330978394,\n",
              "   0.060830019414424896,\n",
              "   0.4465949535369873,\n",
              "   -0.2133568376302719,\n",
              "   -0.10864049196243286,\n",
              "   0.3098314702510834,\n",
              "   -0.25895825028419495,\n",
              "   -0.08539588749408722,\n",
              "   -0.2889077365398407,\n",
              "   -0.23519128561019897,\n",
              "   0.26675936579704285,\n",
              "   0.04000916704535484,\n",
              "   -0.24498939514160156,\n",
              "   0.5467664003372192,\n",
              "   -0.6734578609466553,\n",
              "   0.052415866404771805,\n",
              "   0.014709978364408016,\n",
              "   0.735084593296051,\n",
              "   -0.022676609456539154,\n",
              "   0.6968939900398254,\n",
              "   -0.3963638246059418,\n",
              "   0.18063288927078247,\n",
              "   0.30110716819763184,\n",
              "   -0.16026686131954193,\n",
              "   0.1160908117890358,\n",
              "   0.5715014934539795,\n",
              "   0.5019261240959167,\n",
              "   0.19729141891002655,\n",
              "   -0.05939493700861931,\n",
              "   -0.018326198682188988,\n",
              "   0.44175106287002563,\n",
              "   1.2116262912750244,\n",
              "   -0.21756374835968018,\n",
              "   -0.2628367841243744,\n",
              "   -0.014537855982780457,\n",
              "   -0.2428494691848755,\n",
              "   0.047952715307474136,\n",
              "   0.2901979982852936,\n",
              "   0.5726801753044128,\n",
              "   -0.06537190824747086,\n",
              "   0.3040161728858948,\n",
              "   0.24427533149719238,\n",
              "   -0.4823308289051056,\n",
              "   0.4452595114707947,\n",
              "   0.2958153784275055,\n",
              "   0.1765884906053543,\n",
              "   -0.4986928701400757,\n",
              "   0.5181301236152649,\n",
              "   0.735266387462616,\n",
              "   -1.1309131383895874,\n",
              "   -0.3840613067150116,\n",
              "   0.1550079584121704,\n",
              "   -0.1510510891675949,\n",
              "   0.3608417510986328,\n",
              "   0.0656336322426796,\n",
              "   0.345674991607666,\n",
              "   0.198468878865242,\n",
              "   0.22763635218143463,\n",
              "   0.2470051497220993,\n",
              "   0.23642154037952423,\n",
              "   0.7152199745178223,\n",
              "   -0.041901230812072754,\n",
              "   0.07639237493276596,\n",
              "   0.07780684530735016,\n",
              "   0.4266577661037445,\n",
              "   -0.26736021041870117,\n",
              "   0.5906212329864502,\n",
              "   -0.44363921880722046,\n",
              "   -0.03898126631975174,\n",
              "   -0.19994136691093445,\n",
              "   -0.11708720028400421,\n",
              "   -0.5476464033126831,\n",
              "   0.40558692812919617,\n",
              "   0.5102976560592651,\n",
              "   -0.7016052603721619,\n",
              "   0.5713555812835693,\n",
              "   0.42322662472724915,\n",
              "   0.8897120356559753,\n",
              "   1.1133767366409302,\n",
              "   0.6145327091217041,\n",
              "   0.3090360462665558,\n",
              "   0.12581086158752441,\n",
              "   -0.1406499296426773,\n",
              "   0.6489725708961487,\n",
              "   0.3011642396450043,\n",
              "   0.023314576596021652,\n",
              "   0.48735544085502625,\n",
              "   -0.3661530613899231,\n",
              "   -0.046598006039857864,\n",
              "   0.19301193952560425,\n",
              "   0.4896162152290344,\n",
              "   -0.29504474997520447,\n",
              "   0.45352494716644287,\n",
              "   -0.5274980068206787,\n",
              "   0.6943797469139099,\n",
              "   0.655493438243866,\n",
              "   0.30747324228286743,\n",
              "   0.32549065351486206,\n",
              "   -0.8065140247344971,\n",
              "   0.40447482466697693,\n",
              "   0.35125428438186646,\n",
              "   0.2972058951854706,\n",
              "   -0.31371888518333435,\n",
              "   -0.027178233489394188,\n",
              "   -0.0585593618452549,\n",
              "   0.14376704394817352,\n",
              "   0.10597794502973557,\n",
              "   0.39224913716316223,\n",
              "   -0.5466863512992859,\n",
              "   -0.4395632743835449,\n",
              "   -0.022378718480467796,\n",
              "   -0.29356276988983154,\n",
              "   -0.5430237650871277,\n",
              "   0.07031416893005371,\n",
              "   0.4255985617637634,\n",
              "   -0.19828654825687408,\n",
              "   -0.16254432499408722,\n",
              "   -0.3310619592666626,\n",
              "   -0.3379283845424652,\n",
              "   -0.3446587920188904,\n",
              "   0.09334952384233475,\n",
              "   0.19034743309020996,\n",
              "   0.06100668013095856,\n",
              "   -0.22282248735427856,\n",
              "   0.349692702293396,\n",
              "   -0.047521017491817474,\n",
              "   -0.2516234517097473,\n",
              "   -0.48220115900039673,\n",
              "   -0.8326680064201355,\n",
              "   0.057095881551504135,\n",
              "   -0.35781142115592957,\n",
              "   -0.16456885635852814,\n",
              "   0.48294976353645325,\n",
              "   0.5996198654174805,\n",
              "   -0.04944271221756935,\n",
              "   -0.021856345236301422,\n",
              "   -0.6416512131690979,\n",
              "   -0.1227349117398262,\n",
              "   0.09375932067632675,\n",
              "   -0.2964693605899811,\n",
              "   -0.14332900941371918,\n",
              "   -0.5130800604820251,\n",
              "   0.16409079730510712,\n",
              "   -0.5142018795013428,\n",
              "   -0.1449495106935501,\n",
              "   -0.012087036855518818,\n",
              "   0.4028061032295227,\n",
              "   -0.42504867911338806,\n",
              "   0.46296268701553345,\n",
              "   -0.010473832488059998,\n",
              "   0.06221102550625801,\n",
              "   0.29011863470077515,\n",
              "   -0.46403059363365173,\n",
              "   -0.173700749874115,\n",
              "   -0.03886670246720314,\n",
              "   0.6201503872871399,\n",
              "   -0.4298233091831207,\n",
              "   -0.31763961911201477,\n",
              "   0.10613851994276047,\n",
              "   -0.008710495196282864,\n",
              "   -1.2643464803695679,\n",
              "   -0.11668172478675842,\n",
              "   0.21385100483894348,\n",
              "   -0.5942909717559814,\n",
              "   -0.32306867837905884,\n",
              "   -0.2536655068397522,\n",
              "   0.3809938132762909,\n",
              "   0.20046623051166534,\n",
              "   -0.774588406085968,\n",
              "   0.2633326053619385,\n",
              "   -0.18171821534633636,\n",
              "   -0.3130056858062744,\n",
              "   0.26533043384552,\n",
              "   -0.11808644235134125,\n",
              "   -0.7036102414131165,\n",
              "   1.0348531007766724,\n",
              "   0.019630923867225647,\n",
              "   -0.32053592801094055,\n",
              "   0.2640025317668915,\n",
              "   0.5325737595558167,\n",
              "   -0.6883054375648499,\n",
              "   -0.48538294434547424,\n",
              "   -0.45679911971092224,\n",
              "   -0.02202577143907547,\n",
              "   -0.26385989785194397,\n",
              "   0.7046661376953125,\n",
              "   -0.2175767868757248,\n",
              "   0.06321947276592255,\n",
              "   -0.4739477336406708,\n",
              "   0.08719861507415771,\n",
              "   0.34013745188713074,\n",
              "   -0.31850945949554443,\n",
              "   -0.350249707698822,\n",
              "   0.2091936469078064,\n",
              "   -0.4400755763053894],\n",
              "  [0.33173173666000366,\n",
              "   -0.14760203659534454,\n",
              "   0.29173359274864197,\n",
              "   0.015208086930215359,\n",
              "   0.31718263030052185,\n",
              "   -0.4323523938655853,\n",
              "   0.04815678298473358,\n",
              "   0.7011218070983887,\n",
              "   -0.3249628245830536,\n",
              "   0.024659818038344383,\n",
              "   0.3044085204601288,\n",
              "   -0.4290601313114166,\n",
              "   -0.28853029012680054,\n",
              "   0.6533055901527405,\n",
              "   -0.22860491275787354,\n",
              "   0.01612475886940956,\n",
              "   0.4535825848579407,\n",
              "   -0.41308847069740295,\n",
              "   -0.17201140522956848,\n",
              "   -0.048113156110048294,\n",
              "   0.1480933129787445,\n",
              "   -0.06093817949295044,\n",
              "   -0.43916818499565125,\n",
              "   0.2721720337867737,\n",
              "   0.15002287924289703,\n",
              "   -0.006484067998826504,\n",
              "   -0.40131816267967224,\n",
              "   -0.1810033768415451,\n",
              "   -0.43266016244888306,\n",
              "   -0.40442630648612976,\n",
              "   0.5849907994270325,\n",
              "   -0.4595262408256531,\n",
              "   -0.35613760352134705,\n",
              "   0.1882714182138443,\n",
              "   -0.16734746098518372,\n",
              "   -0.4936252236366272,\n",
              "   0.14929942786693573,\n",
              "   0.030356159433722496,\n",
              "   -0.45203131437301636,\n",
              "   0.31232044100761414,\n",
              "   0.375598281621933,\n",
              "   -0.45176270604133606,\n",
              "   0.1662498116493225,\n",
              "   -0.0825735479593277,\n",
              "   0.22494791448116302,\n",
              "   0.12973619997501373,\n",
              "   1.4259059429168701,\n",
              "   0.1308402419090271,\n",
              "   -0.3146001994609833,\n",
              "   -0.08615894615650177,\n",
              "   -0.40933650732040405,\n",
              "   0.16521626710891724,\n",
              "   0.08173199743032455,\n",
              "   0.05931772291660309,\n",
              "   -0.040255382657051086,\n",
              "   0.6871633529663086,\n",
              "   0.43354716897010803,\n",
              "   -0.2935802638530731,\n",
              "   -0.653650164604187,\n",
              "   0.05173501744866371,\n",
              "   0.27610141038894653,\n",
              "   -0.5079454779624939,\n",
              "   -0.2387341856956482,\n",
              "   -0.3495887517929077,\n",
              "   -0.19304950535297394,\n",
              "   -0.22722598910331726,\n",
              "   0.45960426330566406,\n",
              "   0.020228633657097816,\n",
              "   -0.312320351600647,\n",
              "   0.147883340716362,\n",
              "   -0.46209055185317993,\n",
              "   -0.16346481442451477,\n",
              "   0.23702475428581238,\n",
              "   0.10676924884319305,\n",
              "   -0.28003332018852234,\n",
              "   0.08265388756990433,\n",
              "   0.2765067219734192,\n",
              "   0.18219561874866486,\n",
              "   0.33149996399879456,\n",
              "   -0.03070586360991001,\n",
              "   0.05436692386865616,\n",
              "   1.008675217628479,\n",
              "   -0.6759341359138489,\n",
              "   0.6779494881629944,\n",
              "   -0.019814616069197655,\n",
              "   -0.11085176467895508,\n",
              "   -0.581321656703949,\n",
              "   -0.3138858675956726,\n",
              "   -0.09891815483570099,\n",
              "   0.8113221526145935,\n",
              "   -0.24499952793121338,\n",
              "   -0.18246617913246155,\n",
              "   0.04358435049653053,\n",
              "   0.25048816204071045,\n",
              "   -0.0856739729642868,\n",
              "   -0.4934263229370117,\n",
              "   -0.2827761173248291,\n",
              "   0.2813851237297058,\n",
              "   0.10523080825805664,\n",
              "   0.6257508993148804,\n",
              "   -0.20916815102100372,\n",
              "   -0.6968843936920166,\n",
              "   0.3917941451072693,\n",
              "   0.49196791648864746,\n",
              "   0.20926682651042938,\n",
              "   0.12332697212696075,\n",
              "   0.25343596935272217,\n",
              "   0.05620009824633598,\n",
              "   0.12259025871753693,\n",
              "   0.25610190629959106,\n",
              "   -0.11309148371219635,\n",
              "   -0.8602526187896729,\n",
              "   0.05205122381448746,\n",
              "   0.23080745339393616,\n",
              "   0.11325667053461075,\n",
              "   0.33329567313194275,\n",
              "   -0.16560231149196625,\n",
              "   0.02842562086880207,\n",
              "   0.24168603122234344,\n",
              "   -0.15787245333194733,\n",
              "   -0.5899460911750793,\n",
              "   0.00494446000084281,\n",
              "   -0.07244206964969635,\n",
              "   0.6896998286247253,\n",
              "   -0.1473250389099121,\n",
              "   0.23858565092086792,\n",
              "   -0.8697162866592407,\n",
              "   -0.2704547047615051,\n",
              "   -0.3353562355041504,\n",
              "   -0.627516508102417,\n",
              "   0.3664330244064331,\n",
              "   1.061427116394043,\n",
              "   0.2339792549610138,\n",
              "   -0.44112756848335266,\n",
              "   -0.46262580156326294,\n",
              "   0.5962968468666077,\n",
              "   0.25643932819366455,\n",
              "   -0.3823232650756836,\n",
              "   -0.3745025098323822,\n",
              "   -0.27848657965660095,\n",
              "   0.163768470287323,\n",
              "   0.02675524167716503,\n",
              "   0.8475709557533264,\n",
              "   0.3130800127983093,\n",
              "   0.14655910432338715,\n",
              "   -0.38947594165802,\n",
              "   -0.1595151424407959,\n",
              "   -0.6319994926452637,\n",
              "   0.03305252268910408,\n",
              "   -0.09828541427850723,\n",
              "   -0.42820730805397034,\n",
              "   0.058319300413131714,\n",
              "   -0.14187422394752502,\n",
              "   -0.19506891071796417,\n",
              "   -0.2782643139362335,\n",
              "   -0.1855010837316513,\n",
              "   -0.387569397687912,\n",
              "   0.011555313132703304,\n",
              "   0.6255831122398376,\n",
              "   0.13028599321842194,\n",
              "   0.501824676990509,\n",
              "   -0.022094272077083588,\n",
              "   -0.3239953815937042,\n",
              "   0.9020143151283264,\n",
              "   -0.13053274154663086,\n",
              "   0.3374537527561188,\n",
              "   0.1187170073390007,\n",
              "   0.8785540461540222,\n",
              "   0.37011221051216125,\n",
              "   0.13187168538570404,\n",
              "   -0.4021732807159424,\n",
              "   0.23742501437664032,\n",
              "   1.227391242980957,\n",
              "   -0.3142973482608795,\n",
              "   -1.0812010765075684,\n",
              "   0.4099591076374054,\n",
              "   0.19382451474666595,\n",
              "   -0.2209664136171341,\n",
              "   -0.053069815039634705,\n",
              "   0.23884348571300507,\n",
              "   -0.5156747102737427,\n",
              "   0.5546403527259827,\n",
              "   -0.03757555037736893,\n",
              "   -0.34084516763687134,\n",
              "   0.6781218647956848,\n",
              "   -0.22475787997245789,\n",
              "   0.6072317957878113,\n",
              "   -0.34414222836494446,\n",
              "   -0.45551180839538574,\n",
              "   -0.08326850086450577,\n",
              "   -0.3619680106639862,\n",
              "   -0.3737993538379669,\n",
              "   -0.7415778040885925,\n",
              "   -0.32935357093811035,\n",
              "   -0.1345464438199997,\n",
              "   -0.7498567700386047,\n",
              "   -0.13282610476016998,\n",
              "   -0.03301605209708214,\n",
              "   -0.0049787769094109535,\n",
              "   0.03839286044239998,\n",
              "   0.25296247005462646,\n",
              "   -0.13657280802726746,\n",
              "   0.2553688883781433,\n",
              "   0.01818370819091797,\n",
              "   -0.17552152276039124,\n",
              "   -0.02949087880551815,\n",
              "   0.23832350969314575,\n",
              "   0.006882447749376297,\n",
              "   -0.22608521580696106,\n",
              "   0.4671604335308075,\n",
              "   -0.8548495173454285,\n",
              "   0.6825330853462219,\n",
              "   -0.37579742074012756,\n",
              "   0.6965553164482117,\n",
              "   0.06906916946172714,\n",
              "   -0.28695785999298096,\n",
              "   0.49625301361083984,\n",
              "   0.5295522809028625,\n",
              "   -0.24439169466495514,\n",
              "   0.22830452024936676,\n",
              "   -0.028975069522857666,\n",
              "   -0.17765316367149353,\n",
              "   -0.3625630736351013,\n",
              "   0.6872929930686951,\n",
              "   -0.509912371635437,\n",
              "   0.24987909197807312,\n",
              "   0.3443549573421478,\n",
              "   -0.7010414600372314,\n",
              "   0.058739982545375824,\n",
              "   0.5633981227874756,\n",
              "   0.30440521240234375,\n",
              "   -0.4335272014141083,\n",
              "   0.6160148978233337,\n",
              "   0.04295223206281662,\n",
              "   -0.10332424193620682,\n",
              "   0.5279539823532104,\n",
              "   -0.02021314948797226,\n",
              "   -0.030484328046441078,\n",
              "   -0.32307299971580505,\n",
              "   -0.3058355748653412,\n",
              "   -0.3768008053302765,\n",
              "   -0.05724681541323662,\n",
              "   1.2927470207214355,\n",
              "   0.07790783792734146,\n",
              "   0.596814751625061,\n",
              "   0.6075121760368347,\n",
              "   0.41838526725769043,\n",
              "   0.07469666004180908,\n",
              "   -0.14738668501377106,\n",
              "   -0.4320341646671295,\n",
              "   0.018611885607242584,\n",
              "   -0.4579840898513794,\n",
              "   0.05269200727343559,\n",
              "   -0.7793022990226746,\n",
              "   -0.5357867479324341,\n",
              "   -0.7818381786346436,\n",
              "   -0.32162946462631226,\n",
              "   -0.5658084154129028,\n",
              "   0.07102182507514954,\n",
              "   0.5780715346336365,\n",
              "   0.0031314713414758444,\n",
              "   0.014682183973491192,\n",
              "   0.46147939562797546,\n",
              "   -0.2851790487766266,\n",
              "   -0.3560575246810913,\n",
              "   -0.6382986903190613,\n",
              "   0.7701606154441833,\n",
              "   0.4225385785102844,\n",
              "   -0.10699255764484406,\n",
              "   0.12908661365509033,\n",
              "   0.22708362340927124,\n",
              "   -0.11591614782810211,\n",
              "   -0.03762826696038246,\n",
              "   0.9827033877372742,\n",
              "   -0.5540289878845215,\n",
              "   -0.5099261403083801,\n",
              "   0.1356753408908844,\n",
              "   0.23219579458236694,\n",
              "   -0.12964418530464172,\n",
              "   -0.2934628427028656,\n",
              "   0.4668436646461487,\n",
              "   0.6657198667526245,\n",
              "   -0.4352174997329712,\n",
              "   0.20016975700855255,\n",
              "   0.00818023644387722,\n",
              "   -0.3490239679813385,\n",
              "   0.0958113819360733,\n",
              "   -0.3500332832336426,\n",
              "   -0.42011499404907227,\n",
              "   -0.46426352858543396,\n",
              "   -0.20012398064136505,\n",
              "   -0.03175950422883034,\n",
              "   -0.30329445004463196,\n",
              "   0.07934339344501495,\n",
              "   0.7642289996147156,\n",
              "   0.13091108202934265,\n",
              "   0.19259978830814362,\n",
              "   -0.22278831899166107,\n",
              "   -0.03625119477510452,\n",
              "   -0.3453829288482666,\n",
              "   -0.39649826288223267,\n",
              "   0.018105877563357353,\n",
              "   0.40673619508743286,\n",
              "   -0.08965825289487839,\n",
              "   0.19014057517051697,\n",
              "   0.0440470390021801,\n",
              "   0.09254443645477295,\n",
              "   -0.22984164953231812,\n",
              "   -3.9784622192382812,\n",
              "   0.05238714441657066,\n",
              "   -0.4415937662124634,\n",
              "   -0.35640332102775574,\n",
              "   0.1897292286157608,\n",
              "   -0.32993489503860474,\n",
              "   -0.05043761804699898,\n",
              "   -0.11645466089248657,\n",
              "   -1.08527410030365,\n",
              "   -0.3536229729652405,\n",
              "   -0.289504736661911,\n",
              "   0.06927580386400223,\n",
              "   0.7523029446601868,\n",
              "   0.9378072619438171,\n",
              "   0.618334174156189,\n",
              "   -0.6157339811325073,\n",
              "   0.001790207577869296,\n",
              "   -0.1609472632408142,\n",
              "   -0.43947604298591614,\n",
              "   0.7440303564071655,\n",
              "   -0.15303315222263336,\n",
              "   -0.42647305130958557,\n",
              "   0.38953742384910583,\n",
              "   -0.001091061974875629,\n",
              "   1.082220196723938,\n",
              "   0.3510820269584656,\n",
              "   0.43457844853401184,\n",
              "   0.3341938853263855,\n",
              "   -0.17966873943805695,\n",
              "   -0.35851070284843445,\n",
              "   0.015608918853104115,\n",
              "   -0.44734737277030945,\n",
              "   0.12885548174381256,\n",
              "   0.44200390577316284,\n",
              "   0.3394224941730499,\n",
              "   0.37302371859550476,\n",
              "   -0.15228694677352905,\n",
              "   -0.6959240436553955,\n",
              "   0.6788187623023987,\n",
              "   0.5361433625221252,\n",
              "   0.1950119435787201,\n",
              "   -0.5458523631095886,\n",
              "   -0.05354025959968567,\n",
              "   0.49630457162857056,\n",
              "   1.0376895666122437,\n",
              "   -0.41963517665863037,\n",
              "   0.030685456469655037,\n",
              "   -0.6197599768638611,\n",
              "   0.2689265012741089,\n",
              "   -0.04297410696744919,\n",
              "   1.0111900568008423,\n",
              "   -0.03717445582151413,\n",
              "   -0.07365183532238007,\n",
              "   -0.5197331309318542,\n",
              "   -0.4141469895839691,\n",
              "   -0.48942747712135315,\n",
              "   0.2811431884765625,\n",
              "   0.9974080920219421,\n",
              "   -0.4249056875705719,\n",
              "   -0.6288694143295288,\n",
              "   0.31393441557884216,\n",
              "   -0.48524603247642517,\n",
              "   -0.49793902039527893,\n",
              "   0.06475387513637543,\n",
              "   -0.7388423085212708,\n",
              "   -0.006390594877302647,\n",
              "   -0.01901940442621708,\n",
              "   -0.40033799409866333,\n",
              "   0.16666550934314728,\n",
              "   0.41154471039772034,\n",
              "   0.30484557151794434,\n",
              "   0.5964617729187012,\n",
              "   -0.6261074542999268,\n",
              "   -0.9625838398933411,\n",
              "   -0.0076871044002473354,\n",
              "   0.8915247917175293,\n",
              "   -0.5902165770530701,\n",
              "   -0.2560214698314667,\n",
              "   0.09073973447084427,\n",
              "   0.1035231426358223,\n",
              "   -0.29703015089035034,\n",
              "   -0.7842533588409424,\n",
              "   -0.3975295424461365,\n",
              "   0.058588992804288864,\n",
              "   0.19716256856918335,\n",
              "   -0.832649290561676,\n",
              "   0.0098051642999053,\n",
              "   -1.1349248886108398,\n",
              "   0.14066338539123535,\n",
              "   0.1578439325094223,\n",
              "   -0.2917722761631012,\n",
              "   0.09527568519115448,\n",
              "   0.2768605649471283,\n",
              "   0.048504751175642014,\n",
              "   1.0581377744674683,\n",
              "   -0.30043965578079224,\n",
              "   -0.0060301790945231915,\n",
              "   -0.3371065855026245,\n",
              "   0.24733611941337585,\n",
              "   -0.5804908275604248,\n",
              "   0.29689455032348633,\n",
              "   -0.11196746677160263,\n",
              "   0.25409454107284546,\n",
              "   -0.09028103202581406,\n",
              "   0.05040104314684868,\n",
              "   -0.14374437928199768,\n",
              "   -0.43216943740844727,\n",
              "   0.270818293094635,\n",
              "   -0.3556787073612213,\n",
              "   0.4198857545852661,\n",
              "   0.3311101794242859,\n",
              "   0.32256489992141724,\n",
              "   0.8668006658554077,\n",
              "   0.026825174689292908,\n",
              "   -0.46617332100868225,\n",
              "   -0.4894813001155853,\n",
              "   0.33026912808418274,\n",
              "   0.7788209915161133,\n",
              "   -0.18547557294368744,\n",
              "   -0.16875162720680237,\n",
              "   -0.3002155125141144,\n",
              "   0.5456793308258057,\n",
              "   -0.5007172226905823,\n",
              "   -0.43736088275909424,\n",
              "   -0.3155759572982788,\n",
              "   -0.3080999255180359,\n",
              "   -0.5111978054046631,\n",
              "   -1.1245157718658447,\n",
              "   -0.29898858070373535,\n",
              "   0.17828623950481415,\n",
              "   -0.5061886310577393,\n",
              "   -0.10479725152254105,\n",
              "   0.316876620054245,\n",
              "   0.0029341462068259716,\n",
              "   0.03970431536436081,\n",
              "   -0.4167100489139557,\n",
              "   0.23289456963539124,\n",
              "   -0.8156749606132507,\n",
              "   -0.17707280814647675,\n",
              "   0.8682207465171814,\n",
              "   0.4157792627811432,\n",
              "   0.7208543419837952,\n",
              "   0.2823021709918976,\n",
              "   -0.20127472281455994,\n",
              "   -0.5144235491752625,\n",
              "   1.0326666831970215,\n",
              "   -0.35384881496429443,\n",
              "   0.407099187374115,\n",
              "   -0.4067366421222687,\n",
              "   0.9560574889183044,\n",
              "   -0.1395987719297409,\n",
              "   -0.4739132523536682,\n",
              "   -0.24718153476715088,\n",
              "   -0.05306442826986313,\n",
              "   0.3961840569972992,\n",
              "   -0.16888172924518585,\n",
              "   0.4156043529510498,\n",
              "   -0.45391038060188293,\n",
              "   0.17491415143013,\n",
              "   -0.2712191045284271,\n",
              "   -0.15832772850990295,\n",
              "   -0.2514224052429199,\n",
              "   0.8203174471855164,\n",
              "   0.5249965190887451,\n",
              "   -0.19367866218090057,\n",
              "   0.4325515627861023,\n",
              "   -0.2914865016937256,\n",
              "   0.1291026622056961,\n",
              "   -0.4116452634334564,\n",
              "   0.2944374084472656,\n",
              "   0.20688948035240173,\n",
              "   -0.028314387425780296,\n",
              "   -0.0845969170331955,\n",
              "   0.3089055120944977,\n",
              "   -0.5985800623893738,\n",
              "   0.40525269508361816,\n",
              "   0.05892027169466019,\n",
              "   -0.5949391722679138,\n",
              "   -0.40056219696998596,\n",
              "   0.40326812863349915,\n",
              "   0.19891177117824554,\n",
              "   -0.44928622245788574,\n",
              "   -0.029454516246914864,\n",
              "   -0.22458136081695557,\n",
              "   1.0067853927612305,\n",
              "   0.32753869891166687,\n",
              "   0.2747199833393097,\n",
              "   0.2655746042728424,\n",
              "   0.21857091784477234,\n",
              "   0.6680418848991394,\n",
              "   0.0292192455381155,\n",
              "   0.2619982957839966,\n",
              "   -0.3468128442764282,\n",
              "   -0.10145934671163559,\n",
              "   0.18195123970508575,\n",
              "   -0.07303103059530258,\n",
              "   0.8493559956550598,\n",
              "   0.0340915322303772,\n",
              "   0.18890497088432312,\n",
              "   0.0762895867228508,\n",
              "   0.0937829241156578,\n",
              "   -0.7042386531829834,\n",
              "   -0.2186652570962906,\n",
              "   -0.23867741227149963,\n",
              "   0.03336011990904808,\n",
              "   -0.15333430469036102,\n",
              "   0.07086202502250671,\n",
              "   0.5349066853523254,\n",
              "   -0.009611108340322971,\n",
              "   0.22379180788993835,\n",
              "   -0.17279674112796783,\n",
              "   -0.49758967757225037,\n",
              "   -0.14359577000141144,\n",
              "   -0.5404969453811646,\n",
              "   -0.4666300117969513,\n",
              "   0.8337980508804321,\n",
              "   0.1791170984506607,\n",
              "   0.02827557362616062,\n",
              "   -0.12623418867588043,\n",
              "   -0.34490686655044556,\n",
              "   -0.08484448492527008,\n",
              "   -0.130991131067276,\n",
              "   -0.12083107978105545,\n",
              "   0.07072485983371735,\n",
              "   -0.777074933052063,\n",
              "   -0.2224069982767105,\n",
              "   -0.3759332597255707,\n",
              "   -0.5397971272468567,\n",
              "   -0.3538210391998291,\n",
              "   0.06356531381607056,\n",
              "   0.5783427357673645,\n",
              "   -0.9137288928031921,\n",
              "   -0.3632884919643402,\n",
              "   -0.05457562208175659,\n",
              "   -0.23553158342838287,\n",
              "   0.2389531284570694,\n",
              "   -0.5007571578025818,\n",
              "   0.16330495476722717,\n",
              "   -0.01313431840389967,\n",
              "   -0.0128197455778718,\n",
              "   -0.528637170791626,\n",
              "   -0.5129384994506836,\n",
              "   -0.5098448395729065,\n",
              "   -0.2913927733898163,\n",
              "   0.1691771000623703,\n",
              "   0.2036757916212082,\n",
              "   -0.4266694486141205,\n",
              "   0.59697026014328,\n",
              "   -0.1509532630443573,\n",
              "   0.4820472002029419,\n",
              "   -0.10822606831789017,\n",
              "   0.19633784890174866,\n",
              "   -0.11340142786502838,\n",
              "   -0.29648882150650024,\n",
              "   -0.21059080958366394,\n",
              "   -0.3279661238193512,\n",
              "   -0.5821835398674011,\n",
              "   0.0067051067017018795,\n",
              "   0.011266620829701424,\n",
              "   -0.38715797662734985,\n",
              "   -0.11694180965423584,\n",
              "   0.2918758690357208,\n",
              "   -0.4111241102218628,\n",
              "   0.1464112102985382,\n",
              "   0.1549316793680191,\n",
              "   0.6975356936454773,\n",
              "   -0.38185617327690125,\n",
              "   0.6227893829345703,\n",
              "   0.6789138913154602,\n",
              "   0.09758318960666656,\n",
              "   -0.2387804090976715,\n",
              "   -0.2561788856983185,\n",
              "   -0.007019608281552792,\n",
              "   0.133222296833992,\n",
              "   -0.27248066663742065,\n",
              "   -0.4384423494338989,\n",
              "   0.36762410402297974,\n",
              "   -0.8532983660697937,\n",
              "   0.39078760147094727,\n",
              "   0.12183629721403122,\n",
              "   -0.14408734440803528,\n",
              "   0.4965750277042389,\n",
              "   0.37508174777030945,\n",
              "   -0.1462477296590805,\n",
              "   -0.05548010766506195,\n",
              "   -0.3636796772480011,\n",
              "   0.01926274783909321,\n",
              "   0.28134316205978394,\n",
              "   0.11427250504493713,\n",
              "   -0.36107420921325684,\n",
              "   0.5793284177780151,\n",
              "   0.5429684519767761,\n",
              "   -0.0578952394425869,\n",
              "   0.18109142780303955,\n",
              "   0.08164014667272568,\n",
              "   0.0920572504401207,\n",
              "   0.3780290186405182,\n",
              "   0.2264794558286667,\n",
              "   -0.307953417301178,\n",
              "   0.009088353253901005,\n",
              "   -0.3540329933166504,\n",
              "   -0.4709234833717346,\n",
              "   0.4234139323234558,\n",
              "   -0.17401044070720673,\n",
              "   -0.09002352505922318,\n",
              "   0.46456894278526306,\n",
              "   0.251594603061676,\n",
              "   -0.15534257888793945,\n",
              "   0.4970821738243103,\n",
              "   0.2894682288169861,\n",
              "   0.3345247805118561,\n",
              "   -0.5831872224807739,\n",
              "   0.3776245415210724,\n",
              "   1.2795000076293945,\n",
              "   -0.6121267080307007,\n",
              "   -0.4490970969200134,\n",
              "   -0.10910488665103912,\n",
              "   -0.12535233795642853,\n",
              "   0.07959973812103271,\n",
              "   0.5160837769508362,\n",
              "   0.43752920627593994,\n",
              "   -0.12753890454769135,\n",
              "   0.5795896053314209,\n",
              "   0.3091965317726135,\n",
              "   0.1546117067337036,\n",
              "   0.11622603982686996,\n",
              "   -0.07423976808786392,\n",
              "   -0.27059492468833923,\n",
              "   -0.05300271138548851,\n",
              "   0.11874070018529892,\n",
              "   -0.19886747002601624,\n",
              "   -0.08480075746774673,\n",
              "   -0.5001444220542908,\n",
              "   0.04868001118302345,\n",
              "   0.42800581455230713,\n",
              "   -0.24240945279598236,\n",
              "   0.540931761264801,\n",
              "   0.12300626933574677,\n",
              "   -0.09483566135168076,\n",
              "   -0.6389992833137512,\n",
              "   0.40319645404815674,\n",
              "   -0.07017314434051514,\n",
              "   0.7735742926597595,\n",
              "   0.6288003325462341,\n",
              "   0.17174676060676575,\n",
              "   0.3068709075450897,\n",
              "   -0.007844429463148117,\n",
              "   -0.5127314925193787,\n",
              "   0.5231634378433228,\n",
              "   0.9851685762405396,\n",
              "   -0.055905696004629135,\n",
              "   0.23605678975582123,\n",
              "   0.043675389140844345,\n",
              "   -0.25503745675086975,\n",
              "   0.3614228367805481,\n",
              "   -0.9750216007232666,\n",
              "   0.32909950613975525,\n",
              "   0.3027456998825073,\n",
              "   -0.5609654784202576,\n",
              "   0.7853285670280457,\n",
              "   0.1896098256111145,\n",
              "   0.22181379795074463,\n",
              "   0.011510645970702171,\n",
              "   -0.640597403049469,\n",
              "   -0.05935871973633766,\n",
              "   0.13593025505542755,\n",
              "   0.5337145328521729,\n",
              "   -0.5345402359962463,\n",
              "   -0.15644843876361847,\n",
              "   -0.13447041809558868,\n",
              "   0.07632678002119064,\n",
              "   0.5966635942459106,\n",
              "   -0.31202468276023865,\n",
              "   -0.19813112914562225,\n",
              "   -0.10682126134634018,\n",
              "   -0.09586774557828903,\n",
              "   -0.4644317626953125,\n",
              "   0.1250220090150833,\n",
              "   -0.23038946092128754,\n",
              "   -0.32623958587646484,\n",
              "   0.3942587971687317,\n",
              "   0.35402771830558777,\n",
              "   -0.5268223285675049,\n",
              "   -0.012002885341644287,\n",
              "   0.3257615268230438,\n",
              "   -0.836827278137207,\n",
              "   0.02859428897500038,\n",
              "   -0.32891416549682617,\n",
              "   0.25867748260498047,\n",
              "   -0.12636300921440125,\n",
              "   -0.010953293181955814,\n",
              "   -0.3205658793449402,\n",
              "   -0.8478938937187195,\n",
              "   -1.099409818649292,\n",
              "   0.5464880466461182,\n",
              "   -0.3006657361984253,\n",
              "   -0.0019155613845214248,\n",
              "   0.051095202565193176,\n",
              "   0.6916923522949219,\n",
              "   0.3993411064147949,\n",
              "   0.3180685341358185,\n",
              "   -0.9138910174369812,\n",
              "   0.3220107853412628,\n",
              "   0.018008394166827202,\n",
              "   -0.36506688594818115,\n",
              "   -0.04793504253029823,\n",
              "   -0.3057238459587097,\n",
              "   -0.01771787740290165,\n",
              "   -0.6262047290802002,\n",
              "   0.15438808500766754,\n",
              "   -0.316223680973053,\n",
              "   0.9266972541809082,\n",
              "   -0.505014955997467,\n",
              "   0.6012329459190369,\n",
              "   0.2436755895614624,\n",
              "   -0.055165305733680725,\n",
              "   0.14493878185749054,\n",
              "   -0.29675760865211487,\n",
              "   0.46639588475227356,\n",
              "   0.06996391713619232,\n",
              "   0.17178206145763397,\n",
              "   -0.14001967012882233,\n",
              "   -0.3514511287212372,\n",
              "   0.5909923911094666,\n",
              "   -0.13572794198989868,\n",
              "   -0.5239260792732239,\n",
              "   -0.3197345435619354,\n",
              "   0.2776731252670288,\n",
              "   -0.01900417171418667,\n",
              "   0.05146299675107002,\n",
              "   -0.16235707700252533,\n",
              "   0.5413675904273987,\n",
              "   -0.39480826258659363,\n",
              "   -0.44149842858314514,\n",
              "   -0.21226055920124054,\n",
              "   0.1696086823940277,\n",
              "   -0.23785561323165894,\n",
              "   -0.3288050889968872,\n",
              "   -0.2579159736633301,\n",
              "   -0.6169050335884094,\n",
              "   0.10137584060430527,\n",
              "   0.11434170603752136,\n",
              "   0.1591065526008606,\n",
              "   -0.37865516543388367,\n",
              "   0.8348303437232971,\n",
              "   -0.47115713357925415,\n",
              "   -0.45602625608444214,\n",
              "   -0.15956740081310272,\n",
              "   0.05728577822446823,\n",
              "   -0.3822483420372009,\n",
              "   0.4461548328399658,\n",
              "   -0.0965173989534378,\n",
              "   0.013883698731660843,\n",
              "   -0.6688232421875,\n",
              "   -0.09363449364900589,\n",
              "   -0.22884038090705872,\n",
              "   0.24785169959068298,\n",
              "   0.07431354373693466,\n",
              "   0.23877115547657013,\n",
              "   -0.20276431739330292],\n",
              "  [0.9434615969657898,\n",
              "   0.11608622223138809,\n",
              "   -0.29942673444747925,\n",
              "   0.4918740689754486,\n",
              "   -0.23300285637378693,\n",
              "   -1.028062105178833,\n",
              "   0.38709795475006104,\n",
              "   -0.2289535105228424,\n",
              "   0.38722649216651917,\n",
              "   -0.015510144643485546,\n",
              "   0.3854888379573822,\n",
              "   -0.09314683824777603,\n",
              "   -0.06949082016944885,\n",
              "   -0.23107796907424927,\n",
              "   -0.6342856884002686,\n",
              "   -0.36117255687713623,\n",
              "   0.1467372477054596,\n",
              "   -0.17403678596019745,\n",
              "   0.3359518051147461,\n",
              "   0.14054548740386963,\n",
              "   0.5556771755218506,\n",
              "   -0.22225823998451233,\n",
              "   0.46925705671310425,\n",
              "   0.36480477452278137,\n",
              "   0.1596003770828247,\n",
              "   0.04045655205845833,\n",
              "   -0.3751662075519562,\n",
              "   -0.16948193311691284,\n",
              "   -0.600649893283844,\n",
              "   -0.5825843214988708,\n",
              "   -0.4953349232673645,\n",
              "   -0.3164559304714203,\n",
              "   -0.395168274641037,\n",
              "   0.2729012072086334,\n",
              "   0.36867639422416687,\n",
              "   -0.1824922412633896,\n",
              "   0.3494209349155426,\n",
              "   -0.01002979651093483,\n",
              "   -0.37189817428588867,\n",
              "   -0.24208763241767883,\n",
              "   -0.4699942171573639,\n",
              "   0.31682369112968445,\n",
              "   -0.2683295011520386,\n",
              "   0.5779463648796082,\n",
              "   -0.1641177535057068,\n",
              "   -0.6103894710540771,\n",
              "   0.5361384153366089,\n",
              "   0.3860775828361511,\n",
              "   -0.2929714322090149,\n",
              "   0.6282212138175964,\n",
              "   0.13434429466724396,\n",
              "   0.41686713695526123,\n",
              "   -0.02857946790754795,\n",
              "   0.13786457479000092,\n",
              "   0.5319806933403015,\n",
              "   0.3949924111366272,\n",
              "   0.15584686398506165,\n",
              "   -0.7281008362770081,\n",
              "   0.13703040778636932,\n",
              "   0.22120985388755798,\n",
              "   0.20061203837394714,\n",
              "   0.628450334072113,\n",
              "   -0.5340333580970764,\n",
              "   -0.4179953932762146,\n",
              "   0.26582762598991394,\n",
              "   0.19125783443450928,\n",
              "   -0.15720927715301514,\n",
              "   -0.36119207739830017,\n",
              "   -0.7756381034851074,\n",
              "   -0.3654681146144867,\n",
              "   -0.3349916636943817,\n",
              "   -0.8045904040336609,\n",
              "   0.6590434312820435,\n",
              "   0.27052974700927734,\n",
              "   0.15052248537540436,\n",
              "   0.6069878935813904,\n",
              "   -0.8004348874092102,\n",
              "   0.646440327167511,\n",
              "   0.2162024974822998,\n",
              "   0.43112924695014954,\n",
              "   0.23116055130958557,\n",
              "   0.31930604577064514,\n",
              "   -0.09496823698282242,\n",
              "   0.27595019340515137,\n",
              "   0.4103211462497711,\n",
              "   -0.23507872223854065,\n",
              "   -0.5718371272087097,\n",
              "   -0.2473241239786148,\n",
              "   -0.24602769315242767,\n",
              "   -0.10023757070302963,\n",
              "   0.27664700150489807,\n",
              "   0.27594658732414246,\n",
              "   0.3489827811717987,\n",
              "   -0.1444338858127594,\n",
              "   0.010481993667781353,\n",
              "   0.3996570110321045,\n",
              "   -0.14953844249248505,\n",
              "   0.0026802215725183487,\n",
              "   -0.1901208907365799,\n",
              "   0.11960143595933914,\n",
              "   0.27167460322380066,\n",
              "   -0.19547276198863983,\n",
              "   -0.025761593133211136,\n",
              "   1.1667073965072632,\n",
              "   0.045123931020498276,\n",
              "   -0.20120161771774292,\n",
              "   0.141847163438797,\n",
              "   0.3213166892528534,\n",
              "   0.4108031988143921,\n",
              "   1.1898128986358643,\n",
              "   0.7528557777404785,\n",
              "   -0.23754310607910156,\n",
              "   0.14640823006629944,\n",
              "   -0.14865455031394958,\n",
              "   -0.31838560104370117,\n",
              "   -0.598573625087738,\n",
              "   0.3460596203804016,\n",
              "   0.024753157049417496,\n",
              "   0.34559166431427,\n",
              "   -0.12334999442100525,\n",
              "   -0.7913084626197815,\n",
              "   -0.825237512588501,\n",
              "   0.43924906849861145,\n",
              "   1.406172752380371,\n",
              "   -0.18835154175758362,\n",
              "   0.0785360112786293,\n",
              "   -0.13369068503379822,\n",
              "   -0.7639080286026001,\n",
              "   -0.23337006568908691,\n",
              "   -0.7978959679603577,\n",
              "   -0.6713739633560181,\n",
              "   0.5018652677536011,\n",
              "   0.5001754760742188,\n",
              "   0.620398759841919,\n",
              "   -0.33315423130989075,\n",
              "   0.32196202874183655,\n",
              "   0.0434257835149765,\n",
              "   0.12860609591007233,\n",
              "   -0.8621176481246948,\n",
              "   0.4154820740222931,\n",
              "   0.023868216201663017,\n",
              "   0.8060859441757202,\n",
              "   0.49676018953323364,\n",
              "   -0.6280070543289185,\n",
              "   0.29963552951812744,\n",
              "   0.6138358116149902,\n",
              "   0.9387604594230652,\n",
              "   -0.1338508427143097,\n",
              "   0.4502495229244232,\n",
              "   -0.26917898654937744,\n",
              "   1.043113112449646,\n",
              "   -0.5042195320129395,\n",
              "   -0.3732367753982544,\n",
              "   -0.14329008758068085,\n",
              "   -0.5022607445716858,\n",
              "   -0.06651703268289566,\n",
              "   -0.4064960777759552,\n",
              "   -0.2875174880027771,\n",
              "   0.684350848197937,\n",
              "   0.9141201972961426,\n",
              "   0.2672283351421356,\n",
              "   0.14939069747924805,\n",
              "   0.03231578320264816,\n",
              "   0.3450184464454651,\n",
              "   -0.5542604327201843,\n",
              "   0.14886482059955597,\n",
              "   -0.5891348123550415,\n",
              "   0.15231993794441223,\n",
              "   0.19462697207927704,\n",
              "   0.4513166844844818,\n",
              "   -0.7416885495185852,\n",
              "   0.013751411810517311,\n",
              "   0.15084004402160645,\n",
              "   0.03181827813386917,\n",
              "   -0.29980626702308655,\n",
              "   0.17149265110492706,\n",
              "   0.03604081645607948,\n",
              "   0.002606454771012068,\n",
              "   -0.028541872277855873,\n",
              "   -0.5379793643951416,\n",
              "   -9.571798324584961,\n",
              "   -0.5343807935714722,\n",
              "   0.09435927122831345,\n",
              "   0.05685781314969063,\n",
              "   0.2577533423900604,\n",
              "   -0.20826293528079987,\n",
              "   -0.416786789894104,\n",
              "   -0.21453861892223358,\n",
              "   -0.4945160746574402,\n",
              "   -0.8611323237419128,\n",
              "   -0.0007982580573298037,\n",
              "   -0.19211171567440033,\n",
              "   -0.49061164259910583,\n",
              "   0.32364556193351746,\n",
              "   0.6615499258041382,\n",
              "   -0.3179338872432709,\n",
              "   -0.12180174887180328,\n",
              "   0.015011753886938095,\n",
              "   -0.4045390188694,\n",
              "   0.07077398896217346,\n",
              "   -0.024289445951581,\n",
              "   -0.11433675140142441,\n",
              "   0.11888504028320312,\n",
              "   0.5292394757270813,\n",
              "   -0.6012688279151917,\n",
              "   -1.5533349514007568,\n",
              "   0.2543472647666931,\n",
              "   -0.26484131813049316,\n",
              "   0.11542816460132599,\n",
              "   0.33077141642570496,\n",
              "   -1.2222650051116943,\n",
              "   0.10956361889839172,\n",
              "   0.08863011747598648,\n",
              "   -0.370561420917511,\n",
              "   -0.07033424079418182,\n",
              "   -0.5390257835388184,\n",
              "   0.18480274081230164,\n",
              "   -0.4345252215862274,\n",
              "   -1.0446919202804565,\n",
              "   -0.3744187653064728,\n",
              "   -0.6172442436218262,\n",
              "   0.005798022262752056,\n",
              "   -0.4602431356906891,\n",
              "   0.08184265345335007,\n",
              "   0.5861850380897522,\n",
              "   -1.4738273620605469,\n",
              "   0.47439855337142944,\n",
              "   0.9363905191421509,\n",
              "   0.17175480723381042,\n",
              "   0.24829137325286865,\n",
              "   -0.002323722466826439,\n",
              "   -0.27864450216293335,\n",
              "   0.7809939384460449,\n",
              "   0.18452681601047516,\n",
              "   -0.326413094997406,\n",
              "   -0.07695101201534271,\n",
              "   -0.05798787251114845,\n",
              "   -0.1170579120516777,\n",
              "   -0.22354361414909363,\n",
              "   -0.6745185852050781,\n",
              "   -0.22924108803272247,\n",
              "   0.21451154351234436,\n",
              "   0.1513432264328003,\n",
              "   -0.4703122675418854,\n",
              "   -0.20693336427211761,\n",
              "   -0.2332572042942047,\n",
              "   -0.02753191627562046,\n",
              "   0.8406788110733032,\n",
              "   0.5794862508773804,\n",
              "   -0.3428148925304413,\n",
              "   -0.22835321724414825,\n",
              "   -0.40193241834640503,\n",
              "   0.6164329051971436,\n",
              "   -0.6962840557098389,\n",
              "   0.5392209887504578,\n",
              "   0.40603193640708923,\n",
              "   -0.38146471977233887,\n",
              "   0.022874755784869194,\n",
              "   1.019443154335022,\n",
              "   -0.4729078412055969,\n",
              "   0.551000714302063,\n",
              "   0.7219224572181702,\n",
              "   0.5844700336456299,\n",
              "   0.06115742027759552,\n",
              "   -0.5134475827217102,\n",
              "   0.4095786213874817,\n",
              "   0.15594513714313507,\n",
              "   0.1218532845377922,\n",
              "   -0.290289968252182,\n",
              "   0.26306799054145813,\n",
              "   0.2950386106967926,\n",
              "   -0.25891926884651184,\n",
              "   -0.13393233716487885,\n",
              "   0.9539369344711304,\n",
              "   -0.23861436545848846,\n",
              "   -0.6019048094749451,\n",
              "   0.3267335295677185,\n",
              "   -0.23567673563957214,\n",
              "   0.24930442869663239,\n",
              "   -0.4183051586151123,\n",
              "   -0.3253767788410187,\n",
              "   0.3249284029006958,\n",
              "   -0.31322842836380005,\n",
              "   0.6034569144248962,\n",
              "   0.577202320098877,\n",
              "   -0.49843674898147583,\n",
              "   -0.7774069309234619,\n",
              "   0.2141912430524826,\n",
              "   0.2868143320083618,\n",
              "   -0.8806480169296265,\n",
              "   -0.14506979286670685,\n",
              "   -0.21105016767978668,\n",
              "   -0.11833030730485916,\n",
              "   0.195879265666008,\n",
              "   -0.12938694655895233,\n",
              "   0.07728366553783417,\n",
              "   0.9541714191436768,\n",
              "   -0.4209287166595459,\n",
              "   -0.34704968333244324,\n",
              "   -0.08857975155115128,\n",
              "   0.366086483001709,\n",
              "   -0.4592417776584625,\n",
              "   0.07845053821802139,\n",
              "   -0.18755018711090088,\n",
              "   -0.45574280619621277,\n",
              "   0.06075601652264595,\n",
              "   0.38136112689971924,\n",
              "   -0.011964593082666397,\n",
              "   1.625912070274353,\n",
              "   -0.10671693831682205,\n",
              "   0.2846162021160126,\n",
              "   -0.7607277631759644,\n",
              "   0.236781045794487,\n",
              "   0.09028053283691406,\n",
              "   -0.11060436069965363,\n",
              "   -0.25999918580055237,\n",
              "   -0.34510338306427,\n",
              "   -0.2583491802215576,\n",
              "   0.08483642339706421,\n",
              "   -0.5527199506759644,\n",
              "   0.41548117995262146,\n",
              "   0.36615094542503357,\n",
              "   0.21164441108703613,\n",
              "   0.5960125923156738,\n",
              "   0.043756622821092606,\n",
              "   -0.3090302348136902,\n",
              "   -0.5931174755096436,\n",
              "   -0.4863985478878021,\n",
              "   -0.05024627968668938,\n",
              "   0.12378151714801788,\n",
              "   0.3010476529598236,\n",
              "   -0.1255340725183487,\n",
              "   0.9057583212852478,\n",
              "   -0.3519026041030884,\n",
              "   -0.6149167418479919,\n",
              "   0.03765852004289627,\n",
              "   0.24746619164943695,\n",
              "   0.21251048147678375,\n",
              "   -0.2514743506908417,\n",
              "   -0.6503592729568481,\n",
              "   -0.45407605171203613,\n",
              "   -0.29423272609710693,\n",
              "   0.2978810667991638,\n",
              "   -0.030087925493717194,\n",
              "   -0.1716989129781723,\n",
              "   0.3424179255962372,\n",
              "   -0.0020304620265960693,\n",
              "   -0.34140777587890625,\n",
              "   0.06483328342437744,\n",
              "   0.3915337026119232,\n",
              "   -0.18084393441677094,\n",
              "   -0.32524049282073975,\n",
              "   -0.20474602282047272,\n",
              "   -0.06705507636070251,\n",
              "   -0.4213041067123413,\n",
              "   -0.5217095613479614,\n",
              "   0.008899507112801075,\n",
              "   0.4850391745567322,\n",
              "   0.5597350597381592,\n",
              "   -0.0021134987473487854,\n",
              "   -0.34707561135292053,\n",
              "   -0.06952650099992752,\n",
              "   -0.663470983505249,\n",
              "   -0.7499743103981018,\n",
              "   0.25632503628730774,\n",
              "   0.39451539516448975,\n",
              "   -0.5548381209373474,\n",
              "   -0.48661068081855774,\n",
              "   0.30119597911834717,\n",
              "   -0.41169339418411255,\n",
              "   0.43410128355026245,\n",
              "   0.4095905125141144,\n",
              "   -0.1881251484155655,\n",
              "   -0.056980062276124954,\n",
              "   0.10282349586486816,\n",
              "   0.2939620018005371,\n",
              "   -0.3227461278438568,\n",
              "   0.1750410944223404,\n",
              "   -0.07179808616638184,\n",
              "   -0.27429085969924927,\n",
              "   -0.6904009580612183,\n",
              "   -0.7261757254600525,\n",
              "   -0.46144676208496094,\n",
              "   0.2304198145866394,\n",
              "   -0.4060913324356079,\n",
              "   0.18808513879776,\n",
              "   0.7372357249259949,\n",
              "   0.1900641769170761,\n",
              "   0.5102964043617249,\n",
              "   0.10329300910234451,\n",
              "   0.18772394955158234,\n",
              "   0.1935284286737442,\n",
              "   -0.23655828833580017,\n",
              "   -0.40549415349960327,\n",
              "   -0.19685639441013336,\n",
              "   -0.016628772020339966,\n",
              "   0.20816592872142792,\n",
              "   -0.5790663957595825,\n",
              "   0.4234503507614136,\n",
              "   0.3555288016796112,\n",
              "   0.041477102786302567,\n",
              "   0.30806663632392883,\n",
              "   0.06514805555343628,\n",
              "   0.3415980637073517,\n",
              "   0.22645261883735657,\n",
              "   0.02008921653032303,\n",
              "   0.2632140517234802,\n",
              "   0.2039424329996109,\n",
              "   -0.435621440410614,\n",
              "   -0.27575212717056274,\n",
              "   0.837847113609314,\n",
              "   -0.05107361450791359,\n",
              "   -0.01325664110481739,\n",
              "   0.7378951907157898,\n",
              "   -0.6276900768280029,\n",
              "   -0.373607337474823,\n",
              "   -0.5135191082954407,\n",
              "   -0.3096177875995636,\n",
              "   -0.16477137804031372,\n",
              "   -0.5634745359420776,\n",
              "   0.5713385343551636,\n",
              "   -0.007278511766344309,\n",
              "   -0.1556060016155243,\n",
              "   -0.5053883790969849,\n",
              "   0.3319132924079895,\n",
              "   0.8506295680999756,\n",
              "   -0.0242368932813406,\n",
              "   0.4940657913684845,\n",
              "   0.19848129153251648,\n",
              "   0.31044602394104004,\n",
              "   0.40083998441696167,\n",
              "   0.3765406906604767,\n",
              "   -0.09493811428546906,\n",
              "   -0.1257002353668213,\n",
              "   0.26671653985977173,\n",
              "   -0.40405476093292236,\n",
              "   0.32663777470588684,\n",
              "   -0.3085141181945801,\n",
              "   0.15955965220928192,\n",
              "   0.10990293323993683,\n",
              "   -0.133503258228302,\n",
              "   -0.3349515199661255,\n",
              "   -0.2697484791278839,\n",
              "   0.3694877624511719,\n",
              "   0.5141761302947998,\n",
              "   0.6628126502037048,\n",
              "   0.5387817621231079,\n",
              "   0.6384678483009338,\n",
              "   0.439546674489975,\n",
              "   0.3453759551048279,\n",
              "   -0.8598013520240784,\n",
              "   -0.23287662863731384,\n",
              "   0.008977307006716728,\n",
              "   0.8048456907272339,\n",
              "   -0.408630907535553,\n",
              "   0.25981542468070984,\n",
              "   0.42759644985198975,\n",
              "   0.07758557051420212,\n",
              "   0.015398215502500534,\n",
              "   0.37835124135017395,\n",
              "   0.03593839704990387,\n",
              "   -1.0971492528915405,\n",
              "   0.3328554928302765,\n",
              "   -0.3034064471721649,\n",
              "   0.12528198957443237,\n",
              "   -0.023416878655552864,\n",
              "   -0.7328168749809265,\n",
              "   0.2383158653974533,\n",
              "   -0.6166571974754333,\n",
              "   -0.06922892481088638,\n",
              "   0.4893501102924347,\n",
              "   0.041154954582452774,\n",
              "   -0.825276255607605,\n",
              "   -0.7746365666389465,\n",
              "   0.22911286354064941,\n",
              "   -0.7444508671760559,\n",
              "   -0.3094765245914459,\n",
              "   0.35576727986335754,\n",
              "   0.40161237120628357,\n",
              "   -0.08795130252838135,\n",
              "   0.49738866090774536,\n",
              "   -0.4104515314102173,\n",
              "   -0.5392853617668152,\n",
              "   0.6669891476631165,\n",
              "   -0.05615362897515297,\n",
              "   -0.4386866092681885,\n",
              "   -0.2695087790489197,\n",
              "   0.0367889441549778,\n",
              "   0.04892119765281677,\n",
              "   -1.0758839845657349,\n",
              "   -0.1425744891166687,\n",
              "   0.0008810093277134001,\n",
              "   0.23383386433124542,\n",
              "   0.2881501317024231,\n",
              "   0.31030264496803284,\n",
              "   -0.7848044633865356,\n",
              "   0.5241736173629761,\n",
              "   0.007330933585762978,\n",
              "   0.3675420880317688,\n",
              "   -0.2893823981285095,\n",
              "   -0.7147198915481567,\n",
              "   0.20595599710941315,\n",
              "   -0.5120553970336914,\n",
              "   0.3079491853713989,\n",
              "   0.5539975762367249,\n",
              "   -0.031261853873729706,\n",
              "   0.6329977512359619,\n",
              "   0.05597217008471489,\n",
              "   0.5608533620834351,\n",
              "   0.1893714964389801,\n",
              "   -0.4802718758583069,\n",
              "   0.22806045413017273,\n",
              "   -0.23796409368515015,\n",
              "   -0.2729688286781311,\n",
              "   0.3123747408390045,\n",
              "   0.03843270242214203,\n",
              "   -0.3909648656845093,\n",
              "   -0.19770552217960358,\n",
              "   -0.1617526113986969,\n",
              "   -0.5260769128799438,\n",
              "   -0.4001326262950897,\n",
              "   -0.2265699803829193,\n",
              "   0.527247428894043,\n",
              "   0.4058172106742859,\n",
              "   0.35537904500961304,\n",
              "   0.17283819615840912,\n",
              "   1.1233525276184082,\n",
              "   -0.3269866108894348,\n",
              "   -0.9269784092903137,\n",
              "   -0.1264100968837738,\n",
              "   -0.06048940122127533,\n",
              "   0.11211378127336502,\n",
              "   -0.18099665641784668,\n",
              "   -0.9106782078742981,\n",
              "   -0.6280978918075562,\n",
              "   0.4768413305282593,\n",
              "   -0.43030494451522827,\n",
              "   0.032913099974393845,\n",
              "   0.1946091204881668,\n",
              "   0.7703198790550232,\n",
              "   -0.4024624526500702,\n",
              "   0.0489090196788311,\n",
              "   0.6535007357597351,\n",
              "   0.14393094182014465,\n",
              "   -0.3538419008255005,\n",
              "   0.02600129507482052,\n",
              "   0.12120932340621948,\n",
              "   -0.1563498079776764,\n",
              "   -0.3809041380882263,\n",
              "   -0.3303571343421936,\n",
              "   -0.08562850207090378,\n",
              "   -0.289617121219635,\n",
              "   0.27275171875953674,\n",
              "   -0.24496908485889435,\n",
              "   0.0971851721405983,\n",
              "   1.471604585647583,\n",
              "   -0.027165820822119713,\n",
              "   -0.843975305557251,\n",
              "   0.03896290808916092,\n",
              "   -0.1470530927181244,\n",
              "   0.317916601896286,\n",
              "   -0.061515845358371735,\n",
              "   -0.5719586610794067,\n",
              "   -0.4095675051212311,\n",
              "   -0.5289144515991211,\n",
              "   0.2641780972480774,\n",
              "   -0.057756662368774414,\n",
              "   -0.46082136034965515,\n",
              "   0.12524712085723877,\n",
              "   -0.07886654883623123,\n",
              "   0.8503167629241943,\n",
              "   -0.5728546380996704,\n",
              "   0.4814435541629791,\n",
              "   0.23073549568653107,\n",
              "   -0.25200164318084717,\n",
              "   0.29865795373916626,\n",
              "   0.16282159090042114,\n",
              "   0.08258073031902313,\n",
              "   0.06217990443110466,\n",
              "   0.6161581873893738,\n",
              "   -0.2347383052110672,\n",
              "   0.21739062666893005,\n",
              "   -0.8759064078330994,\n",
              "   0.08077739924192429,\n",
              "   0.21331702172756195,\n",
              "   -0.7242211103439331,\n",
              "   -0.49223756790161133,\n",
              "   -0.3120953142642975,\n",
              "   -0.1331915259361267,\n",
              "   0.22680158913135529,\n",
              "   -0.2754589319229126,\n",
              "   -0.12353558838367462,\n",
              "   -0.08595062047243118,\n",
              "   -0.37079304456710815,\n",
              "   0.2030828297138214,\n",
              "   0.44463321566581726,\n",
              "   -0.051584623754024506,\n",
              "   -0.38129034638404846,\n",
              "   0.7636991739273071,\n",
              "   0.3446892499923706,\n",
              "   -0.39712628722190857,\n",
              "   -0.014221527613699436,\n",
              "   -0.04470653086900711,\n",
              "   0.18045668303966522,\n",
              "   -0.11772298067808151,\n",
              "   0.30239295959472656,\n",
              "   0.09713753312826157,\n",
              "   0.1155642420053482,\n",
              "   -0.23912404477596283,\n",
              "   0.8631505966186523,\n",
              "   0.0022742736618965864,\n",
              "   0.10466907173395157,\n",
              "   0.07109777629375458,\n",
              "   0.1705252081155777,\n",
              "   0.17284712195396423,\n",
              "   0.17106413841247559,\n",
              "   0.19367504119873047,\n",
              "   0.35904303193092346,\n",
              "   -0.7170839309692383,\n",
              "   0.30714768171310425,\n",
              "   0.236972838640213,\n",
              "   0.41629210114479065,\n",
              "   -1.4417518377304077,\n",
              "   0.04708487540483475,\n",
              "   -0.1532144397497177,\n",
              "   -0.48209646344184875,\n",
              "   -0.06609191745519638,\n",
              "   0.8310714960098267,\n",
              "   0.24020032584667206,\n",
              "   0.006900395732372999,\n",
              "   0.21457736194133759,\n",
              "   -0.26880180835723877,\n",
              "   0.06233251094818115,\n",
              "   0.1602770984172821,\n",
              "   -0.673809289932251,\n",
              "   -0.1604238748550415,\n",
              "   -0.35625335574150085,\n",
              "   0.12297654151916504,\n",
              "   0.45561614632606506,\n",
              "   0.6740416884422302,\n",
              "   -0.18974344432353973,\n",
              "   0.4119674563407898,\n",
              "   0.22930847108364105,\n",
              "   -0.24322471022605896,\n",
              "   0.3898460865020752,\n",
              "   0.2590322196483612,\n",
              "   0.09137021750211716,\n",
              "   -0.6201463341712952,\n",
              "   0.3854433000087738,\n",
              "   -0.1458641141653061,\n",
              "   0.00787635613232851,\n",
              "   0.48369160294532776,\n",
              "   0.23787792026996613,\n",
              "   -0.0044933767057955265,\n",
              "   0.06207727640867233,\n",
              "   -0.3850691318511963,\n",
              "   0.13075487315654755,\n",
              "   -0.3881700038909912,\n",
              "   0.3324205279350281,\n",
              "   0.08105319738388062,\n",
              "   -0.13707011938095093,\n",
              "   -0.07814265042543411,\n",
              "   -0.4765930771827698,\n",
              "   -0.08815898001194,\n",
              "   0.33077341318130493,\n",
              "   0.3739830553531647,\n",
              "   0.21548721194267273,\n",
              "   0.024205368012189865,\n",
              "   -0.13591767847537994,\n",
              "   -0.4526553153991699,\n",
              "   -0.8138393759727478,\n",
              "   -0.5133274793624878,\n",
              "   0.16836749017238617,\n",
              "   0.9885444641113281,\n",
              "   0.3959435820579529,\n",
              "   -0.45137766003608704,\n",
              "   -0.7706408500671387,\n",
              "   0.3422488272190094,\n",
              "   0.6341436505317688,\n",
              "   0.6710309386253357,\n",
              "   -0.17201249301433563,\n",
              "   -0.6651149988174438,\n",
              "   0.20736831426620483,\n",
              "   0.6013907194137573,\n",
              "   -0.4109296202659607,\n",
              "   0.20893363654613495,\n",
              "   -0.6514403223991394,\n",
              "   0.29708272218704224,\n",
              "   0.6129928827285767,\n",
              "   0.16757918894290924,\n",
              "   -0.5578657984733582,\n",
              "   -0.070741668343544,\n",
              "   0.14674334228038788,\n",
              "   -0.4204404652118683,\n",
              "   -0.4347764849662781,\n",
              "   -0.38599368929862976,\n",
              "   0.20442421734333038,\n",
              "   -0.05559753626585007,\n",
              "   0.1429145336151123,\n",
              "   -0.82969069480896,\n",
              "   0.21421843767166138,\n",
              "   -0.30332860350608826,\n",
              "   0.6046147346496582,\n",
              "   -0.09804365783929825,\n",
              "   0.5679493546485901,\n",
              "   0.2554423213005066,\n",
              "   0.4772583544254303,\n",
              "   0.49273884296417236,\n",
              "   0.7818059921264648,\n",
              "   -0.6183952689170837,\n",
              "   -0.7850161194801331,\n",
              "   0.39457038044929504,\n",
              "   0.2474549263715744,\n",
              "   0.043237198144197464,\n",
              "   1.0713374614715576,\n",
              "   -0.3124530613422394,\n",
              "   0.48497530817985535,\n",
              "   -0.5076556205749512,\n",
              "   -0.7171796560287476,\n",
              "   -0.21141241490840912,\n",
              "   -3.8647878170013428,\n",
              "   0.20203036069869995,\n",
              "   -0.11934472620487213,\n",
              "   0.14802268147468567,\n",
              "   -0.1870955228805542,\n",
              "   0.16104166209697723,\n",
              "   -0.007227924186736345,\n",
              "   -0.16359733045101166,\n",
              "   -0.044711314141750336,\n",
              "   -0.21686996519565582,\n",
              "   0.37449508905410767,\n",
              "   0.20179274678230286,\n",
              "   -0.5887540578842163,\n",
              "   0.4528898298740387,\n",
              "   0.22183942794799805,\n",
              "   0.36918964982032776,\n",
              "   -0.015824049711227417,\n",
              "   -0.5068497657775879,\n",
              "   0.1642438918352127,\n",
              "   0.1286948025226593,\n",
              "   0.3222702443599701,\n",
              "   0.0808778926730156,\n",
              "   0.42278721928596497,\n",
              "   -0.33552780747413635,\n",
              "   -0.35731419920921326,\n",
              "   0.3205622434616089,\n",
              "   -0.4539613425731659,\n",
              "   -0.6271904110908508,\n",
              "   -0.030040955170989037,\n",
              "   -0.24421973526477814,\n",
              "   0.23223750293254852,\n",
              "   -0.1365804821252823,\n",
              "   0.7282962203025818,\n",
              "   -0.40536603331565857,\n",
              "   0.7596624493598938,\n",
              "   -0.12480579316616058,\n",
              "   0.1751147359609604,\n",
              "   -0.1704702228307724,\n",
              "   -0.056012775748968124,\n",
              "   0.031436558812856674,\n",
              "   -0.2980785369873047,\n",
              "   -0.891408383846283,\n",
              "   0.16090010106563568,\n",
              "   -0.8163376450538635,\n",
              "   0.04070892557501793,\n",
              "   0.034089721739292145,\n",
              "   -0.3393726348876953,\n",
              "   -0.5623222589492798]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA7XPr0gVU2C",
        "colab_type": "text"
      },
      "source": [
        "The huggingface/transformers repository lists the other pipeline functions, such as ner extraction, sequence classification, and masking. You are encouraged to explore them. \n",
        "https://github.com/huggingface/transformers#quick-tour-of-pipelines\n",
        "\n",
        "## <span style=\"color:red\">*Exercise 2*</span>\n",
        "\n",
        "<span style=\"color:red\">In the cells immediately following, use the pipeline functions or the word or sentence vector functions (e.g., similarity) to explore the social game underlying the production and meaning of texts associated with your final project. You have used similar, but often weaker versions in previous weeks. How does BERT help you gain insight regarding your research question that is similar and different from prior methods?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7VjYiVD2REe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I will use the pipeline function to explore the social meaning of twitter texts from my dataset\n",
        "df3 = pd.read_csv(\"/content/drive/My Drive/week 8/twitter_data_to_concat.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHabH91G2RPJ",
        "colab_type": "code",
        "outputId": "26383c65-b7ad-49e0-9efd-e2f8947873be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "419583b13181418a8f0fb14ca0c159e6",
            "eb9af3a95dba4cddb9837d6b38a6d1b6",
            "375b0fc149bb4d9f9237b6e2ca5308b3",
            "ba2d3b938c9449a39b43e8ef92f6955f",
            "c899aa63a81b4900af5f9c6cbc123631",
            "f200809d6c1946168c71b221c4148b1d",
            "fa271e71d9d040febe7edbb0bbda8469",
            "48d101e1f47b4d74bd8aa5c561f21df2"
          ]
        }
      },
      "source": [
        "# Allocate a pipeline for sentiment-analysis\n",
        " nlp_sentiment = pipeline('sentiment-analysis')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "419583b13181418a8f0fb14ca0c159e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=230, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv84CJ0N2RY8",
        "colab_type": "code",
        "outputId": "72c9c28b-0ea3-4406-811a-db4b3c218ea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "df3.text.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    finally after a month as corporate refugee bac...\n",
              "1    a demonstration takes place in glasgow calling...\n",
              "2    a demonstration is set to take place in glasgo...\n",
              "3    watching a hurricane katrina documentary befor...\n",
              "4    pakistan announces it will close four afghan r...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-GjwuaL3rPA",
        "colab_type": "code",
        "outputId": "af5a5919-6f63-47c5-9730-e594dbe65fdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "label = []\n",
        "score = []\n",
        "for i in range(len(df3.text[:1000])):\n",
        "  result = nlp_sentiment(df3.text[i])\n",
        "  label.append(result[0]['label'])\n",
        "  score.append(result[0]['score'])\n",
        "  print(\"Iteration {} -- Remaining Iterations: {}\".format(i + 1, len(df3.text) - i), end = '\\r')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1yG0Ud34h2M",
        "colab_type": "code",
        "outputId": "2f5180a6-aa10-4135-8cb7-a2f11f36e672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len([l for l in label if l == 'POSITIVE'])/len(label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.213"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz5Y5M2V57lN",
        "colab_type": "code",
        "outputId": "291dbab6-13b2-4b4f-bef1-4bb37fa7261f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len([l for l in label if l == 'NEGATIVE'])/len(label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.787"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjucsM2P6BlA",
        "colab_type": "text"
      },
      "source": [
        "If I just look at the first 1000 rows of data, only around 20 percent of tweets are positive, and almost 80 percent of tweets are negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJq2eRoY8MKj",
        "colab_type": "code",
        "outputId": "fe5e140a-5da7-4a1d-9c72-250db4b20e55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.asarray(score).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9341244"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ3ThqQN8U6F",
        "colab_type": "text"
      },
      "source": [
        "Overall, the confidence of the sentiment is fairly high, meaning that the sentiments among those tweets are fairly obvious"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV_DZt9W6K-c",
        "colab_type": "text"
      },
      "source": [
        "BERT helped me gain more insight as to the distribution of the sentiments for twitter data regarding the topic of immigration and refugee. This sentiment data would otherwise be difficult to obtain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpvhf-QbVU2C",
        "colab_type": "text"
      },
      "source": [
        "## Text Generation using BERT\n",
        "\n",
        "The last method which we will explore is text generation. While some may regard it as a parlour trick due to unpredictability, recent dramatic improvements in text generation suggest that these kind of models can find themselves being used in more serious social scientific applications, such as in survey design and construction, idiomatic translation, and the normalization of phrase and sentence meanings.\n",
        "\n",
        "These models can be quite impressive, even uncanny in how human like they sound. Check out this [cool website](https://transformer.huggingface.co), which allows you to write with a transformer. The website is built by the folks who wrote the package we are using. The code underneath the website can be found in their examples: [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py).\n",
        "\n",
        "We will be using the built in generate function, but the example file has more detailed code which allows you to set the seed differently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3feEv1knVU2D",
        "colab_type": "code",
        "outputId": "4289dba9-3b61-4495-82a1-43b519876f89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212,
          "referenced_widgets": [
            "3920749555a941638f53fb875d305289",
            "6069aa702c854535b12675a1ab1e49e3",
            "a299cb9d41bb431ca4af66742d6e8844",
            "b7a6739e829d460eb748e2e2b9579cb9",
            "44359362b6d144fa931a0d508dc4b01f",
            "d94f5fbc7c68470bb7b914caebc70efb",
            "a220eaebe77d4f54b9e0688f3403cc2b",
            "956d03f069e748c48049fcc63cc1b7af",
            "db39367aee87405bbc3c36847fb9f9e1",
            "fbc6da3bedca43ebb6b8bf670725c657",
            "d66ed220a64847158d4b9df782c2421f",
            "780f6727c3ab41dbaca15a778f7b019b",
            "cd3feaf6a1d84c6d89b033c067d99071",
            "77be4070ef454a01b3195f89c2bbbc70",
            "a679e1ec3ef144b9b8f3e55e16f8b849",
            "55353aead72e4078957007a31e767d4e",
            "11364f433f134157907dab72475e2903",
            "f9b3108b31f34f328cd9965c6f857da0",
            "ed8875965a044113a2d4e2a8e64f8c5f",
            "0585182ddec54a3da10edcf66fa057d5",
            "8ecf708c50394928948c5e762dd2c862",
            "0bb279123bb343bb8e26d78ff8075edb",
            "01829b7d76974b5295dbe36806d8f2d7",
            "c7999d73833f42508aca4ef687d0b9db",
            "cdbdc66f13374ca0b0aee5fea9027b6b",
            "f98de45b0fbf45ab8faa14078acaa860",
            "05c557d4a2704b9eb9b31fe384982775",
            "55e9c2d3169c46b3a08165397bf9c875",
            "9e965849cd974b07805e9a2d06591260",
            "988d10b8b6da46a4bf885867dd3bd2bb",
            "5230857292b54f34a77ac729a4d94dda",
            "3f9eb419fb6343609952da5270e3afd7"
          ]
        }
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3920749555a941638f53fb875d305289",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=224, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db39367aee87405bbc3c36847fb9f9e1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=1042301, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11364f433f134157907dab72475e2903",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=456318, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cdbdc66f13374ca0b0aee5fea9027b6b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=548118077, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT6CVaupVU2F",
        "colab_type": "code",
        "outputId": "3fe8ce6f-0f5a-4f7c-c6eb-e0aab136b3b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "sequence = \"Nothing that we like to do more than analyse data all day long and\"\n",
        "\n",
        "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_gpt.generate(input, max_length=50)\n",
        "\n",
        "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nothing that we like to do more than analyse data all day long and then try to figure out what's going on.\n",
            "\n",
            "\"We're not going to be able to do that. We're not going to be able to do that.!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSMF7H2cVU2G",
        "colab_type": "text"
      },
      "source": [
        "Wow. A little creepy, and as we can see, far from perfect: GPT doesn't alwats work out flawlessly, but it sometimes can, and we will try and see if fine-tuning helps. We are going to tune the model on a complete dataset of Trump tweets, as they have a set of distinctive, highly identifiable qualities.\n",
        "\n",
        "### Creating a domain-specific language model\n",
        "\n",
        "One of the most exciting things about BERT and GPT is being able to retune them the way we want to. We will be training models to perform two tasks - one is to create a BERT with an \"accent\", by traning a model with english news data from the UK, from the US, and from India. We will also train a language generation model with a bunch of Trump tweets. \n",
        "\n",
        "We can train models specifically over a certain domain to make its language generation similar to that domain. \n",
        "[run_language modelling.py](https://github.com/huggingface/transformers/blob/master/examples/run_language_modeling.py), followed by [run_generation.py](https://github.com/huggingface/transformers/blob/master/examples/run_generation.py). I've downloaded these files and added them to this directory so we can run them through the notebook. You are encouraged to look at these files to get a rough idea of what is going on.\n",
        "\n",
        "### Loading Data \n",
        "\n",
        "We want to now get our Trump tweets and our English news datasets ready. The data the scripts expect is just a text file with relevant data. We load the Trump tweets and then write them to disk as train and test files with only data. I leave the original dataframes in case you would like to use it for your own purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY8orn9UVU2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dfs = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flob8LrQVU2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for file in os.listdir(\"/content/drive/My Drive/week 8/data/trump_tweets\"):\n",
        "    dfs.append(pd.read_json(\"/content/drive/My Drive/week 8/data/trump_tweets/\" + file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzjYLbMsVU2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat(dfs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5a7KzcCVU2P",
        "colab_type": "code",
        "outputId": "301fc8f7-8d1d-4e0a-abd3-38c351aac31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>id_str</th>\n",
              "      <th>text</th>\n",
              "      <th>created_at</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>in_reply_to_user_id_str</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>is_retweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>947824196909961216</td>\n",
              "      <td>Will be leaving Florida for Washington (D.C.) ...</td>\n",
              "      <td>2018-01-01 13:37:52+00:00</td>\n",
              "      <td>8237</td>\n",
              "      <td>NaN</td>\n",
              "      <td>51473</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>947810806430826496</td>\n",
              "      <td>Iran is failing at every level despite the ter...</td>\n",
              "      <td>2018-01-01 12:44:40+00:00</td>\n",
              "      <td>14595</td>\n",
              "      <td>25073877.0</td>\n",
              "      <td>53557</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>947802588174577664</td>\n",
              "      <td>The United States has foolishly given Pakistan...</td>\n",
              "      <td>2018-01-01 12:12:00+00:00</td>\n",
              "      <td>49566</td>\n",
              "      <td>NaN</td>\n",
              "      <td>138808</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>947614110082043904</td>\n",
              "      <td>HAPPY NEW YEAR! We are MAKING AMERICA GREAT AG...</td>\n",
              "      <td>2017-12-31 23:43:04+00:00</td>\n",
              "      <td>35164</td>\n",
              "      <td>NaN</td>\n",
              "      <td>154769</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Twitter for iPhone</td>\n",
              "      <td>947592785519173632</td>\n",
              "      <td>As our Country rapidly grows stronger and smar...</td>\n",
              "      <td>2017-12-31 22:18:20+00:00</td>\n",
              "      <td>39428</td>\n",
              "      <td>NaN</td>\n",
              "      <td>157655</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               source              id_str  ... favorite_count is_retweet\n",
              "0  Twitter for iPhone  947824196909961216  ...          51473      False\n",
              "1  Twitter for iPhone  947810806430826496  ...          53557      False\n",
              "2  Twitter for iPhone  947802588174577664  ...         138808      False\n",
              "3  Twitter for iPhone  947614110082043904  ...         154769      False\n",
              "4  Twitter for iPhone  947592785519173632  ...         157655      False\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b1S9VD5VU2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_text, test_text = train_test_split(df['text'], test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KbY2K78VU2U",
        "colab_type": "code",
        "outputId": "1e03752f-3100-4ab8-9ace-0dca049e89ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "train_text.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "552     Ready to lead. Ready to Make America Great Aga...\n",
              "379     Anyone who doubts the strength or determinatio...\n",
              "3899    Serious doubt in Illinois as to whether or not...\n",
              "1345    \"@katlynne1994: @realDonaldTrump @foxandfriend...\n",
              "6782    \"@burgundylue: I am enjoying watching Joan Riv...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXM-vRkGVU2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text.to_frame().to_csv(r'train_text_trump', header=None, index=None, sep=' ', mode='a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta-u4rNPVU2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_text.to_frame().to_csv(r'test_text_trump', header=None, index=None, sep=' ', mode='a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBvToAl-VU2Y",
        "colab_type": "text"
      },
      "source": [
        "I now used the Google Colab GPUs to train the Trump tweet models. We'll be doing the same for our blog posts too.\n",
        "\n",
        "### GloWBe dataset\n",
        "\n",
        "We'll now load up the GloWbe (Corpus of Global Web-Based English) dataset which have different texts from different countries. We'll try and draw out texts from only the US, UK and India. We'll then save these to disk. Note that this is a Davies Corpora dataset: the full download can be done with the Dropbox link I sent in an announcement a few weeks ago. The whole download is about 3.5 GB but we only need two files, which are anout 250 MB each. The other files might be useful for your research purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dHM-5OGVU2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lucem_illud_2020"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnj0if3JVU2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "address = \"/content/drive/My Drive/week 8\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9IJjpY-VU2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these are the exact name of the files\n",
        "us = \"/text_us_blog_jfy.zip\"\n",
        "gb = \"/text_gb_blog_akq.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdcqeC0CVU2g",
        "colab_type": "code",
        "outputId": "67fca28f-8fb5-41f5-9273-3649f4ec270b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "us_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"us_blog\", num_files=5000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text_us_blog_jfy.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z140a29MVU2h",
        "colab_type": "code",
        "outputId": "40af5ef5-19a4-4cd7-9ddf-04ca0cd98ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gb_texts = lucem_illud_2020.loadDavies(address, corpus_style=\"gb_blog\", num_files=5000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text_gb_blog_akq.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buXuD_q_VU2j",
        "colab_type": "text"
      },
      "source": [
        "We now have a dictionary with document ids mapping to text. Since we don't need any information but the text, we can just save these to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJeaMcrcVU2j",
        "colab_type": "code",
        "outputId": "e22fa3f3-cc55-4244-9a5b-951261b86b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "' '.join(list(us_texts.values())[10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"< h > Look how men 's and women 's magazines write about money < p > I read Oprah 's magazine once in a while Yes I said it And my jaw drops when I see an article like the recent 25 things you do n't have to worry about What Why do I need a magazine to tell me what I do n't need to worry about < p > But I 'm not the target audience It turns out men and women 's magazines are very different in the way they present money Whether that 's good or bad is the subject of a guest post by Nina Smith from Queercents I love how she 's used excerpts to point out the differences in how men 's and women 's magazines write about money < p > -Ramit < p > < p > Investment advice comes in all shapes and sizes Grab a few back issues of Esquire and a quick scan of its investing columns reveals investing advice For example they give the Baby Bells vs Cable Companies < p > So here 's my Aha moment why do men get investing advice in their magazines and we get financial basics in ours < p > Women get touchy feely encouragement Suze Orman is an O Magazine columnist and typically I like her advice She 's spot on when it comes to buying a used car and term life insurance But when speaking to women is she talking down to us < p > Case in point look at what she says about establishing a rainy day fund in this O article She writes Ideally you will have eight months of living expenses stashed in a savings account I know that sounds daunting but make it a goal Start putting away a little each month Every penny you save is a step toward building your own personal insurance plan < p > Ahem I know that sounds daunting but make it a goal < p > Would a male personal finance expert ever instruct a man this way columnist at Esquire and author of The Green Magazine Guide to Personal Finance A No B.S. Money Book for Your Twenties and Thirties writes You 're keeping your emergency cash in a money market fund In other words do n't fund the expansion of your portfolio into stocks and bonds with the money you 're keeping on reserve but feel free to consider that money part of your portfolio < p > See the difference First let 's consider the demographics of O Magazine The median age is 45 readership is predictably female 91 married 66 and a median household income of $ 88,000 Their readers are n't exactly females fresh out of college < p > So Orman is encouraging forty year old women to make sure they have an emergency fund and Kurson assumes twentysomething guys already have a stash in reserve Perhaps this is why CNBC gets Jim Cramer and The Today Show has Jean Chatzky < p > A commenter at BloggingStocks had this analysis of Orman 's writings by saying want math I bought one of Suze 's books and when she started talking about how I felt about my money I put the book down in disgust Behavior patterns as applied to money fall under psychology everything else is quantifiable I do n't need to have a good relationship with my money I need to understand how the stock market the housing market and my 401 K work < p > Gender specific behaviors with money But do female money experts talk down to us or are we inviting the tone by behavior After all according to Manisha Thakor and Sharon Kedar authors of On My Own Two Feet the average woman between ages 24 and 35 has only $ 500 in savings < p > Woman 's Day acts like their readers only have $ 500 in savings as well and I do n't know any women under fifty subscribing to that magazine Mary Hunt is their columnist and a quick glance at her 2007 columns reveal topics like Big Online Bargains and Slash Your Food Bills @n't Mess with Taxes gives her perception on the male vs. female financial behaviors She writes Even today some gender specific societal expectations manage to persist That is a lot of women take a more supportive fiscal approach focusing on money maintenance holding on to what they have instead of taking steps to advance it < p > We need to get over that right now and get more aggressive when it comes to money making it saving it investing it The go for it approach seems to be more typical of male financial bloggers Men at least in my anecdotal observations are more apt to be risk takers with their money They embrace the idea that to make more money you sometimes have to take some financial risks with what you 've got < p > And it 's not just Oprah I could n't find any money advice in InStyle magazine but they offer plenty of ideas on how to spend it Glamour claims to have a money expert online debt quiz Take it and see how you stack up with their readers If you 're a regular follower of personal finance blogs then it 's likely you 're way of ahead of these well heeled and in style consumers < p > Just to be fair and balanced I reviewed some other men 's magazines and money was either missing or sexualized and presented by young attractive female writers Check out the article by Anya Kamenetz in Men 's Health called 7 Financial Habits of Highly Laid Men Enough said otherwise this might segue into a different discussion < p > But maybe money is missing from general interest magazines because men go to the source for their financial advice by subscribing to the money periodicals As an example here is the male female readership break down for Fortune and Money < p > Money Male Female 64 36 Fortune Male Female 79 21 < p > And guess who is reading The Wall Street Journal and Financial Times < p > Money spends the same whether it 's carried of the advice really matter In the end money is money and basic truths are better than nothing at all But if empowerment and financial independence are what Suze Orman wants for the ladies then maybe it 's time to butch up the advice Do n't sugarcoat or wrap it in a soft pretty package We 're ready to take it like a man That 's how you turn women savers into women investors < p > Finally for the sake of starting a conversation below do you agree that women get fed the softer side of money from women 's magazines Or will some of you accuse me of gender generalizing If you agree then what should we do about it Write to Oprah Or just subscribe to Fortune and Money like the big boys < h > 68 comments < p > Having read way too many women 's magazines in my time I certainly ca n't disagree with anything you 've said about what they say about money I have n't read enough men 's magazines I 'll take your word for it < p > I want to learn more about investing I would say I have a good grasp of the basics but that 's it However I find that my eyes gloss over when I pick up Forbes Fortune etc That said my husband reads them but not for the investing advice but rather for the profiles and features < p > To play devil 's advocate here they are just writing for their core demographic Their 25;2031;TOOLONG tell them that to write a financial article geared to women it should be done in such a way This may seem demeaning to women who are educated in finance or who just crave the hard facts but for the majority of readers I bet it is just perfect My wife for example would enjoy the article in the O Magazine than one with hard hitting facts And let 's face it these magazines are primarily written for entertainment < p > My wife reminded me of this when you would assume is written for parents But their readership is primarily female so almost all of their articles are geared towards women The articles written for men They talk down to us like kids as if we could n't possibly change a diaper or raise a child without serious help It 's all basic information < p > My advice Ditch the gender specific magazines and pick up a real financial publication All the mens magazines have the same junk articles < p > I am a regular reader of O Magazine For some context I am a 27 year old woman I cohabitate with my fiance I am employed full time and we have a household income above the average quoted for the typical O Magazine reader I am a lot younger than her typical readers but otherwise have quite a bit in common with them < p > I do n't read men 's magazines so I ca n't really say whether or not financial advice is presented differently to men or women somewhat touchy feely manner to women And you know what I think it 's a good thing Maybe it 's because of my younger age than the average O reader but I do n't consider Suze 's advice style to be condescending or talking down to me I appreciate that she bothers to acknowledge that her advice can seem daunting I have about one month 's worth of living expenses in my emergency fund and it 's taken a long time to build it up because I have a lot of fixed expenses to deal with so if someone told me to build up 8 months worth of living expenses in my emergency fund I would probably just write it off as over ambitious advice not based in reality But she brings some reality to it by acknowledging that it will be difficult and then she boosts your confidence by telling you that you can do it anyway I like that style < p > I do n't like the hard hitting facts only style that you describe for men 's magazines but I have a different communication style from most men So if the advice is spot on and being delivered in a column I find well written realistic and interesting to read I 'm more likely to pay attention Men do n't seem to appreciate this style They want just the facts so their magazines provide that I 'm not saying that my preference would hold true for all women but there are a lot of women who prefer their financial advice packaged that way I do n't care if that makes it softer ultimately if I follow the advice I 'll still end up making smart financial moves regardless of how the suggestions were phrased < p > We talk down to women about money for the same reason we give our daughters names like Tiffany and Amber whilie naming our sons things like Robert and David Our expectation is that they will be homemakers or trophies and nothing else How many physicians have you known named Amber magazines are made to provide a touchy feeling with anything they write about I like them for exactly this just a lazy afternoon and a comforting talk They are not meant to be educational If I want facts I 'll get them somewhere else Usually this would be a decent book that covers the basics properly and shows where to get more knowledge on special topics afterwards And just a word on the hard investing advice in men 's magazines I do n't consider sell apple and buy you choose as such < p > I just watched yesterday 's Oprah where Suze Orman and two families in deep debt were on I did observe that she spoke more like a motivational speaker and only gave two or three good tips to each family With the kind of debt they were in they need more than a general get a job and sell your house or your car is not your baby the baby is the baby buying things to define themselves and that ignorance is not an excuse That 's good advice especially when you look at what 's being spouted in women 's magazines these days The information is out there but sometimes the wakeup call is what is needed first < p > I think the traditional attitude towards money roles is that the women take care of the day to day money while men make the decisions for the big purchases and investments I remember asking a 60-something lady about finances asking from a 20-ish year old female 's point of view and she said that women in her generation were taught to rely on their husbands about money Household money was theirs but they had no idea what to do with $ 1 million and what investment risk meant I replied that was also the last generation that people believed politicians had their best interests in mind < p > It 'd be great if parents would teach their children finances in a genderless manner just like men and women should know how to use a screwdriver boil to being self sufficient If parents do n't want to raise their child to depend on a spouse they need to teach them everything they 'd teach either gender < p > I agree that women get the softer side of finance and I think its more of that supportive nurturing stuff that I 'm not such a big fan of Its interesting that I do n't know any of the female personal finance bloggers who put forward that sort of tone < p > I think we should stop getting our financial advice from second rate sources and start using better sources someone will market something better to us if we create a demand < p > Suze seems to take a certain tone in her books as well not unlike the one in O. You 're right that it feels a bit patronizing But I think her books are intended for both genders and not trying to be sexist < p > That said I 've never read anything really financially useful in a womens magazine Some basic stuff but that 's it guys are still on top < p > Women 's magazines in general have decided that I am a shallow idiot They are not funny or clever they offer advice on how to please my man but not on how to be pleased and they are constantly trying to convince me that I am ugly and need their beauty tips Because of this I started subscribing to men 's general interest magazines < p > I agree that women should n't be coddled and we certainly need to be more aggressive about earning and investing money There 's no reason to avoid investing jargon and a little bit of math when writing to a female audience < p > HOWEVER if you want to talk stereotypes I think men get the short end of the financial stick Men are conditioned to care only about sexy financial matters like day trading flipping houses entrepreneurship hedge funds and investing in commodities < p > Most men need to worry less about today 's hot stock and worry more about so called costs how and why to invest in a Roth IRA why credit is important why you should have an emergency fund or cash reserves as a men 's magazine would refer to it < p > I 'm a private banker and work with a lot of rich or wanna be rich men These guys are really smart for the most part But most men I know have NO idea what their credit score is or why it should matter little to no cash tons of expensive electronics cars and no plan for the future < p > Today 's men are eager to start trading stocks as soon as they have savings They assume they 'll make it big eventually and do n't need to save now Women may be too conservative but they 're learning valuable lessons like how to make do on what you 're earning now and that boring index funds beat day trading every year < p > I found this article to be quite thought provoking Sure about money I would also agree that women want a more relatable and interesting read otherwise they would read the numbers What I wonder is if the type of advice comes from elsewhere What I mean by this is the assupmtion that women let the men in their lives control their joint finances not the case with my partner and me Also not wanting to go into that whole gender gap discussion but women are reported to have more to lose in the event of divorce death of spouse etc Factors like these could influence why the advice is not based on accumulating more wealth but rather building and saving wealth Not that I 'm denying the accumulation of more wealth could improve a woman 's standing the message 's priority 1 safety net 2 investing Frankly I find the state of women 's savings alarming both the stats most give and personal experiences and yes I want to shake some of them and tell them to wake up and save how the heck would you start trying to convince them to invest My mom is quite proud of herself that not one penny of hers is in stocks she also lacks what I would call adequate savings in fact she just now at age 50 decided she should start saving for retirement Seriously she needs to read about saving she needed to read about saving 30 years ago Although investing now may make up for her poor saving habits the first battle is to get her to save enough money that she could invest without fear of the 1929 stock market crash or whatever it is that scares her about investing As a young woman I enjoy reading Cosmo and O. I like magazines that acknowledge the changes involved with being a woman and trying to give women the tools to empower themselves The difficulty for women magazines is there are many faces of women just as there are many different faces of feminism I 'm not saying that men are n't diverse and are n't that prefer to act more masculine to prove equality through sameness vs. the feminists who embrace and act more feminine to prove different but equal Women 's mags tend to try to reach them all They 're not saying women should be like x but rather saying if you 're a woman trying to be like x here are some helpful hints to get you there It seems to be the same with money in these mags Cosmo will talk about the babe 's budget why women are whimps with money how these women became rich etc Again I 'm not saying men do n't spend money differently my partner spends money on videogames and technotoys while others would spend it on cd 's and cars What I am saying is women mags seem to make reaching a diverse female audience a priority and that needs to be taken into consideration when critiquing them Also Suze Orman will give that same advice to men It is just her style It of understanding first and then say too bad do it anyway A lot of people hate that in her men and women alike but some actually respond to it better because it humanizes the process I agree with the other comments if women want sound financial advice go to a better source If women are looking for a mag that they can relate to and may have financial pointers career advice etc then enjoy your O Cosmo Self or whatever it is they enjoy reading < p > Household money was theirs but they had no idea what to do with $ 1 million and what investment risk meant I replied that was also the last generation that people believed politicians had their best interests in mind < p > Wow That 's kinda harsh The reason she married her husband is because she loved and trusted him you ca n't have that type of skepticism with your husband like you do with politicians crikey < p > Shoot I think it 'd be great about money listening to the boss ladeda go on little lunch dates and have the day to myself and my hyperactive children Hell yeah < p > blah blah blah That s all I got from the above article If there was a point to be made it was lost in the first paragraph Was it are men and woman presented money differently or maybe Why are men and woman presented personal finance differently In either case I must say are you kidding Sorry to be harsh but what you are saying is n't anything resembling an original thought the prime reason I visit this site < p > So let 's take what you wrote about O magazine takes it easy on females while Esquire does n't Is that about it Wow we should write Oprah and let her know that we have finished some crazy research and came to some crazy conclusions that she should know about If you think for a second that big magazines like O and Esquire do precise makeup of their readers you need to go back to school Chris I liked your comment < p > So while you were offended by not getting the hard hitting facts in O magazine there are a thousand 40 year old house wives nodding at the interesting article It 's called business and Oprah is not in the business of turning readers viewers away < p > Did you not realize that women and men are different in almost every aspect of money Did you notice the guy to girl ratio in finance and econ majors Or the ratio in the investment management banking or trading industries Because if you had n't let me tell you your average girl does n't give a crap about your hard hitting facts They want to feel good about what they are doing with money They want that positive relationship with their money They want to be taken care of I loved the Mens Health article exactly zero females in the survey prefer to make more than their husband one on one about how their employee stock purchase plan works and how exactly stock options 401k and ROTH IRAs work and not one female maintained interest They wanted to know What should I do When should I sell They want to be told how and what to do and not be bothered with details Guys on the other hand maintained interest asked me about different strategies and craved more and more information < p > Why is that Well that answer would actually be something worth reading I believe Ramit is working on that < p > But just to say that women and men are presented money differently Give me a break this is n't a high school paper were you simply regurgitate facts and include pictures and quotes < p > I 'm a feminist in that I believe in equality between men and women But I do n't believe that equality must be delivered in the same package Men and women are different and things that appeal to men do n't always appeal have expressed that they are pleased with the status quo of financial advice geared toward women and that their eyes glaze over when they read financial advice geared toward men Why should men and women try to be the same when they are n't We will always be different but that does n't mean we ca n't be equal If the financial advice in O Magazine appeals to women over that in Esquire then let the ladies read O < p > Not that I took most of your comment seriously JP but I do want to point out one thing to you < p > Your personal experience counseling all of 30 coworkers about their finances talk about in depth research btw taught you that women are n't interested in finance I do n't suppose you ever considered that the women were unsure That because women are NOT exposed to as much financial information as men and that because women are simply different that they were looking for some advice Women tend to do that y'know I have to wonder also if your own attitude had anything to do with what you observed Self fulfilling prophecy and all that Based on what you 've said here after all you 're pretty much an a when it comes to women < p > I checked out that Online Debt Quiz from Glamour and found some of the questions to be especially telling Under how do you pay your credit card bills the last answer is I do n't have any credit cards But the very next question is about the average interest rate your cards give you and there 's no way to opt out and say you have no cards there Oh but you CAN say you do n't know < p > I 've noticed that the more patronizing financial advice is the more firmly it assumes that the reader has credit cards and is in debt to them But then it got even worse One of the final questions is If you carry debt how I checked off every day because I do have debt that I am paying off and I do something every day to improve my finances < p > Then I saw their response They called it financial worries and sternly informed me that Credit counselors say that obsessing about debt is a warning sign Hello I KNOW but are they trying to say that it 's a warning sign about my mental health or my debt It 's really irresponsible of them to dictate how often their readers should think about financial issues and assume that there is no way that they could be thinking about them healthily And I do n't appreciate financial quiz lectures from a magazine that ca n't even articulate what a healthy relationship with money or debt would be < p > It 's no big secret that generalist entertainment sources like wo men 's magazines and MSN are n't the best place to get accurate reliable financial advice 10 hot gifts this holiday season but what consistently respectable publications like the Wall Street Journal and Financial Times They 're constantly pushing the idea that financial success comes through sexy trading strategies ownership of the hottest hedge funds and sophisticated tax management think Cayman Islands It 's just another case of telling your target demographic what they want to hear in this case the wannabe Masters of the Universe set but when I see it in publications that are supposed to have some stature in these matters it goes a long way toward explaining why for example men trade their stocks far too often and vastly overrate their own ability to predict the future given the benefit of hindsight I also blame Rich Dad Poor Dad for this may its name be cursed to the thousandth generation < p > Personal finance is n't that complicated but it 's sometimes counterintuitive and as long as people dislike being told their innate prejudices are wrong I would n't expect to see much solid advice in the mainstream media < p > Let me ask a tangential question reader < p > What message(s do these men 's magazines send poor men I do n't subscribe to any but I read a lot since I work in a convenience store and get to take mags home to read I constantly get the message that if I do n't have a hefty portfolio and the latest tech gadgets I 'm a big zero < p > I think that the noted advice sent through the O mag channel is relatively sound for your average North American How to buy life insurance The basics of financial planning and investing Home finance basics everyone should know Know how much home you can afford How to play rollover with your 401(k Most people who work full time jobs outside the financial industry have a spouse or significant other and have children do not really have time to do the necessary work to decide whether or not the current price of a stock = the future value < p > I agree that Womens magazines have a feel good aspect to them and make them feel that buying 1000 items have their feel good touch to them Do you think reading an article in Esquire about Walmart being a good buy is a good enough source for any investor to leap on I hope not These articles make the men feel like men and allow them to say to their friends you know this stock took a dive of late and let me tell you why and if I had enough money to make this investment worthwhile while having even more money to be able to risk this investment without doing further research I would be first to jump in Give me a break The majority of these magazines mentioned tailor to materialism and commercialism not a source for particularly sound information < p > O Mag is close to providing good advice on what should be done by most North Americans and that is to find a financial planner who understands what the financial market is what your needs are what you want in terms of wealth and freedom and will tell you what you need to do to get there Esquire is a good source for investment advice again Because it is shallow much like its content < p > On further reflection this post might have been a better analysis if I compared the advice found in women 's magazine to what is written in money magazines with a predominantly male readership This in turn would change the call to action and question why more women do n't subscribe to magazines offering investment advice instead of just catching the basics found in women 's magazines < p > With that in mind here are a few thoughts in response to some of the comments < p > Chris I like your example about how the parenting magazines talk down to men when they try to write father specific content Interesting point and I agree with your suggestion < p > John M My doctor 's name is Chris as in Christina Someone should write a post about how your name impacts earning potential < p > Katie There are n't demographics about book readership but if the money magazines are an indication the buy sell example is weak but the point I meant to make is that I 've never seen financial advice in a women 's magazine go beyond a play it safe strategy I understand it 's a generalization but women save play it safe and men invest take risks I 'm just trying to figure out why < p > Freecia I love the point that gender specific views are often learned at home as a child Hopefully more parents in my generation and beyond will focus on teaching their kids to be self sufficient < p > Meg Most men need to worry less about today 's hot stock and worry more about so called womanly financial issues I had n't thought about it from this perspective Thanks for raising that question < p > JP Your point about men maintaining interest is well taken So why is that Is their inquisitive nature innate or was it learned If so how was it learned and why are n't young you write a guest post and we 'll see how well you fared in high school < p > Cady I 'm glad you clicked through and took the quiz I was surprised by their analysis of the results < p > Pete You 're right about sexy financial advice in the mainstream media Sensationalism is what sells magazines when it 's served up with an enticing headline < p > Hopefully most people read Esquire Men 's Health Cosmo and Glamor for entertainment and NOT sound financial guidance Extrapolating that the financial advice in those mags is geared toward their gender specific demographic is not brain surgery it 's obvious Esquire is for wanna be stylish gadget lusting young men at least in my opinion I 'd never read it for financial advice < p > A nice follow up to this piece might be to try contacting some of the writers mentioned and asking them why they write that way < p > They might be willing to take a few minutes to answer You can usually get columnist e mails a magazine 's PR department and explaining you 'd like to interview one of their writers < p > Could work and would be interesting to hear different magazine writers talk about why they write about finance in a particular way < p > I am female and have always been very interested in finance Some girls get trips to Cancun or a new car for their 18th birthday I got The Richest Man in Babylon some Suze Orman books $ 1000 and a financial advisor Maybe women do n't go to O or money to get advice on finance but women are talkers and knowing my friends we talk about our investments Of course we never talk amounts but the other day a friend of mine asked me I know you used to own shares in Wild Oats and now that Whole Foods bought them out what am I supposed to do with my Wild Oats shares What did you do O magazine and the other women 's magazines only skim the surface of a lifestyle magazines and not financial magazines Who cares what kind of advice women are getting in women 's magazines Smart women who are intelligent enough to get real financial advice are n't going to go to O to get their facts just like smart men do n't go to Esquire I do n't believe that any true women 's or men 's magazines really offer hard hitting financial advice They just know how to package their stories for their demographics < p > On further reflection this post might have been a better analysis if I compared the advice found in women 's magazine to what is written in money magazines with a predominantly male readership < p > Comparing womens to mens magazines is fair although if you have read many men 's magazines you should realize that in my opinion Suzie Orman 's advice is probably more practical then Ivonka Trump 's stuff magazine < p > Comparing entertainment magazines with finical magazines is however not a fair comparison This would be like me comparing men 's entertainment magazines take on is that I do not think that you picked a topic that had the potential to be what you wanted it to be Although I think you did a good job researching and writing this article < p > Interesting how this topic brings out some people 's fundamental hostility to women And sad < p > As a moderately savvy but math challenged woman I never have been able to stomach Suze Orman 's saccharine touchy feely advice that is full of commonplaces and says little or nothing that anyone who is awake should n't already know When PBS 's cameras pan over her audiences you can see that many of the entranced folks gazing slack jawed and nodding as they lap up her pearls of wisdom are men and so I assume it 's not just women she speaks to and not just women whose level of financial sophistication is alarmingly low < p > That said the fact is that a vast proportion of American women spend their old age in poverty And I have had women university students in my classes who do not personal financial matters because they think it 's boring they 'll find eating cat food pretty interesting when they get to be about 75 and who never have balanced their checking accounts ever < p > Among the current generation of elderly women an important reason for widespread poverty is having spent a large portion their adult lives as fulltime homemakers and having unwisely depended on a husband 's income and generosity to support them The next generation of elderly women though is likely to face poverty because of ignorance bad financial planning and spending habits fostered by those very women 's magazines that dispense sugar coated grade school level advice < p > Whether or not gender differences in communication style exist women as well as men clearly need competent training in how to handle money if they are to survive and grow in the economy we have today I personally do n't find patronizing advice helpful but if others do evidently there 's a place for it < p > I am a female PF blogger and do agree how to save money at the grocery store crap and alot of the advice for men is the top 5 hot stocks right now < p > I volunteer as a financial counselor and do find that many women are lazy when it comes to finances < p > They do n't take the initiative to learn this very essential life skill and many are still thinking it 'll all work out somehow < p > sigh < p > Oh ca n't stand O Magazine or Oprah for that matter < p > There was a Finance issue she did a couple of years ago that made me nutty it was very emotive and basically had a theme of do n't sweat it money is n't what 's important in life anyway it 's the relationships you have < p > Thus totally marginalizing the empowerment women could have if they learned this very essential life skill < p > Men and women are different plain and simple and this it 's reflected in the different writing styles typical of a diffcult time understanding this not surprisingly < p > JP You have got to be kidding You are a complete moron if you think your statistics based on advising 30 people is anywhere near an accurate sampling of women If you do it just proves you 're a narrow minded sexist neanderthal < p > As someone else pointed out think about how girls and boys are educated about money < p > My girlfriends and I discuss stocks and investing and we actually invest Imagine < p > By your own admission you missed the point of the article Yes men and women have different approaches to money But have you considered that one of the reasons more men tend to have more money is that they are hmmm presented with more INVESTING advice instead of the SAVING advice presented to women That 's a significant factor < p > While I do n't read O I do read other women 's magazines I 've not noticed any hard advice on how to really in a fashion magazine dealing with debt and other uber basic topics Nothing about whether or not investing in tech is still viable lists of dividend stocks etc < p > SR it 's funny how you slam JP 's use of anecdotal personal evidence yet rely on it in a later paragraph < p > The first rule of writing is to know your audience Oprah did n't build her empire by ignoring who her audience is quite the contrary I bet the editor and writers of her magazine spend inordinate amount of time and money figuring out what the fears and dreams of their readers are and write to exploit those fears and dreams Just because there is a minority group of women who know or would like to know about the hard financial facts does n't mean they go to O for those facts < p > Finally writing buy Walmart is more useless advice than do n't buy branded commodities despite the former being hard advice a book or take a course And pick up O or Esquire if you want to fry your brain < p > I do n't with the assessment of Suze Orman I know she can be kind of annoying and definitely talks about relationships with money in a way that sounds kind of girly but she is definitely all about female empowerment when it comes to finances She is not afraid to tell a female caller on her show to leave a boyfriend who is causing her financial problems or to sign a pre nup to protect what 's hers to not lend money to friends etc She also talks about why it 's hard because of social pressures to do these things sometimes even when it 's in the person 's best interest She has her faults and God it 's annoying how she calls everyone Boyfriend and Girlfriend but I think she 's one of the few PF people who is both accessible to ALL women i.e. those who are n't already well versed in finances goes way beyond saving a rainy day fund < p > Ok I am seeing a lot of posting about what is NOT a good source for financial advice so here 's my question < p > I know nothing about finances except how to spend money really well I am attempting for the first time in my life I am 27 to get control of my finances I have a sneaking suspicion that those O magazine articles are geared toward my dumb @ss < p > So I ask you good readers where do I go I do n't want touchy feely junk I need sound advice I have a child I 'd like to continue to feed and clothe her Where does someone like me start Nearly every place I was looking has been slammed today so where do you recommend < p > I 'm reading this blog for one and I get the daily feed from the Motley Fool but frankly until I have some savings and actual jargon < p > As a women in her late twenties who has been investing for 10 + years and has well over $ 500 in emergency savings my reply is that I do not look for financial advice in Glamour Cosmo Self or any other female magazine I read Kiplinger 's Money or another finance oriented magazibe when I want to read about finances just as I read T+L or Budget Travel when I want to read about travel If you want to read about financial stuff read a different magazine you will not find what you are looking for in O And yes the financial industry does NOT treat men and women the same but who cares seek out the information you need and find an advisor that respects you and your needs < p > Lei amazon.com There are hundreds of great books on personal finance and investing which are much better at giving a thorough and well rounded understanding of money management than any blog could Reading websites or blogs times but not a substitute to books Amazon has excellent lists by readers on any subject pick a list which caters to your interests read a lot of reviews and order a few books < p > It sounds like you would most benefit from books on simple living or voluntary simplicity While many of the books claim they are not about eschewing the comforts of modern living they go on advocating making your own soap or hugging trees Nevertheless many of them have excellent advice for those trying to get out of a debt spiral or to increase their savings The best book on the subject I 've read was Voluntary Simplicity by Duane Elgin Although it 's mostly feel good throw away fluff it 's thankfully not full of verbose anecdotes used to poorly illuminate a point which pollute modern writing < p > The chapter about how to calculate the true cost of widgets to you was the biggest eye opener I 've had in years Instead of looking at how much a widget to invest in earning enough money to buy that widget When you realize that you spend 90 minutes of your life earning money to pay for that mocchachino it really makes you rethink your priorities < p > Or the other alternative is to enroll in classes at your local college < p > Lei If you 're just starting out on your financial journey I would recommend Smart Women Finish Rich by David Bach Yes Ramit he has the Latte Factor in there What I a 26 year old female 25 when I first read the book found to be helpful in this book is it did change my relationship with the world around me in terms of money goals etc Prior to reading the book I knew I wanted a house someday and I picked 2 years as a good measure of time to get a house I read the book somewhere in it it had things you wanted to accomplish within 3 years so buying a house was one of them Because it made me spell out for myself what is preventing me from buying a house and how do I overcome that it also makes you write down for every goal even if it is five years off what is an action that you will take toward that goal within the next 48 hours Essentially it taught me that two years still starts right now Other things that happened after reading that book was that I got a promotion to a position the company created for me I 'm now in charge of Human Resources I credit the book for this because in my pursuit of getting the company to offer a 403B my boss listened to me ask questions about whether Roth options were available terms I never would have known had I not read the book etc and I became the person with authority on these topics Prior to my promotion I was a weldor so HR was n't a natural progression Was the David Bach book the last thing on finances to his book Yep I find it helpful easy to understand and motivational I also read somewhere to Read something anything about money everyday I like that advice and wish more people used it There are so many people who have real shame when they ask questions about money Should they know it on their own Most likely but everyone needs to start somewhere Most people assume they wo n't be able to grasp the concepts before they even really try Then there 's people like my partner He has no shame in saying You deal with the money because you 're smart and then you tell me what to do with mine Along those lines I 'm glad to hear that you read things just to get comfortable with the jargon for now That 's more than what most do Long answer to your question but I hope you find something useful in it < p > I do n't think anyone should bash women 's magazines for getting articles like those found in men 's magazines results in bigger nest eggs In fact I have seen studies showing how women outperform men with investments primarily because they follow a more basic path Rather than picking stocks women tend to create more diversified portfolios They trade less so they incur fewer transaction fees and pay less in capital gains All of this adds up to more money I will have to find the study but I know it exists because it stuck with me < p > So bash the women 's magazines all you want for not giving enough hard core quantitative analysis When it is all said and done the only thing that matters is performance And women tend to top men < p > This has been an area of great frustration for me since I was in high school I agree that gender issues continue to be a problem and a difference between men and women I got a deal for a free subscription to Smart Money which of course came with a solicitation and Motor Week It was assumed that I am a man because I subscribed to Smart Money instead of Cosmo < p > When I tried to find a mentor to help me learn more about investing I was met with either women telling me they left the investing up to their husbands or the men telling me that I should n't bother with such a small sum Occassionally I would find a guy my age that would discuss investments and industries but that was rare < p > Finally I agree with your observations on Suzie Orman and her tendency to go for the touchy feely version of money but there is a population that it serves While I would really like to see more writers addressing starting out small and growing smart I ca n't deny that everyone has their own learning style I look forward to the day that women can be more confident in the financial arena beyond keeping a home and family < p > I believe that for a person who 's still trying to figure out things information and jargon I have no clue about It 's the same if I try to watch a finance TV channel like CNBC So I guess I prefer a mix of down to brass tacks stuff like that on oprah.com and blogs like yours where the advice though not sugar coated is n't really whizzing way above my head < p > From Lei So I ask you good readers where do I go I do n't want touchy feely junk I need sound advice < p > I first became aware of money matters by reading Consumer Reports It not only has consumer reviews and advice it also occasionally provides easy to understand financial advice Money Magazine is also pretty easy to follow and seems to have good information Is Kiplinger 's still out there I used to read that now again and found it interesting If you listen to NPR or go to the NPR website The Motley Fool is pretty good < p > Also my financial adviser was doing a short course on personal finance for women of financial managers do this it was inexpensive and what she had to say was a real eye opener You might want to check community colleges near you for offerings like this < p > And I learn a surprising amount painlessly by cruising the personal finance blogs Sometimes you pick up as much from the comments as from the bloggers posts especially if you google the subjects and try to learn as much as you can from reliable online reference sources There 's a site with links to 100 PF blogs it 's fun to go back there once every month or so and see what 's new on the sites that you do n't visit often http 35;2058;TOOLONG < p > People seem to think of an average women as a financially dumb person We can argue about whether that s true or not Its not relevant I am sure Oprah magazine does enough surveys to make sure their magazine sells well so there are definitely a lot of females who like some parts of females who want bare facts and not some BS about touchy feely finance topics I for one am not impressed by Suze and the likes she does n't tell me something I do n't know already I do read WSJ Fortune Money blogs for personanl finance which have something decent So my advice to females out there do n't depend on women mags to tell you everything about finance Infact somethings you only come to know by doing < p > wrt JP 's comment I do have some female co workers who are HIGHLY interested in what their 401k is doing and how they can invest further in different ways to increase their networth If I were to generalize based on this females are more interested in hard facts and numbers Moral of the story generalizations do n't help Each person had a different way of looking at an article information < p > A hidden premise in this discussion is that writing aimed at men is somehow the standard The truth may be that the style of writing to men has bit like this It says Take your 7/16 crescent wrench It means You are supposed to have a 7/16 crescent wrench dumbass We wo n't overtly insult you in public by telling you this but we 're content if you are ashamed that you do n't have one Kurson may not actually assume twentysomething guys have cash in reserve I 'll bet they do n't he just assumes that that 's the way you must write for men I have to question that wisdom as well The for dummies series of books probably sells just as well to men as to women that 's my own conjecture < p > ps Chris is right Men 's health is an absolute joke I 'd rather guests find a cache of 3rd rate porn at my house than a copy of men 's health < p > As a woman I can relate to this article I hate it when I read articles on money when the audience is to and that I have to be emotional about my money Give me the financial info straight up without the fluffy bs I read articles in men 's magazines and yes I do read the wall street journal in order to get the latest info on what 's happening on the street I want to be informed the same as men not made to feel that just because I 'm a woman everything about money has to be glossed over like lipstick and butterflies It 's about time someone recognized this annoying issue < p > One reason women give for not taking control of their investing future is that they lack confidence Evidence suggests that this is unfounded For those women who want to build up their confidence the following article and podcast can help The article gives the scientific reasons women are good investors The podcast gives information about the neurobiology behind the skill This knowledge helps women go forward with their investing futures which is so important today when women women are different yes but saying that women are n't smart enough to pick up a men geared magazine and she is not be able to understand it is just as bad as saying women are scared of finances anyway While traditional men vs. women roles are still in flux women these days are super women sexy feminine sensitive yet strong financially savvy and independent as opposed to dependent and submissive Some women still want to carry on the predecessors roles and read O Magazine that 's fine for them but women who are breaking the mold are smart enough to know that real financial advice is not found in a fluffy magazine If your looking for hard facts I give men and women alike enough credit to look in the right places despite the gender it 's geared for Get over it femenists who get mad at the way things are the more cross gender reading there is publisher 's will realize times are changing well said mark0157 < p > For they have in the finance investing section And I do mean everything The reason is because the more you read the more discerning you will become and be able to differentiate good advice from BS Just start Read some of the financial magazines too maybe subscribe to one or two that appeal to you Watch financial shows on TV It is like learning a new language you learn it a lot faster if you immerse yourself in it For now just set some savings goals and start setting aside something as much and as often as possible When you are ready to invest in say mutual funds or 401 k you will not only have a knowledge base but actual cash to use < p > I really enjoyed reading Suze Orman 's books and enjoy her show as well because it gave me a general overview of personal fianance and the courage to go out and buy Money and Fortune magazine I also enjoy Jim Cramer 's Mad Money and his books and recently purchased my first stocks to begin somewhere and Orman 's style helped give me the kick in the ass I needed to begin my journey We all have to start somewhere < p > I am a forty plus female I have never read O magazine and do not feel inclined to do so I like just the facts when it comes to money and investing Leave off the sugar coating I can do without it I read The Wall Street Journal because I enjoy it and The New Yorker also Who cares where the advice comes from so long as you get the right information I 've read Money and Fortune because I 'm interested in what they have to say again about money and investing < p > I would have to agree that a lot of gender specific magazines give pretty vague advice but they ARE just magazines and mostly general interest magazines after all No one should expect in depth advice from O or any other non financial magazine If it gets women or men for that I 'd expect If you want solid in depth advice take a hard look at your finances with a trusted educated financial counsellor Personally I hate all things Oprah Martha Stewart etc Suze Orman always leaves me with mixed feelings She is a great advocate for women taking control of their finances and I admire her for that especially since she 's been there done that But I do find her condescending especially calling everyone boyfriend and girlfriend all the time I guess it 's just her style but enough already < p > As for the touchy feely approach used by many to address female readers or viewers To some extent I get why they do this Women in general are more likely to express their money worries as feelings it makes them anxious nervous scared or if they 're not worried and doing well they 're confident happy etc Men do tend to get into the emotions of an issue It 's not that women do n't want the facts it 's that they need to express and address their feelings about it before they can get to what to do and how to do it And no I 'm not trying to be sexist I 've encountered this is many different areas other than finances And for the record I am 45 years old and in charge of my family 's finances I tend to view finances and many other things more in the way men do- maybe because I had 5 older brothers and spent 8 years in the army reserves in a nearly all male unit I think women have been conditioned this way and only now are we waking up to it and the implications of how it affects us < p > If the tone of an article troubles you write the publisher or author and tell them so The only way changes will be made is if they know they 're not addressing what they 're readership wants than men Think of it from an evolutionary standpoint men are built to run around distributing sperm hence greater tendency to take risks and women are meant to be the acquirers and managers of resources in order to care for children both younger and older women Thus men may take bigger risks but women have the right mix of rational thinking risk balance and appreciation of life needs to make better investment decisions < p > According to the data women make more money in the stock market than men make fewer investment errors and overall make gains of 1.5 to 3 over men in an investment portfolio There is also really good research based evidence that women serve their fiduciary duty better than men and negotiate better deals on behalf of clients investors than men do for their clients investors This is so much the case that investing institutions have started to look at the number of women who run finances at a company as a major plus for profitability < p > So our assumptions and behavior run counter We have an irrational male hero fetish and it manifests itself in stupid behavior In a rational world more women should manage money than men At any rate I would rather put my money with a so called boring housewife than a big badass risk taker like Madoff Who was that guy who ran Enron Henry Paulson Do n't get me started on men screwing up the government < p > Interesting stats on how few women read publications dedicated to finance especially if you believe the other older stat of women being responsible for 80 of household spending decisions If that 's the case then are husbands boyfriends assigned an investment budget by their ladies who then typically let them invest it however they want < p > Maybe < p > And I know this is totally contrary to the spirit of Ramit 's blog but women 's more protective attitude towards money and I am totally generalizing here may end up resulting in fewer investment gains in the long run but less family schemes There 's your yin yang moment of the day < p > As for being talked down to as a woman I do tend to read women 's magazines and I would resent the tone of the occasional nugget of finance wisdom except that I 'm an accounting grad who spent several years working in finance and I 'd say that even stuff I read in the New York Times can sound dumb ed down to me sometimes It 's not people 's fault that most of them are clueless about finance I could n't read a medical brief unless a doctor held my hand through the whole thing and kept me from passing out of boredom < p > I could however do with a little less of Suzie O 's hey Girfrieeeend and boyfrieeeend references and her book on women and money was the first book I ever owned that I would have happily set on fire That was the single worst purchase I ever made worse that the $ 300 Marc Jacobs dress I hated two hours after\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIlWr92sVU2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dict_to_texts(texts, file_name):\n",
        "    text = []\n",
        "    for doc in list(texts.values()):\n",
        "        text.append(' '.join(doc).replace(\"< h >\", \"\").replace(\"< p >\", \"\"))\n",
        "    train_text, test_text = train_test_split(text, test_size=0.2)\n",
        "    with open(file_name + \"_train\", 'w') as f:\n",
        "        for item in train_text:\n",
        "            f.write(\"%s\\n\" % item)\n",
        "    \n",
        "    with open(file_name + \"_test\", 'w') as f:\n",
        "        for item in test_text:\n",
        "            f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjqVa9GZVU2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_to_texts(us_texts, \"us_blog\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rflXQpTBVU2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict_to_texts(gb_texts, \"gb_blog\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSColB81VU2o",
        "colab_type": "text"
      },
      "source": [
        "We now have the training and testing files for both US and GB blogs in English. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wem_OXo0VU2p",
        "colab_type": "text"
      },
      "source": [
        "## WARNING - SHIFT TO GOOGLE COLAB\n",
        "\n",
        "The [Google Colab file](https://colab.research.google.com/drive/1_G6iGqiXb-zPBTurRxd7cgGrXyNaKGsA) walks you through the process of fine-tuning models, as we did before for the classification task. Move now to the colab file to fine tune your models. Once you downloaded all the models and their information, place those files in the directory of the HW to use them as demonstrated below. \n",
        "\n",
        "\n",
        "\n",
        "### Running Scripts\n",
        "\n",
        "We use the scripts to do language modelling and text generation. The following cells run the code as if you would have run it in a terminal. I trained all of these models using the Googlr Colab file, and then saved the models to disk.\n",
        "\n",
        "#### Trump GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV5B0eMtVU2q",
        "colab_type": "code",
        "outputId": "40e14822-cc3f-4bbe-c32f-3305bbe00a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " !python \"/content/drive/My Drive/week 8/run_language_modelling.py\" --output_dir=output_gpt_trump --model_type=gpt2 --model_name_or_path=gpt2 --do_train --train_data_file=train_text_trump --do_eval --eval_data_file=test_text_trump"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/09/2020 02:37:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/09/2020 02:37:48 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
            "03/09/2020 02:37:48 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "03/09/2020 02:37:50 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "03/09/2020 02:37:50 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/09/2020 02:37:51 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "03/09/2020 02:37:58 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='test_text_trump', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_gpt_trump', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train_text_trump', warmup_steps=0, weight_decay=0.0)\n",
            "03/09/2020 02:37:58 - INFO - __main__ -   Creating features from dataset file at \n",
            "03/09/2020 02:38:01 - INFO - __main__ -   Saving features into cached file gpt2_cached_lm_1024_train_text_trump\n",
            "03/09/2020 02:38:01 - INFO - __main__ -   ***** Running training *****\n",
            "03/09/2020 02:38:01 - INFO - __main__ -     Num examples = 669\n",
            "03/09/2020 02:38:01 - INFO - __main__ -     Num Epochs = 1\n",
            "03/09/2020 02:38:01 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "03/09/2020 02:38:01 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "03/09/2020 02:38:01 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "03/09/2020 02:38:01 - INFO - __main__ -     Total optimization steps = 168\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/168 [00:00<?, ?it/s]\u001b[ATraceback (most recent call last):\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 749, in main\n",
            "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 353, in train\n",
            "    outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 596, in forward\n",
            "    inputs_embeds=inputs_embeds,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 479, in forward\n",
            "    hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 229, in forward\n",
            "    self.ln_1(x), layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 532, in __call__\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 192, in forward\n",
            "    attn_outputs = self._attn(query, key, value, attention_mask, head_mask)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py\", line 146, in _attn\n",
            "    w = w / math.sqrt(v.size(-1))\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 6.84 GiB already allocated; 164.88 MiB free; 6.87 GiB reserved in total by PyTorch)\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGyYeZr5VU2r",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa US"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SethuP9zVU2s",
        "colab_type": "code",
        "outputId": "bad46fed-3351-4d2f-9bb1-b55df7496e0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " !python \"/content/drive/My Drive/week 8/run_language_modelling.py\" --output_dir=output_roberta_US --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/09/2020 02:39:39 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/09/2020 02:39:40 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/09/2020 02:39:40 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/09/2020 02:39:41 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/09/2020 02:39:41 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/09/2020 02:39:42 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/09/2020 02:39:47 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "03/09/2020 02:39:50 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_US', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='', warmup_steps=0, weight_decay=0.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 744, in main\n",
            "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 152, in load_and_cache_examples\n",
            "    return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 88, in __init__\n",
            "    assert os.path.isfile(file_path)\n",
            "AssertionError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3AVO4y6VU2t",
        "colab_type": "text"
      },
      "source": [
        "#### RoBERTa UK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJyzmaQqVU2t",
        "colab_type": "code",
        "outputId": "5afa8e15-4ea2-4323-f1d6-563ca36a31e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python \"/content/drive/My Drive/week 8/run_language_modelling.py\" --output_dir=output_roberta_UK --model_type=roberta --model_name_or_path=roberta-base --do_train --train_data_file=$TRAIN_FILE --do_eval --eval_data_file=$TEST_FILE --mlm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/09/2020 02:36:28 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "03/09/2020 02:36:29 - INFO - filelock -   Lock 140689656663176 acquired on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
            "03/09/2020 02:36:29 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpth2i45ms\n",
            "Downloading: 100% 524/524 [00:00<00:00, 503kB/s]\n",
            "03/09/2020 02:36:30 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json in cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/09/2020 02:36:30 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/09/2020 02:36:30 - INFO - filelock -   Lock 140689656663176 released on /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6.lock\n",
            "03/09/2020 02:36:30 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "03/09/2020 02:36:30 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"do_sample\": false,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "03/09/2020 02:36:31 - INFO - filelock -   Lock 140689656660488 acquired on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "03/09/2020 02:36:31 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpd_b9bzkz\n",
            "Downloading: 100% 899k/899k [00:01<00:00, 820kB/s]\n",
            "03/09/2020 02:36:33 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json in cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/09/2020 02:36:33 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/09/2020 02:36:33 - INFO - filelock -   Lock 140689656660488 released on /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b.lock\n",
            "03/09/2020 02:36:33 - INFO - filelock -   Lock 140689658590208 acquired on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/09/2020 02:36:33 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpewjyde81\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 630kB/s]\n",
            "03/09/2020 02:36:35 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt in cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/09/2020 02:36:35 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/09/2020 02:36:35 - INFO - filelock -   Lock 140689658590208 released on /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
            "03/09/2020 02:36:35 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "03/09/2020 02:36:35 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "03/09/2020 02:36:36 - INFO - filelock -   Lock 140689656598368 acquired on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
            "03/09/2020 02:36:36 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpnz_k3p8v\n",
            "Downloading: 100% 501M/501M [00:39<00:00, 12.7MB/s]\n",
            "03/09/2020 02:37:16 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin in cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/09/2020 02:37:16 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/09/2020 02:37:16 - INFO - filelock -   Lock 140689656598368 released on /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e.lock\n",
            "03/09/2020 02:37:16 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "03/09/2020 02:37:21 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "03/09/2020 02:37:24 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=512, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=True, mlm_probability=0.15, model_name_or_path='roberta-base', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='output_roberta_UK', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='', warmup_steps=0, weight_decay=0.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 799, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 744, in main\n",
            "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 152, in load_and_cache_examples\n",
            "    return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
            "  File \"/content/drive/My Drive/week 8/run_language_modelling.py\", line 88, in __init__\n",
            "    assert os.path.isfile(file_path)\n",
            "AssertionError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXvoEs8JVU2u",
        "colab_type": "text"
      },
      "source": [
        "### Loading and using models\n",
        "\n",
        "Let us now load the four models we have and see how we can use them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86BolPc3VU2v",
        "colab_type": "text"
      },
      "source": [
        "And now - let us see what our Trump Tweet Bot looks like!\n",
        "You can generate text via command line using the command below. You can also load a model once it is saved - I trained my model using Google Colab, downloaded the model, and am loading it again via the command below. Note that you have to download all the files in your folder of the fine-tuned model to use the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUD8qzzOVU2w",
        "colab_type": "code",
        "outputId": "b9346e90-1fa9-45a1-9af0-900747d42af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        }
      },
      "source": [
        " !python \"/content/drive/My Drive/week 8/run_generation.py\" --model_type=gpt2 --model_name_or_path=output_trump_gpt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "03/10/2020 02:54:39 - INFO - transformers.tokenization_utils -   Model name 'output_trump_gpt' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'output_trump_gpt' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/My Drive/week 8/run_generation.py\", line 263, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/My Drive/week 8/run_generation.py\", line 203, in main\n",
            "    tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\", line 309, in from_pretrained\n",
            "    return cls._from_pretrained(*inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py\", line 410, in _from_pretrained\n",
            "    list(cls.vocab_files_names.values()),\n",
            "OSError: Model name 'output_trump_gpt' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'output_trump_gpt' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt'] but couldn't find such vocabulary files at this path or url.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwQn79OYVU2x",
        "colab_type": "code",
        "outputId": "39c9ae6c-48b9-4e7a-d276-13a692d88186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "tokenizer_trump = AutoTokenizer.from_pretrained(\"output_trump_gpt\")\n",
        "model_trump = AutoModelWithLMHead.from_pretrained(\"output_trump_gpt\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-eedc5b73c3aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer_trump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_trump_gpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_trump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelWithLMHead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_trump_gpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpQqUNq7VU21",
        "colab_type": "code",
        "outputId": "4149ff15-a90d-41cc-9757-67c57f4e405a",
        "colab": {}
      },
      "source": [
        "sequence = \"Obama is going to\"\n",
        "\n",
        "input = tokenizer_trump.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_trump.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_trump.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama is going to be a disaster for the United States. He is a total loser. He is a total loser!\"\n",
            "\"\"\"@jeff_mcclaren: @realDonaldTrump @realDonaldTrump @foxandfriends @megynkelly @\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpKR2UyVU23",
        "colab_type": "text"
      },
      "source": [
        "Wow - our Trump bot is nasty, so we know our model trained well. What happens if we try the same sentence for our non-fine tuned model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBEJJwOgVU23",
        "colab_type": "code",
        "outputId": "58264281-e1ff-4b63-fbea-545172d04786",
        "colab": {}
      },
      "source": [
        "sequence = \"Obama is going to\"\n",
        "\n",
        "input = tokenizer_gpt.encode(sequence, return_tensors=\"pt\")\n",
        "generated = model_gpt.generate(input, max_length=50, bos_token_id=1, pad_token_id=1, eos_token_ids=1)\n",
        "\n",
        "resulting_string = tokenizer_gpt.decode(generated.tolist()[0])\n",
        "print(resulting_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obama is going to be a very good president,\" said Sen. John McCain (R-Ariz.). \"He's going to be a very good president. He's going to be a very good president. He's going to be a very\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pteBlOHVU25",
        "colab_type": "text"
      },
      "source": [
        "Quite the contrast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qtah2BbVU26",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:red\">*Exercise 3*</span>\n",
        "\n",
        "<span style=\"color:red\">Construct cells immediately below this that generate a BERT-powered chatbot tuned on text related to your final project. What is interesting about this model, and how to does it compare to an untrained model? What does it reveal about the social game involved with your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pT3TtwVVU26",
        "colab_type": "text"
      },
      "source": [
        "Let's now check out our UK and GB embeddings - how do you think the two models will differ? Maybe in the way different words relate to each other in the same sentence? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOEUcroQVU27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import RobertaConfig, RobertaModel, RobertaTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-3EBR6_VU28",
        "colab_type": "code",
        "outputId": "af6fcea9-5c7a-44b4-ee3b-f64db20d7f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "roberta_us_model_embedding = RobertaModel.from_pretrained('roberta_us')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-145-19cdbb6a372d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mroberta_us_model_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'roberta_us'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m                 \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m             )\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \"\"\"\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m                     )\n\u001b[1;32m    240\u001b[0m                 )\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Model name 'roberta_us' was not found in model name list. We assumed 'https://s3.amazonaws.com/models.huggingface.co/bert/roberta_us/config.json' was a path, a model identifier, or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVqympNAVU29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_us_tokenizer = RobertaTokenizer.from_pretrained('roberta_us')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpIVDgfGVU2-",
        "colab_type": "text"
      },
      "source": [
        "Let us try to visualise how words in a sentence or different or similar to each other. We will try to construct sentences where words might mean different things in different countries - in the US, people might eat chips with salsa, but in the UK, chips are what Americans call french fries, and might eat it fried fish instead. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzZkPQqOVU2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Do you have your chips with fish or with salsa?\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB0QxeUCVU3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text1 = \"He went out in just his undershirt and pants.\" #pants are underwear in Britain; maybe closer to an undershirt\n",
        "text2 = \"His braces completed the outfit.\" #braces are suspenders (in Britain); maybe closer to an outfit\n",
        "text3 = \"Does your pencil have a rubber on it?\" #rubber is an eraser in Britain); maybe closer to a pencil\n",
        "text4 = \"Was the bog closer to the forest or the house?\" #bog is a toilen in Britain); maybe closer to a house\n",
        "text5 = \"Are you taking the trolley or the train to the grocery market\" #trolley is a food carriage; possibly closer to a market"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c0eULwiVU3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAyUp2iQVU3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualise_diffs(text, model, tokenizer):\n",
        "    word_vecs = []\n",
        "    for i in range(0, len(text.split())):\n",
        "        word_vecs.append(word_vector(text, i, model, tokenizer))\n",
        "    L = []\n",
        "    for p in word_vecs:\n",
        "        l = []\n",
        "        for q in word_vecs:\n",
        "            l.append(1 - cosine(p, q))\n",
        "        L.append(l)\n",
        "    M = np.array(L)\n",
        "    fig = plt.figure()\n",
        "    div = pd.DataFrame(M, columns = list(text.split()), index = list(text.split()))\n",
        "    ax = sns.heatmap(div)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcS7XRG1VU3I",
        "colab_type": "code",
        "outputId": "962abda6-2365-4600-c980-ae154d559c3d",
        "colab": {}
      },
      "source": [
        "visualise_diffs(text, roberta_us_model_embedding, roberta_us_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OwJdUdzVU3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "roberta_gb_model_embedding = RobertaModel.from_pretrained('roberta_gb')\n",
        "roberta_gb_tokenizer = RobertaTokenizer.from_pretrained('roberta_gb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp8WZe_JVU3L",
        "colab_type": "code",
        "outputId": "185c1302-89a1-4607-e895-b60a50a1799a",
        "colab": {}
      },
      "source": [
        "visualise_diffs(text, roberta_gb_model_embedding, roberta_gb_tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X2cVWW99/HPVwRREVHxmIqKmr4Is3wgLZMOWpZ6ysz0Fo+9kl4lmYfUSs3uSj2e9FiUnu7Uusk8pHb7EGl6jCQDH0hNQQF58AlBE00LEhUfgJn53X+sa3QxzszeM7Nmzd57vm9f6+Xa6+l3rT2b3772ta61LkUEZmbWeDbq6wKYmVnvcII3M2tQTvBmZg3KCd7MrEE5wZuZNSgneDOzBuUEb2bWoJzgzcwalBO8mVmD2rivC9Ad61cuK+X22013GFtGGAB2GbpdabFeb3qztFh/f/3l0mJdtP0hpcS57NUFpcQBOGLoqNJiDUClxbp9zZOlxVq2cl6PT6wrOWfg8N3KeyMrcA3eGkJZyd2sntRlDd7MrFQtzX1dgm5xgjczq6S5qa9L0C1O8GZmFUS09HURusUJ3syskhYneDOzxuQavJlZg/JF1o5JagYWAgOBJuBq4NKo14YtM+tf6jRVlVWDfyMi9gGQ9E/A/wOGAueVFN/MrNuiTnvRlH6jU0T8DZgITFJmsKT/lrRQ0jxJvmPFzGpLS0v1Uw3pkztZI2IZMAD4J+DfskWxN3AC8EtJg9vuI2mipLmS5l559XXlFtjM+rdoqX6qIbVwkfVg4CcAEfGYpGeAPYFH8htFxBRgCpT3LBozM8AXWbtC0m5AM/C3vohvZtYlNVYzr1bpCV7StsDPgMsiIiTNBk4EZknaE9gZeLzscpmZdahOL7KWleA3lTSft7tJXgNcktZdAfxU0sK0bkJErC2pXGZmldXYxdNqlZLgI2JAJ+veBL5QRjnMzLojwm3wZmaNyW3wZmYNyk00ZmYNyjV4M7MG1by+r0vQLU7wZmaVuImmPJvuMLaUOG88P7uUOAA37f3d0mJtsnF5NwLP37KcAeZfJ2gpaSz7E4fuXU4gYIfm8p4msuP68pLYkCGjSotVCDfRmPWdspK79VOuwZuZNSgneDOzxhR1epG1Tx4XbGZWVwp8XLCkwyU9LmmppHPaWb+LpJmSHpF0l6QRuXU7S/qDpEclLZE0srNYTvBmZpUUNOCHpAHA5cARwGjgBEmj22z2Q+DqiHgfcAHwn7l1VwOTI+I9wAFUeCKvE7yZWSXF1eAPAJZGxLKIWAdcD3y6zTajgVlp/s7W9emLYOOIuAMgItZExOudBXOCNzOrpAs1+Pzoc2mamDvSjsCzudcr0rK8BcAxaf4zwBaStiEbCGm1pJvS8KaT0y+CDvkiq5lZJV3oB58ffa6bzgQukzQBuAd4jmyApI2BscC+wF+AG4AJwC86OlDhNXhJF0g6I/f6Qkmnp2+bRWlw7ePTunGSbstt23pSZma1o6mp+qlzzwE75V6PSMveEhHPR8QxEbEv8O20bDVZbX9+at5pAn4L7NdZsN5oorkK+DyApI2A8alg+wDvBz4GTJa0fS/ENjMrXnFt8HOAPSTtKmkQWX68Nb+BpOEpdwJ8iyyntu47LI2KB3AosKSzYIUn+Ih4GlglaV/g48A8soG1r4uI5oh4Ebgb+EBXjptv12ppea3oYpuZdaygXjSp5j0JmAE8CtwYEYtTy8dRabNxwOOSngC2Ay5M+zaTNd/MTCPgCfh5Z/F6qw3+SrK2oXeRffsc1sF2TWz4JTO4owPm27U2HrRjeQ9TMTMr8Fk0ETEdmN5m2bm5+WnAtA72vQN4X7WxeqsXzc3A4WS19BnAbOB4SQPSz4uPAA8CzwCjJW0iaRjw0V4qj5lZ9xVUgy9br9TgI2KdpDuB1RHRLOlm4ENk3X8CODsiXgCQdCOwCFhO1pxjZlZb/DTJt6ULBB8EjgOIiADOStMGIuJs4OzeKIeZWSEq946pSb3RTXI0sBSYGRFPFn18M7PSRVQ/1ZDCa/ARsQTYrejjmpn1mRprW6+W72Q1M6vECd7MrEH5IquZWYNqbu7rEnRLXSb4XYZuV0qcMgfCPmbhf5QW66mDJpUWa+2rW5UWa8w2K0uJs/aN8v7ZrF7b4b1/hdt91KrSYm396LaVN6olbqIx6ztlJXfrp5zgzcwalNvgzcwaU7TUVv/2ajnBm5lV4iYaM7MG5V40ZmYNyjV4M7MGVacJvtsPG5M0UtKiIgtjZlaT/LAxM7MG1d9q8MkAST+XtFjSHyRtKulkSXMkLZD0G0mbSdpS0jOtA8lK2lzSs5IGStpd0u2SHpI0W9KoAs7LzKw4LVH9VEN6muD3AC6PiL2A1cBngZsi4gMR8X6yQWW/GBEvA/OBf077fRKYERHrycZZ/WpE7E82oOwVPSyTmVmxmpurn2pIT5tolkfE/DT/EDASeK+k7wHDgCFkY7IC3AAcD9wJjAeukDQEOAj4taTWY27SXiBJE4GJAMM334mhg4f3sOhmZtWJOm2i6WmCX5ubbwY2BaYCR0fEAkkTgHFp/a3ARZK2BvYHZgGbk43buk+lQBExhay2z+7D96ut30Fm1thqrOmlWoUP2QdsAfxV0kDgxNaFEbEGmAP8GLgtIpoj4hVguaTjAJR5fy+Uycys+6Kl+qmG9EaC/y7wAHAv8FibdTcAn0v/b3Ui8EVJC4DFwKd7oUxmZt1XpxdZu91EExFPA+/Nvf5hbvVPO9hnGqA2y5YDh3e3HGZmva6pti6eVsv94M3MKqmxppdqOcGbmVVSY00v1XKCNzOroL92kzQza3yuwZuZNSgn+PK83vRmKXE22bi8P+pTB00qLdbu911WWqx1Y04vJc5rawbx+NqhpcTarMSf68sHDSgt1qrHti8t1nMlnte4Ig5SY48gqFZdJniztspK7tY/eUxWM7NG5QRvZtag6rQXTW88qsDMrLEU+KgCSYdLelzSUknntLN+F0kzJT0i6S5JI3LrTpL0ZJpOqhTLCd7MrJKCErykAcDlwBHAaOAESaPbbPZD4OqIeB9wAfCfad+tgfOAA4EDgPMkbdVZPCd4M7MKorml6qmCA4ClEbEsItYB1/POByyOJnucOmTjZ7Su/wRwR0T8IyJeAu6gwnO8nODNzCrpQg1e0kRJc3PTxNyRdgSezb1ekZblLQCOSfOfAbaQtE2V+27AF1nNzCroSjfJ/OBE3XQmcFkaMOke4DmyAZW6rCYTvKQBEVGfdxaYWeMprpvkc8BOudcj0rK3RMTzpBp8Gtb0sxGxWtJzbHjf1gjgrs6C9biJRtIFks7Ivb5Q0umSJktaJGmhpOPTunGSbstt2/othaSnJX1f0sPAcT0tl5lZYVq6MHVuDrCHpF0lDSIbn/rW/AaShktqzc3fAq5K8zOAj0vaKl1c/Thvj3ndriLa4K8CPp8KtlEq8ApgH+D9wMeAyZKquQ96VUTsFxHXt12Rb9d6fd1LBRTbzKw60dRS9dTpcSKagElkiflR4MaIWJwqykelzcYBj0t6AtgOuDDt+w/gP8i+JOYAF6RlHepxE01EPC1plaR9U2HmAQcD16Vmlhcl3Q18AHilwuFu6GhFvl1r+2Gj6/O2MjOrTwXe5xQR04HpbZadm5ufBkzrYN+reLtGX1FRbfBXAhOAd6Xgh3WwXRMb/moY3Gb9awWVx8ysMPX6LJqiukneTNYf8wNkPz1mA8dLGiBpW+AjwIPAM8BoSZtIGgZ8tKD4Zma9p7g2+FIVUoOPiHWS7gRWR0SzpJuBD5H15wzg7Ih4AUDSjcAiYDlZc46ZWU2r1xp8IQk+XVz9IKn3S0QEcFaaNhARZwNnt7N8ZBFlMTMrXI3VzKtVRDfJ0cBSYGZEPNnzIpmZ1ZZoqn6qJUX0olkC7FZAWczMalLUaQ2+Ju9kNTOrKU7wZmaNyTV4M7MG5QRfor+//nIpceZvqVLiAKx9tdPn9hdq3ZjTS4v1nrk/LicOsPRDk0qJNWTY2lLiAOy8crPSYj29fkhpsUatK+89LEI0l5cLilSXCd6srbKSu/VPrsGbmTWoaHEN3sysIbkGb2bWoCJcgzcza0iuwZuZNagW96IxM2tM9XqRtZDnwUuaKunYdpbvIKndkUnMzOpFtKjqqZb0ag0+jQ7+jsRvZlZPoj4fB9+9Grykz0t6RNICSdekxR+RdJ+kZa21eUkjJS1K8xMk3SLpLklPSjovLd9c0u/SsRZJOr6QMzMzK0i/qcFL2gv4DnBQRKyUtDVwCbA92WDbo4BbaX/Q2AOA9wKvA3Mk/Q7YBXg+Iv4lHX/LDuJOBCYCaMCWbLTR5l0tuplZt9RrN8nu1OAPBX4dESsBIuIfaflvI6IlPR9+uw72vSMiVkXEG8BNZF8IC4HDJH1f0tiIaPdBMxExJSLGRMQYJ3czK1Nzs6qeaklRg24D5J8e1NFZtm3Jioh4AtiPLNF/T9K5BZbJzKzHIlT1VEu6k+BnAcdJ2gYgNdFU6zBJW0vaFDgauFfSDsDrEXEtMJks2ZuZ1Yx+0wYfEYslXQjcLakZmNeF3R8EfgOMAK6NiLmSPgFMltQCrAe+0tUymZn1pnrtRdOtbpIR8Uvgl52sH5L+/zTZRdVWKyLi6DbbzgBmdKccZmZlqLWaebV8J6uZWQXNLUVerixPaQk+IqYCU8uKZ2ZWlH7VRGNm1p+01FjvmGo5wZuZVVBr3R+r5QRvZlaBm2hKdNH2h5QSZ43K+6uO2WZlabEeXjW8tFgDSxwM+933X1ZKnNdO/1IpcQCGT9qrtFgj5y0uLdbSm+rroqWbaMz6UFnJ3fon96IxM2tQddpCU+izaMzMGlJLqOqpEkmHS3pc0lJJ57SzfmdJd0qalx7LfmQ769dIOrNSLCd4M7MKinrYmKQBwOXAEcBo4ARJo9ts9h3gxojYFxgPXNFm/SXA76spt5tozMwqaCnuUAcASyNiGYCk64FPA0ty2wQwNM1vCTzfukLS0cBy4LVqgrkGb2ZWQaCqpwp2BJ7NvV6RluWdD3xO0gpgOvBVAElDgG8C/15tuUtJ8JKmSxqWplNzy8dJuq2MMpiZdVdTqOpJ0kRJc3PTxC6GOwGYGhEjgCOBayRtRJb4L42INdUeqJQmmog4ErIxWoFTeWebkplZzaqiZv72thFTgCkdrH4O2Cn3ekRalvdF4PB0rPslDQaGAwcCx0r6ATAMaJH0ZkR02Ee4kBq8pLMknZbmL5U0K80fKulXkp6WNBy4GNhd0nxJk9PuQyRNk/RY2rY+7ygws4bV0oWpgjnAHpJ2lTSI7CLqrW22+QvwUQBJ7wEGA3+PiLERMTIiRgL/BVzUWXKH4ppoZgNj0/wYsqQ9MC27J7fdOcBTEbFPRJyVlu0LnEF2RXk34MMFlcnMrBBFtcFHRBMwiWwMjEfJessslnSBpKPSZt8ATpa0ALgOmBDRvYclFNVE8xCwv6ShZGOzPkyW6McCpwHf6mTfByNiBYCk+cBI4E9tN0rtWBMBjtn6AA4cskdBRTcz61yBvWiIiOlkF0/zy87NzS+hQkU3Is6vJlYhNfiIWE/WdWcCcB9Zjf4Q4N1k31KdyQ/W3UwHXzoRMSUixkTEGCd3MytTM6p6qiVF9qKZDZxJ1iQzGzgFmNfmp8WrwBYFxjQz63Utqn6qJUUn+O2B+yPiReDNtOwtEbEKuFfSotxFVjOzmtaCqp5qSWHdJCNiJjAw93rP3PzI3Py/ttn1rty68p4ta2ZWpXp92JgfVWBmVkGRF1nL5ARvZlZBS53enuMEb2ZWQXNfF6CbnODNzCqotd4x1XKCNzOroNZ6x1SrLhP8Za8uKCXOiUP3LiUOwNo3yvtTbNZS3iWjIcPWVt6oAC8ccTJb7FlOX4fNf3xlKXEA1l50Rmmx1j35Smmx3rVzfT2p3L1ozPpQWcnd+ic30ZiZNSh3kzQza1DNrsGbmTUm1+DNzBqUE7yZWYMKN9GYmTWmeq3B90pnVEmnSXpU0kuSzulkuwmSOh1T0MysrzV3YaolvVWDPxX4WOtQfGZm9axe+8EXXoOX9DOywbN/L+lrrTV0ScelgT4WSMoPxL2DpNslPSnpB0WXx8ysp1q6MNWSwmvwEXGKpMPJxmT9ZG7VucAnIuI5ScNyy/cB9iUbm/VxST+JiGeLLpeZWXfVWuKuVpkPhLgXmCrpZGBAbvnMiHg5It4ElgC7tLezpImS5kqau2btP0oorplZJrow1ZLSEnxEnAJ8B9gJeEjSNmlV/mlUzXTwqyIipkTEmIgYM2STrXu3sGZmOfU66HZp3SQl7R4RDwAPSDqCLNGbmdW8WusdU60y+8FPlrQHIGAmsICs/d3MrKa11FzjS3V6JcFHxMg0OzVNRMQx7Wz61vq0zSfb2cbMrE/V60VW38lqZlZBfdbfneDNzCpyDd7MrEE1qT7r8E7wZmYV1Gd6d4I3M6vITTQlOmLoqFLi7NBc3o2+q9cOLi3W8kEDKm9UkJ1XblZKnNUr4d3n71VKrLUXnVFKHIBN/vd/lRaLH51VWqj1964qLVYR3E3SrA+Vldytf6rP9O4Eb2ZWUb020ZT5sDEzs7rUTFQ9VSLpcEmPS1ra3oBIki6VND9NT0hanVv3A0mL04BK/0dSp0+/cQ3ezKyComrwkgYAlwOHASuAOZJujYglrdtExNdy23+V7HHqSDoI+DDwvrT6T8A/A3d1FM81eDOzCqIL/1VwALA0IpZFxDrgeuDTnWx/AnDdW8WAwcAgYBNgIPBiZ8Gc4M3MKihwRKcdgfyARivSsneQtAuwKzALICLuB+4E/pqmGRHxaGfBnODNzCpoIaqe8oMTpWliN8OOB6ZFRDOApHcD7wFGkH0pHCppbGcHcBu8mVkFXekmGRFTgCkdrH6ODcfCGJGWtWc88G+5158B/hwRawAk/R74EDC7o7LUXA1emZorl5n1X01E1VMFc4A9JO0qaRBZEr+17UaSRgFbAffnFv8F+GdJG0saSHaBtfaaaCR9XdKiNJ0haWTqNnQ1sAiP9mRmNaSoi6wR0QRMAmaQJecbI2KxpAskHZXbdDxwfUTkDzgNeApYSDZg0oKI+J/O4pXeRCNpf+ALwIFkozs9ANwN7AGcFBF/7mC/icBEgLFb78d7ttitnAKbWb9X5I1OETEdmN5m2bltXp/fzn7NwJe7EqsvavAHAzdHxGupLekmYCzwTEfJHTYcdNvJ3czKVGA3yVLV0kXW1/q6AGZm7fGjCqo3Gzha0maSNie7MtzhVWAzs77WHFH1VEtKr8FHxMOSpgIPpkVXAi+VXQ4zs2r5ccFdEBGXAJe0WfzeviiLmVkltda2Xq1aaoM3M6tJ9doG7wRvZlaBm2jMzBqUm2jMzBpUrfWOqZYTvJlZBW6iKdEAOh2lqjA7ri/v0sruo8obZX7VY9uXFuvp9UPKifPNZxg3fk0psdY9+UopcQD40VmlhdrkG5NLi7VqRnefoNs3fJHVrA+Vldytf3IbvJlZg3ITjZlZgwpfZDUza0zNrsGbmTUmN9GYmTUoN9GYmTWoeq3Bl/I8eEnTJQ1L06m55eMk3VZGGczMuqteR3QqJcFHxJERsRoYBpxaaXszs1pSrwN+FJLgJZ0l6bQ0f6mkWWn+UEm/kvS0pOHAxcDukuZLar1tboikaZIeS9uWc5uqmVmVWoiqp1pSVA1+NtnA2QBjyJL2wLTsntx25wBPRcQ+EdF6D/a+wBnAaGA34MPtBZA0UdJcSXOXvLqsoGKbmVXW3xP8Q8D+koYCa4H7yRL9WCqPt/pgRKyIiBZgPjCyvY0iYkpEjImIMaO32K2gYpuZVRYRVU+1pJBeNBGxXtJyYAJwH/AIcAjwbuDRCruvzc03F1UmM7Oi1FrNvFpFXmSdDZxJ1iQzGzgFmBcbfqW9CmxRYEwzs17nXjRZUt8euD8iXgTepE3zTESsAu6VtCh3kdXMrKY1R0vVUy0prDkkImYCA3Ov98zNj8zN/2ubXe/KrZtUVHnMzIpSa23r1XJ7t5lZBfXaBu8Eb2ZWQa21rVfLCd7MrIIWN9GYmTUm1+DNzBpUrfWOqZbq8erwbsP3LaXQxw4ZVUYYAI58o6m0WI8N2qS0WKPWra28UUG23GRdKXHetfMrpcQB0Ebl/ftcu6a8+t4Of5hSWqyBw3fr8fOt9tx2TNV/iCf+PrdmnqflGrw1hLKSu/VP9dpEU8rjgs3M6llLRNVTJZIOl/S4pKWSzmln/aXpibvzJT0haXVavo+k+yUtlvSIpOMrxXIN3sysgqJq8JIGAJcDhwErgDmSbo2IJW/Fivhabvuvkj1xF+B14PMR8aSkHYCHJM1IY220ywnezKyC5mgu6lAHAEsjYhmApOuBTwNLOtj+BOA8gIh4onVhRDwv6W/AtoATvJlZdxXYGWVH4Nnc6xXAge1tKGkXYFdgVjvrDgAGAU91Fsxt8GZmFXRlwI/84ERpmtjNsOOBaREb/nyQtD1wDfCFNI5GhwpN8JKmSjq2G/udki4cPCHp/CLLZGbWU10Z8CM/OFGa8n1CnwN2yr0ekZa1ZzxwXX5BGlTpd8C3I+LPlcpdK000S8kuJAh4TNKVEbGij8tkZgYU+qiCOcAeknYlS+zjgbZP2EXSKGArstHxWpcNAm4Gro6IadUEq1iDl7S5pN9JWpCe4368pHMlzUmvp7Q3ULakiyUtSd15fpiWfUrSA5LmSfqjpO0AIuKPEbGOLMFvDLhTs5nVjKIG/IiIJmASMINstLsbI2KxpAskHZXbdDxwfZsBk/4X8BFgQq4b5T6dxaumBn848HxE/AuApC2BOyLigvT6GuCTwP+07iBpG+AzwKiICEnD0qo/AR9My74EnA18IxdrSjqpv1VRLjOzUhT5qIKImA5Mb7Ps3Davz29nv2uBa7sSq5o2+IXAYZK+L2lsRLwMHJJq4guBQ4G92uzzMtmITr+QdAxZ/03I2ptmpP3Oyu+Xvr22B77ZXiHyFy5eeXNlF07RzKxn6nXQ7YoJPvW93I8s0X9P0rnAFcCxEbE38HNgcJt9msj6e04jq93fnlb9BLgs7fflNvu9D/hDR1eF8xcuhg4e3oVTNDPrmSLvZC1TxSaadMfUPyLi2nTL7JfSqpWShgDHkiXy/D5DgM0iYrqke4FladWWvH3F+KQ2oX4LrO/eaZiZ9Z5aq5lXq5o2+L2ByZJayBLwV4CjgUXAC2RXhdvaArhF0mCyC6dfT8vPB34t6SWyzvu75vY5mKwp5/Gun4aZWe9p2CH7ImIG2RXfvLnAd9rZdkLu5QHtrL8FuKWDOD+rVBYzs77QyDV4M7N+rV4H/HCCNzOroNYunlbLCd7MrAI30ZiZNah6HdHJCd7MrALX4M360MtrB3lcVus19doGr3r9ZuoqSRPbPLbTsRyrIc/JsaxVfxrwo7sP3Xesxo7ViOfkWAb0rwRvZtavOMGbmTWo/pTgy2y3c6z6idWI5+RYBvSji6xmZv1Nf6rBm5n1Kw2Z4CU1p/EKF6exZL8hqW7OVdJISYv6uhy9SdJUSce2s3wHSVUNKNzD+NMlDUvTqbnl4yTd1s1jnibpUUkvSTqnk+0mSLqsOzFqSW+8h+3EaPdzUsV+p6R//09IOr+IstSjukl6XfRGROwTEXsBhwFHAOf1cZn6DUkDurtvRDwfEV3+B92NOEdGxGpgGHBqpe2rdCpwWERsFREXF3TMHlOm8H/rvfQeFmUpsC/ZeBYnSRrRx+XpE42a4N+SBvCeCExKH/TBkv5b0kJJ8yQd0pXjpdHPz8i9vlDS6ZImS1qUjnt8WrdBTUbSZZImVBlqgKSfp1rIHyRtKulkSXPSr5LfSNpM0paSnmn9Byxpc0nPShooaXdJt0t6SNJsSaN663wkPZ3G7X0YOK6dOJ+X9Egq+zVp8Uck3SdpWWstLf/rJdV0b5F0l6QnJZ2XO8ffpWMtai1fm3hnSTotzV8qaVaaP1TSr1J5hwMXA7unX3yT0+5DJE2T9FjaVpX+WJJ+BuwG/F7S11pr6JKOS2VcIOme3C47pL/Nk5J+UOn4VcT/eoqzSNIZ6X18XNLVZIPz7NSNY/bKe9je30/SuemzvUjSlPbec0kXS1qSPkc/TMs+pWx86HmS/ihpO4CI+GNErCMbcGhjoH/e5tyVwWTrZQLWtLNsNbAd8A3gqrRsFPAXYHAXjj0SeDjNbwQ8BXwWuAMYkGL8hWwA8XHAbbl9LwMmVBmjCdgnvb4R+BywTW6b7wFfTfO3AIek+eOBK9P8TGCPNH8gMKu3zgd4Gji7g/PZC3gCGJ5ebw1MBX6dYo4GlubKsyjNTwD+CmwDbEqWqMak8v08d/wt24n5QeDXaX428CAwkOyX3JdTeYfn46Vtx5ENGj8ile1+4OAqPxutx5xANvYwZGMZ75jmh+XOaxnZEJaDgWeAnXrwed8/xdkcGAIsJqu9tgAf7MFxe+U9bO/vB2yde30N8Kk0P5VsWNBtyEZ7a+0Y0vpebpVb9iXgR23O4Wpgcnffg3qfGr4G346DgWsBIuIxsn9ce1a7c0Q8DayStC/wcWBeOuZ1EdEcES8CdwMf6GE5l0fE/DT/ENk/ovemmvhC4ESyxAlwA1liBxgP3KBsXNyDyIZInA/8X7Ik3Zvnc0MHyw8lSxQrU8x/pOW/jYiWiFhC9kXSnjsiYlVEvAHclMq2EDgs/WIYGxEvt7PfQ8D+koYCa8mSzBhgLFmy6syDEbEisgHg55O99911LzBV0slkX5itZkbEyxHxJrAE2KUHMQ4Gbo6I1yJiDdn7NBZ4JiL+3IPj9tZ72N7f75BUE19I9nnZq83xXgbeBH4h6Riy4T0h+xKZkfY7K7+fpKPIPvPf7OJ5N4x+keAl7QY0A38r6JBXktXCvgBc1cl2TWz4Hg/uQoy1uflmsp+ZU4FJEbE38O+5490KHC5pa7La3KwUd3Vk1yJap/f08vm8Vumk2sifY0fNIG378UZEPAHsR5Yovifp3HfsFLEeWE52XveRJaRDgHcDj3ahXK3vfbdExClkw1vuBDwkaZuiY3Siq3+PDfTWe9jB3+8K4Nj02f45bT5bEdFENgzoNOCTwO0yVRd8AAACQElEQVRp1U/Ifi3tTfarIr/f+4A/pC+ZfqnhE7ykbYGfkX0IguxDemJatyewM10f6Ptm4HCyWu2MdMzjJQ1I8T5C9nP2GWC0pE0kDQM+2sPT2QL4q6SBrecAkGptc4AfkzWhNEfEK8ByScfBWxfa3t9H5zMLOK41uaUvomodJmlrSZuSDfZ+r6QdgNcj4lpgMlmyaM9s4EzgnjR/CjAvfQ5avUr2vvYKSbtHxAMRcS7wd7rRFl6F2cDRyq7JbA58hso17K4cu9D3sJO/38r0y7O93lVDyJripgNfA1o/y1sCz6X5k9rs9luyyk+/1aiPC940NUsMJKt1XgNcktZdAfw0/aRrImtDXtv+YdoXEesk3UlWQ26WdDPwIWABWY3z7Ih4AUDSjWRtx8vJmj964rvAA2SJ4gE2/Ed1A1mb9rjcshPJzvU7ZO/F9amMpZ5PRCyWdCFwt6TmavdLHgR+Q/ZT/NqImCvpE8BkSS3AeuArHew7G/g2cH9EvCbpTdokvohYJeleZRd2fw/8rgtlq8ZkSXuQ/UKZSfae7lNkgIh4WNJUsvcKsl9kLxV0+N54D/fmnX+/o8k+Vy+QVVba2gK4RdJgsvfy62n5+WTNkC+RVSR2ze1zMFlTTlcrcA3Dd7J2g7IeKw8Dx0XEk31dnp6q1fNR1kNnTERM6uuymNWjhm+iKZqk0WR9bGfWUjLsrkY7HzN7m2vwZmYNyjV4M7MG5QRvZtagnODNzBqUE7yZWYNygjcza1BO8GZmDer/A5rGHYe7MJGRAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSac_0k8VU3N",
        "colab_type": "text"
      },
      "source": [
        "What do you see regarding the relations with chips and sala/fish? What about the other sentences? How about comparing sentence embeddings?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRPnKP76KG5Q",
        "colab_type": "text"
      },
      "source": [
        "The prior models kept running and failed to complete, so I could not really do the exercise 3 and exercise 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS1KRxaIVU3N",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:red\">*Exercise 4*</span>\n",
        "\n",
        "<span style=\"color:red\">Construct cells immediately below this that tune BERT to at least two different textual samples. These could be from different corpora, distinct time periods, separate authors, alternative publishing outlets, etc. Then compare the meaning of words, phrases and sentences to each other across the separate models. What do they reveal about the social worlds inscribed by the distinctive samples?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MQPE9VYKQGu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}